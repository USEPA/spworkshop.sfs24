{
  "hash": "b263439ef13821959eabb6e7c39f7bda",
  "result": {
    "markdown": "# Additional `spmodel` Features {#sec-features}\n\n\n::: {.cell}\n\n:::\n\n\nThroughout this section, we will use both the `spmodel` package and the `ggplot2` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spmodel)\nlibrary(ggplot2)\n```\n:::\n\n\nWe will continue to use the `moss` data throughout this section.\n\n__Goals__: \n\n* Accommodate big spatial data:\n    * Use the `local` argument in `splm()` to fit a spatial linear model to a large data set.\n    * Use the `local` argument in `predict()` (or `augment()`) to make predictions for a large data set.\n* Incorporate additional arguments to `splm()` (and later, `spautor()`) to:\n    * Fit and predict for multiple models simultaneously.\n    * Fit a spatial linear model with non-spatial random effects.\n    * Fit a spatial linear model with anisotropy.\n    * Fit a spatial linear model with a partition factor.\n    * Fix certain spatial covariance parameters at known values.\n    * Fit a random forest spatial residual linear model and make predictions.\n* Use the `spautor()` function in `spmodel` to fit a spatial linear model to areal data:\n    * Connect parameter estimates in the summary output of `spautor()` to the spatial linear model introduced in @eq-splm in @sec-splm.\n    * Apply some of the other functions introduced in @sec-splm to a model object fit with `spautor()`.\n* Simulate spatial Gaussian data using `sprnorm()`.\n\n## Big Spatial Data\n\nFor large observed data sets, fitting spatial linear models or making predictions is challenging because these operations require a covariance matrix inverse, which are computationally challenging to obtain. Typically, observed data samples sizes approaching around 10,000 make model fitting or prediction infeasible on a standard computer in a reasonable amount of time (your definition of this may vary). This necessitates the use of model fitting and prediction tools that work for large data sets. `spmodel` offers big data methods for model fitting and prediction for point-referenced data via the `local` argument to `splm()` and `predict()`.\n\n### Model Fitting\n\n`spmodel` implements \"local\" spatial indexing as described by @hoef2023indexing. Observations are first assigned an index. Then for the purposes of model fitting, observations with different indexes are assumed uncorrelated. Assuming observations with different indexes are uncorrelated induces sparsity in the covariance matrix, which greatly reduces the computational time required for operations that involve its inverse. Models fit using spatial indexing are capable of fitting models with hundreds of thousands of observations relatively quickly. @hoef2023indexing showed that in a variety of scenarios, spatial indexing yielded fixed effect confidence intervals with proper coverage.\n\nTo illustrate spatial indexing in `spmodel`, we first simulate a response variable `sim_response` with `5000` observations at random spatial locations in the unit square (`sim_coords`). Then we place the response and coordinates in a `data.frame`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(06022024)\nsim_params <- spcov_params(\"exponential\", de = 7, ie = 2, range = 0.7)\n\nn <- 5000\nx <- runif(n)\ny <- runif(n)\nsim_coords <- data.frame(x, y)\n\nsim_response <- sprnorm(sim_params, data = sim_coords, xcoord = x, ycoord = y)\nsim_data <- data.frame(sim_coords, sim_response)\n```\n:::\n\n\nWe visualize the data by running\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sim_data, aes(x = x, y = y, color = sim_response)) +\n  geom_point() +\n  scale_color_viridis_c(limits = c(-8, 9)) +\n  theme_gray(base_size = 14)\n```\n\n::: {.cell-output-display}\n![Distribution of simulated data](splm-additional_files/figure-html/fig-simdata-1.png){#fig-simdata width=672}\n:::\n:::\n\n\n::: {.callout-note}\nWe provide more detail regarding using `spmodel` to simulate data later on in this section.\n:::\n\nWe then use `splm()` to fit a spatial model to `sim_data`, providing the `xcoord` and `ycoord` arguments because `sim_data` is a `data.frame`, not an `sf` object. To implement spatial indexing, we use the `local` argument to `splm()`. Setting `local` to `TRUE` chooses default spatial indexing settings. We fit the model and time it by running\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfit_start_time <- proc.time()\nbdmod <- splm(sim_response ~ 1, data = sim_data,\n     spcov_type = \"exponential\",\n     xcoord = x, ycoord = y,\n     local = TRUE)\nfit_end_time <- proc.time()\nfit_end_time - fit_start_time\n#>    user  system elapsed \n#>    5.17    0.53    6.78\n```\n:::\n\n\nThe model with `5000` observations is fit in just 6.78 seconds.\n\n::: {.callout-note}\nWhen the sample size is larger than 5000 observations, `splm()` implements spatial indexing by default, as fitting time without spatial indexing becomes lengthy. This behavior can be overridden by explicitly setting `local` to `FALSE`.\n:::\n\nA summary of the model fit yields\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(bdmod)\n#> \n#> Call:\n#> splm(formula = sim_response ~ 1, data = sim_data, spcov_type = \"exponential\", \n#>     xcoord = x, ycoord = y, local = TRUE)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -8.4301 -1.8626  0.1926  1.8081  7.9181 \n#> \n#> Coefficients (fixed):\n#>             Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)   0.5384     1.2610   0.427    0.669\n#> \n#> Coefficients (exponential spatial covariance):\n#>     de     ie  range \n#> 4.3557 1.9964 0.4643\n```\n:::\n\n\nThe other way to specify `local` in `splm()` is via a list object, which offers much more control and customization over the spatial indexing. To learn more, read about `local` in `splm()`'s help page by running `?splm`.\n\n::: {.callout-note}\nEven for two separate data sets with the same sample size fit on the same machine, the computational time required to fit models via spatial indexing varies, depending on many factors like the number of iterations required for convergence and the number of observations assigned to each spatial index.\n:::\n\n### Local Prediction\n\nUsing the fitted model, @hoef2023indexing evaluates the performance of local neighborhood prediction. Local neighborhood prediction only uses some of the observed data to predict for an unobserved location of interest. Local neighborhood prediction is capable of making predictions of hundreds of thousands of observations relatively quickly. @hoef2023indexing showed that in a variety of scenarios, local neighborhood prediction yielded prediction intervals with proper coverage.\n\nTo illustrate local neighborhood prediction in `spmodel`, we first simulate `3000` new random spatial locations in the unit square (`sim_coords`). Then we place the coordinates in a `data.frame` and visualize:\n\n::: {.cell}\n\n```{.r .cell-code}\nn_pred <- 3000\nx_pred <- runif(n_pred)\ny_pred <- runif(n_pred)\nsim_preds <- tibble::tibble(x = x_pred, y = y_pred)\n```\n:::\n\n\nTo implement local neighborhood prediction, we use the `local` argument to `predict()` (or `augment()`). Setting `local` in `predict()` (or `augment()`) to `TRUE` chooses default local neighborhood prediction settings. We compute local neighborhood predictions at the unobserved locations in `sim_preds` and time it by running\n\n\n::: {.cell}\n\n```{.r .cell-code}\npred_start_time <- proc.time()\nsim_preds$preds <- predict(bdmod, newdata = sim_preds, local = TRUE)\npred_end_time <- proc.time()\npred_end_time - pred_start_time\n#>    user  system elapsed \n#>   14.36    1.03   17.79\n```\n:::\n\n\nThe `3000` predictions are computed in just 17.79 seconds. We visualize them by running\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sim_preds, aes(x = x, y = y, color = preds)) +\n  geom_point() +\n  scale_color_viridis_c(limits = c(-8, 9)) +\n  theme_gray(base_size = 14)\n```\n\n::: {.cell-output-display}\n![Distribution of local neighborhood predictions using the model fit to the large simulated data set.](splm-additional_files/figure-html/fig-simpreds-1.png){#fig-simpreds width=672}\n:::\n:::\n\n\nThese predictions at the unobserved locations closely match the pattern of the observed data.\n\nThe other way to specify `local` in `predict()` (or `augment()`) is via a list object, which offers much more control and customization over the local neighborhood prediction. To learn more, read about `local` in `predict()`'s (or `augment()`'s) help page by running `?predict.spmodel` (or `?augment.spmodel`).\n\n::: {.callout-note}\nMost of the computational burden associated with prediction is actually from the observed data sample size used to fit the model (because an inverse is needed). As long as the observed data sample sizes are a few thousand or fewer, local prediction is not imperative, no matter the size of the prediction data. Note that parallel process can be used whether or not local prediction is implemented.\n:::\n\n::: {.callout-tip}\n`loocv()` also has a `local` argument for large data sets that is structured the same as `local` for `predict()` (and `augment()`).\n:::\n\n## Additional Arguments\n\n### Multiple Models\n\n`splm()` fits multiple models simultaneously when `spcov_type` is a vector with more than one element:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nspmods <- splm(formula = log_Zn ~ log_dist2road, data = moss,\n              spcov_type = c(\"exponential\", \"gaussian\"))\n```\n:::\n\n\n`spmods` is a list with two elements: `exponential`, using the exponential spatial covariance; and `gaussian`, using the Gaussian spatial covariance.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnames(spmods)\n#> [1] \"exponential\" \"gaussian\"\n```\n:::\n\n\n`spmods` is natural to combine with `glances()` to glance at each model fit:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglances(spmods)\n#> # A tibble: 2 × 10\n#>   model      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 expon…   365     2     3  367.  373.  373.  -184.     363.            0.683\n#> 2 gauss…   365     2     3  435.  441.  441.  -218.     363.            0.686\n```\n:::\n\n\nand to combine with `predict()` to predict for each model fit.\n\n::: {.callout-important icon=\"false\"}\n## Exercise\nWork with a neighbor to find 90% confidence intervals for the fixed effects in the Gaussian model using either (1) `tidy()` or (2) `confint()`. Before beginning, decide with your neighbor who will begin working on (1) `tidy()` and who will begin working on (2) `confint()`.\n:::\n\n::: {.callout-important icon=\"false\" collapse=\"true\"}\n## Exercise Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntidy(spmods$gaussian, conf.int = TRUE, conf.level = 0.90)\n#> # A tibble: 2 × 7\n#>   term          estimate std.error statistic p.value conf.low conf.high\n#>   <chr>            <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n#> 1 (Intercept)      9.21     0.196       46.9       0    8.89      9.53 \n#> 2 log_dist2road   -0.519    0.0184     -28.2       0   -0.549    -0.489\nconfint(spmods$gaussian, level = 0.90)\n#>                      5 %       95 %\n#> (Intercept)    8.8853387  9.5310288\n#> log_dist2road -0.5493091 -0.4887111\n```\n:::\n\n:::\n\n### Non-Spatial Random Effects\n\nIn the `moss` data, there are actually some spatial locations that have more than one measurement due to multiple samples being collected at a single location or due to a single sample being tested multiple times in the laboratory. The `sample` variable indexes the spatial location:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmoss\n#> Simple feature collection with 365 features and 7 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: -445884.1 ymin: 1929616 xmax: -383656.8 ymax: 2061414\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 365 × 8\n#>    sample field_dup lab_rep year  sideroad log_dist2road log_Zn\n#>    <fct>  <fct>     <fct>   <fct> <fct>            <dbl>  <dbl>\n#>  1 001PR  1         1       2001  N                 2.68   7.33\n#>  2 001PR  1         2       2001  N                 2.68   7.38\n#>  3 002PR  1         1       2001  N                 2.54   7.58\n#>  4 003PR  1         1       2001  N                 2.97   7.63\n#>  5 004PR  1         1       2001  N                 2.72   7.26\n#>  6 005PR  1         1       2001  N                 2.76   7.65\n#>  7 006PR  1         1       2001  S                 2.30   7.59\n#>  8 007PR  1         1       2001  N                 2.78   7.16\n#>  9 008PR  1         1       2001  N                 2.93   7.19\n#> 10 009PR  1         1       2001  N                 2.79   8.07\n#> # ℹ 355 more rows\n#> # ℹ 1 more variable: geometry <POINT [m]>\n```\n:::\n\n\nWe might expect Zinc concentration to be correlated within a spatial location; therefore, we might want to add `sample` as a non-spatial random effect (here, an intercept random effect) to the model with `log_Zn` as the response and `log_dist2road` as the predictor. The `splm()` function allows non-spatial random effects to be incorporated with the `random` argument, which takes a formula specification that is similar in syntax as the `nlme` [@pinheiro2006mixed] and `lme4` [@bates2015lme4] \\textbf{\\textsf{R}} packages.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrandint <- splm(log_Zn ~ log_dist2road,\n                data = moss, spcov_type = \"exponential\",\n                random = ~ (1 | sample))\n```\n:::\n\n\n:::{.callout-tip}\nFor the `randint` model, in the `random` argument, `sample` is shorthand for `(1 | sample)`. So the `randint` model could be written more concisely as\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrandint <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ sample)\n```\n:::\n\n\n:::\n\nThe summary output now shows an estimate of the variance of the random intercepts, in addition to the estimated fixed effects and estimated spatial covariance parameters.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(randint)\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_type = \"exponential\", \n#>     random = ~sample)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.6234 -1.3228 -0.8026 -0.2642  1.0998 \n#> \n#> Coefficients (fixed):\n#>               Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)    9.66066    0.26770   36.09   <2e-16 ***\n#> log_dist2road -0.55028    0.02071  -26.58   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Pseudo R-squared: 0.6605\n#> \n#> Coefficients (exponential spatial covariance):\n#>        de        ie     range \n#> 3.153e-01 2.094e-02 1.083e+04 \n#> \n#> Coefficients (random effects):\n#> 1 | sample \n#>    0.07995\n```\n:::\n\n\nAnd, `glances()` shows that the model with the random intercepts is a better fit to the data than the model without random intercepts. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nspmod <- splm(log_Zn ~ log_dist2road,\n              data = moss, spcov_type = \"exponential\")\nglances(spmod, randint)\n#> # A tibble: 2 × 10\n#>   model      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 randi…   365     2     4  335.  343.  343.  -168.     363.            0.661\n#> 2 spmod    365     2     3  367.  373.  373.  -184.     363.            0.683\n```\n:::\n\n\nAs another example, we might consider a model that also has random intercepts for `year`, or, a model that also has both random intercepts for `year` and random slopes for `log_dist2road` within `year`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyearint <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ (1 | sample + year))\nyearsl <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ (1 | sample) + \n                       (log_dist2road | year))\n```\n:::\n\n\n`glances()` shows that, of these four models, the model that includes random intercepts for `sample`, random intercepts for `year`, and random slopes for `year` is best, according to the AIC and AICc metrics.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglances(spmod, randint, yearint, yearsl)\n#> # A tibble: 4 × 10\n#>   model      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 yearsl   365     2     6  190.  202.  202.  -94.9     363.            0.215\n#> 2 yeari…   365     2     4  230.  238.  238. -115.      363.            0.729\n#> 3 randi…   365     2     4  335.  343.  343. -168.      363.            0.661\n#> 4 spmod    365     2     3  367.  373.  373. -184.      363.            0.683\n```\n:::\n\n\n::: {.callout-note}\n### Note\n\nThe syntax `~ (log_dist2road | year)` specifies that both random intercepts for `year` and random slopes for `log_dist2road` within `year` should be included in the model. If only random slopes are desired, then we should set `random` to `~ (-1 + log_dist2road | year)`.\n:::\n\n::: {.callout-important icon=\"false\"}\n### Exercise\n\nPerhaps a model with random intercepts for `sample` and random intercepts and slopes for `year` but without any spatial covariance is an even better fit to the data. Fit such a model by specifying `spcov_type` to be `\"none\"`. Then, use `glances()` to see how well this non-spatial model fits the `moss` data compared to the spatially explicit models.\n:::\n\n::: {.callout-important icon=\"false\" collapse=\"true\"}\n### Exercise Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnospcov <- splm(log_Zn ~ log_dist2road,\n                    data = moss, spcov_type = \"none\",\n                    random = ~ (1 | sample) + \n                      (log_dist2road | year))\nglances(spmod, randint, yearint, yearsl, nospcov)\n#> # A tibble: 5 × 10\n#>   model      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 yearsl   365     2     6  190.  202.  202.  -94.9     363.            0.215\n#> 2 yeari…   365     2     4  230.  238.  238. -115.      363.            0.729\n#> 3 randi…   365     2     4  335.  343.  343. -168.      363.            0.661\n#> 4 spmod    365     2     3  367.  373.  373. -184.      363.            0.683\n#> 5 nospc…   365     2     4  456.  464.  464. -228.      363             0.119\n## the model with no explicit spatial covariance has the worst fit \n## of the five models.\n```\n:::\n\n:::\n\n### Anisotropy\n\nBy default, `splm()` uses isotropic spatial covariance. Spatial covariance is isotropic if it behaves similarly in all directions. A spatial covariance is (geometrically) anisotropic if it does not behave similarly in all directions. Anisotropic models require estimation of two additional parameters: `rotate` and `scale`, which control the behavior of the spatial covariance as a function of distance and direction. \n\n\n::: {.cell}\n\n```{.r .cell-code}\naniso <- splm(log_Zn ~ log_dist2road,\n              data = moss, spcov_type = \"exponential\",\n              anisotropy = TRUE)\naniso\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_type = \"exponential\", \n#>     anisotropy = TRUE)\n#> \n#> \n#> Coefficients (fixed):\n#>   (Intercept)  log_dist2road  \n#>         9.548         -0.546  \n#> \n#> \n#> Coefficients (exponential spatial covariance):\n#>        de         ie      range     rotate      scale  \n#> 3.561e-01  6.812e-02  8.732e+03  2.435e+00  4.753e-01\n```\n:::\n\n\nWe can again use glances to compare the model that allows for anisotropy with the isotropic model:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglances(spmod, aniso)\n#> # A tibble: 2 × 10\n#>   model     n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr> <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 aniso   365     2     5  362.  372.  372.  -181.     363             0.705\n#> 2 spmod   365     2     3  367.  373.  373.  -184.     363.            0.683\n```\n:::\n\n\nThe anisotropic model does have lower AIC and AICc than the isotropic model, indicating a better fit. However, the reduction in AIC and AICc is quite small, so we may still prefer the isotropic model for simplicity and interpretability.\n\n::: {.callout-important icon=\"false\"}\n### Exercise\n\nVisualize the anisotropic level curve for `aniso` using `plot()`. Hint: Run `?plot.spmodel` or visit [this link](https://usepa.github.io/spmodel/reference/plot.spmodel.html). Which direction does the model predict two responses will be more correlated?\n\n:::\n\n::: {.callout-important icon=\"false\" collapse=\"true\"}\n### Exercise Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(aniso, which = 8)\n```\n\n::: {.cell-output-display}\n![](splm-additional_files/figure-html/unnamed-chunk-23-1.png){width=672}\n:::\n:::\n\n\nA clockwise rotation of this level curve by `rotate` followed by a scaling of the minor axis by the reciprocal of `scale` yields a spatial covariance that is isotropic.\n\n:::\n\n### Partition Factors\n\nA partition factor is a categorical (or factor) variable that forces observations in different levels of the partition factor to be uncorrelated. The `year` variable in `moss` has two levels, `2001` and `2006`, which correspond to the year of measurement. Suppose the goal is to fit a model that assumes observations from the same year are spatially correlated but observations from different years are not spatially correlated. In this context, `year` is a partition factor. We fit this model by running\n\n\n::: {.cell}\n\n```{.r .cell-code}\npart <- splm(log_Zn ~ log_dist2road,\n             data = moss, spcov_type = \"exponential\",\n             partition_factor = ~ year)\n```\n:::\n\n\nLike the `formula` and `random` arguments, the `partition_factor` argument requires a formula object.\n\n\n\n### Fixing Covariance Parameters\n\nBy default, `splm()` estimates all unknown covariance parameters. However, we can also fix covariance parameters at known values with the `spcov_initial` argument for spatial covariance parameters and with the `randcov_initial` argument for non-spatial covariance parameters.\n\nAs an example, suppose that we want to fit a `\"spherical\"` covariance model to the moss data, but that, we want to fix the `range` at `20000` units so that errors from spatial locations more than 20000 units apart are not spatially correlated. We first create an `spcov_initial` object with the `spcov_initial()` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninit_spher <- spcov_initial(\"spherical\", range = 20000, known = \"range\")\ninit_spher\n#> $initial\n#> range \n#> 20000 \n#> \n#> $is_known\n#> range \n#>  TRUE \n#> \n#> attr(,\"class\")\n#> [1] \"spherical\"\n```\n:::\n\n\nWithin the function call, we specify that, for a `\"spherical\"` covariance, we would like to set the `range` parameter to `20000` and for that value to be known and therefore fixed in any subsequent estimation. We then provide `init_spher` as an argument to `spcov_initial` in `splm()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsplm(log_Zn ~ log_dist2road, data = moss,\n     spcov_initial = init_spher)\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_initial = init_spher)\n#> \n#> \n#> Coefficients (fixed):\n#>   (Intercept)  log_dist2road  \n#>        9.7194        -0.5607  \n#> \n#> \n#> Coefficients (spherical spatial covariance):\n#>        de         ie      range  \n#> 4.545e-01  8.572e-02  2.000e+04\n```\n:::\n\n\nWhen `spcov_initial` is provided, `spcov_type` is not a necessary argument to `splm()`.\n\n::: {.callout-important icon=\"false\"}\n### Exercise\n\nFit a `\"spherical\"` spatial covariance model to the `moss` data set without a nugget effect (i.e., the model should have the `ie` independent variance parameter set to `0` and treated as `known`). Verify in the summary output that the `ie` is indeed `0` for this model.\n\n:::\n\n::: {.callout-important icon=\"false\" collapse=\"true\"}\n### Exercise Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninit_no_ie <- spcov_initial(\"spherical\", ie = 0, known = \"ie\")\nno_ie <- splm(log_Zn ~ log_dist2road, data = moss,\n              spcov_initial = init_no_ie)\nsummary(no_ie)\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_initial = init_no_ie)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.1766 -1.8420 -1.2975 -0.7249  0.6577 \n#> \n#> Coefficients (fixed):\n#>               Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   10.27912   28.99660   0.354    0.723    \n#> log_dist2road -0.56642    0.01974 -28.693   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Pseudo R-squared: 0.6967\n#> \n#> Coefficients (spherical spatial covariance):\n#>        de        ie     range \n#> 8.433e+02 0.000e+00 3.699e+07\n```\n:::\n\n\n:::\n\n### Random Forest Spatial Residual Models\n\nRandom forests are a popular machine-learning modeling tool. The random forest spatial residual model available in `spmodel` combines random forest modeling and spatial linear models. First, the model is fit using random forests and fitted values are obtained. Then the response residuals are used to fit a spatial linear model. Predictions at unobserved locations are computed as the sum of the random forest prediction and the predicted (i.e., Kriged) response residual from the spatial linear model. Suppose we split the `moss` data into training and test data sets, with the goal of predicting `log_Zn` in the test data. \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nn <- NROW(moss)\nn_train <- round(0.75 * n)\nn_test <- n - n_train\ntrain_index <- sample(n, size = n_train)\nmoss_train <- moss[train_index, , ]\nmoss_test <- moss[-train_index, , ]\n```\n:::\n\n\nWe fit a random forest spatial residual model to the test data by running\n\n::: {.cell}\n\n```{.r .cell-code}\nrfsrmod <- splmRF(log_Zn ~ log_dist2road, moss_train,\n                  spcov_type = \"exponential\")\n```\n:::\n\n\nWe make predictions for the test data by running\n\n::: {.cell}\n\n```{.r .cell-code}\n# results omitted\npredict(rfsrmod, moss_test)\n```\n:::\n\n\n::: {.callout-important icon=\"false\"}\n### Exercise\n\nUse `predict()` to store the random forest spatial residual predictions of `log_Zn` at locations in the test data and then compute the mean-squared prediction error (MSPE). Compare this MSPE to the MSPE from fitting a spatial linear model with an exponential covariance function.\n\n:::\n\n::: {.callout-important icon=\"false\" collapse=\"true\"}\n### Exercise Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrf_preds <- predict(rfsrmod, newdata = moss_test)\nrf_errors <- moss_test$log_Zn - rf_preds\nmean(rf_errors^2)\n#> [1] 0.1849228\n\nsplmmod <- splm(log_Zn ~ log_dist2road, moss_train,\n                  spcov_type = \"exponential\")\nsplm_preds <- predict(splmmod, newdata = moss_test)\nsplm_errors <- moss_test$log_Zn - splm_preds\nmean(splm_errors^2)\n#> [1] 0.1514774\n```\n:::\n\n\nFor these data, the spatial linear model yielded more accurate predictions (lower MSPE).\n\n:::\n\n## Areal Data\n\n### Data Introduction\n\nThroughout the section, we will use the `seal` data in the `spmodel` package. The `seal` data is an `sf` object with a `POLYGON` geometry. There are 62 polygons in the data, some of which have non-missing values of `log_trend`, which is the log of the estimated harbor-seal trends that were calculated from abundance data.\n\nThe following code generates a visualization of the `seal` data: polygons that are grey have a missing value for `log_trend`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(seal, aes(fill = log_trend)) +\n  geom_sf() +\n  scale_fill_viridis_c() +\n  theme_bw(base_size = 14) \n```\n\n::: {.cell-output-display}\n![](splm-additional_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nOur goal is to fit a spatial autoregressive model (@eq-splm and @eq-Rareal) to the `log_trend` response variable with the `spautor()` function. Then, we will use the fitted model to predict the `log_trend` for sites where `log_trend` is not recorded.\n\n### `spautor()` Syntax and Output Interpretation\n\nThe syntax for fitting a model to areal data with `spautor()` is very similar to that used for `splm()`. Again, there are generally at least three required arguments:\n\n* `formula`: a formula that describes the relationship between the response variable ($\\mathbf{y}$) and explanatory variables ($\\mathbf{X}$)\n    * `formula` in `spautor()` is the same as `formula` in `lm()` and `splm()`\n* `data`: a `data.frame` or `sf` object that contains the response variable, explanatory variables, and spatial information. Note that if `data` is a `data.frame`, then `W` is an additional required argument to `spautor()`.\n* `spcov_type`: the spatial covariance type (`\"car\"` or `\"sar\"`)\n\nWe can fit a conditional auto-regressive (CAR) model with\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsealmod <- spautor(log_trend ~ 1, data = seal, spcov_type = \"car\")\nsummary(sealmod)\n#> \n#> Call:\n#> spautor(formula = log_trend ~ 1, data = seal, spcov_type = \"car\")\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.34455 -0.10417  0.04410  0.07338  0.20475 \n#> \n#> Coefficients (fixed):\n#>             Estimate Std. Error z value Pr(>|z|)   \n#> (Intercept) -0.07090    0.02497  -2.839  0.00452 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Coefficients (car spatial covariance):\n#>      de   range   extra \n#> 0.03252 0.42037 0.02177\n```\n:::\n\n::: {.cell}\n\n:::\n\n\nWe can relate some of the components in the summary output to the model in @eq-splm and @eq-Rareal: \n\n* the value in the `Estimate` column of the `Coefficients (fixed)` table form $\\boldsymbol{\\hat{\\beta}}$, an estimate of $\\boldsymbol{\\beta}$. \n* the `de` value of 0.033 in the `Coefficients (car spatial covariance)` table is $\\hat{\\sigma}^2_{de}$, which is an estimate of $\\sigma^2_{de}$, the variance of $\\boldsymbol{\\tau}$. \n* the `range` value of 0.42 in the `Coefficients (car spatial covariance)` table is $\\hat{\\phi}$, an estimate of $\\phi$ in @eq-Rareal.\n\n::: {.callout-note}\n### Note\n\nBy default, $\\sigma^2_{ie}$ is assumed to be `0` for autoregressive models and hence, `ie` is omitted from the summary output. \n:::\n\nThough the weight matrix $\\mathbf{W}$ in @eq-Rareal used in the model does not appear in the summary output, we can pull the weight matrix from the `sealmod` object with\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsealmod$W\n```\n:::\n\n\nBy default, `spautor()` uses __queen contiguity__ to form the weight matrix: observations are \"neighbors\" if they share at least one boundary (even if that boundary is a single point). Recall that observations are not considered neighbors with themselves. Also by default, `spautor()` row standardizes the weight matrix so that each of the rows in $\\mathbf{W}$ sum to $1$. Row standardization of the weight matrix is performed by default because doing so results in \"nice\" properties of the resulting covariance matrix [@ver2018spatial]. The first row of the weight matrix is \n\n\n::: {.cell}\n\n```{.r .cell-code}\nsealmod$W[1, ]\n#>  [1] 0.0000000 0.3333333 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#>  [8] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [15] 0.0000000 0.3333333 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [22] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.3333333\n#> [29] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [36] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [43] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [50] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [57] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n```\n:::\n\n\nThe output indicates that the first observation is neighbors with the second observation, the sixteenth observation, and the twenty-eighth observation. \n\n::: {.cell}\n\n```{.r .cell-code}\nwhich(sealmod$W[1, ] > 0)\n#> [1]  2 16 28\n```\n:::\n\n\n\nFinally, if we re-examine $\\mathbf{W}$, we can note that some rows of $\\mathbf{W}$ do not have any positive values, indicating that some observations in the data have no neighbors. Looking back on the plot of the data, we see that there are indeed a few \"island\" sites that do not share a boundary with any other polygons. The errors for these spatial locations are assumed to be uncorrelated with all other random errors, and, they are given a unique variance parameter that is the `extra` spatial covariance estimate in the summary output of the model.\n\n### Additional Analysis\n\nMost of the helper functions for models fit with `splm()` are also useful for models fit with `spautor()`. Additionally, most of the additional arguments for `splm()` are also additional arguments for `spautor()`. \n\nAll helper functions available for `splm()` model objects are also available for `spautor()` model objects:\n\n* `augment()`, `glance()`, and `glances()`\n* model fit statistics with `AIC()`, `AICc()` and `GR2()`\n* model diagnostics statistics with `cooks.distance()`, `residuals()`, `fitted()`, etc.\n\n`spautor()` model objects accommodate all additional arguments previously mentioned except big data sets and anisotropy.\n\n::: {.callout-note}\n### Note\n\nBig data applications are not available because the models are parameterized in terms of their inverse covariance matrix, not the covariance matrix, which makes the \"local\" approach infeasible. The `anisotropy` argument is not available for `spautor()` because the covariance for an autoregressive model is based on the neighborhood structure of the spatial locations, not on distance.\n:::\n\n\n::: {.callout-important icon=\"false\"}\n### Exercise\n\nChoose a couple of the helper functions that you would like to explore and apply those functions to the fitted seal model.\n\n:::\n\n::: {.callout-important icon=\"false\" collapse=\"true\"}\n### Exercise Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nAIC(sealmod)\n#> [1] -30.87584\nfitted(sealmod)\n#>           2           3           4           5           6           7 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>           8          10          11          12          14          16 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>          17          20          21          22          23          24 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>          25          26          28          29          30          31 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>          33          34          35          37          38          39 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>          41          45          59          60 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121\n```\n:::\n\n\n:::\n\n### Prediction with Areal Data\n\nPrediction of response values for unobserved polygons with areal data requires that the polygons with missing response values be included in the `data` argument supplied to `spautor()`. The reason for this requirement is that exclusion of these polygons changes the underlying neighborhood structure of the data, and, therefore changes the covariance matrix.\n\nFor areal data, we can obtain predictions for unobserved polygons using `predict()` on the fitted model object or `augment()` on the fitted model object, specifying the `newdata` argument to be `mod$newdata`. Both approaches are given below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsealmod <- spautor(log_trend ~ 1, data = seal, spcov_type = \"car\")\nsummary(sealmod)\n\npredict(sealmod)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(sealmod, newdata = sealmod$newdata)\n#> Simple feature collection with 28 features and 2 fields\n#> Geometry type: POLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: 913618.8 ymin: 1007542 xmax: 1115097 ymax: 1132682\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 28 × 3\n#>    log_trend  .fitted                                                geometry\n#>  *     <dbl>    <dbl>                                           <POLYGON [m]>\n#>  1        NA -0.115   ((1035002 1054710, 1035002 1054542, 1035002 1053542, 1…\n#>  2        NA -0.00908 ((1043093 1020553, 1043097 1020550, 1043101 1020550, 1…\n#>  3        NA -0.0602  ((1099737 1054310, 1099752 1054262, 1099788 1054278, 1…\n#>  4        NA -0.0359  ((1099002 1036542, 1099134 1036462, 1099139 1036431, 1…\n#>  5        NA -0.0723  ((1076902 1053189, 1076912 1053179, 1076931 1053179, 1…\n#>  6        NA -0.0548  ((1070501 1046969, 1070317 1046598, 1070308 1046542, 1…\n#>  7        NA -0.0976  ((1072995 1054942, 1072996 1054910, 1072997 1054878, 1…\n#>  8        NA -0.0714  ((960001.5 1127667, 960110.8 1127542, 960144.1 1127495…\n#>  9        NA -0.0825  ((1031308 1079817, 1031293 1079754, 1031289 1079741, 1…\n#> 10        NA -0.0592  ((998923.7 1053647, 998922.5 1053609, 998950 1053631, …\n#> # ℹ 18 more rows\n```\n:::\n\n\n::: {.callout-note}\n### Note\n\nThe `mod$newdata` syntax also works for models fit with `splm()`, where the `data` used contains missing values for the response variable at any unobserved locations.\n:::\n\n\n::: {.callout-important icon=\"false\"}\n### Exercise\n\nVerify that the fitted autoregressive model with the `seal` data changes when the polygons with missing response values are excluded from the `data` argument in `spautor()`. The following code creates a data without the polygons with missing values:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nis_missing <- is.na(seal$log_trend)\nseal_nomiss <- seal[!is_missing, , ]\n```\n:::\n\n\n:::\n\n::: {.callout-important icon=\"false\" collapse=\"true\"}\n### Exercise Solution\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsealmod_nomiss <- spautor(log_trend ~ 1,\n                          data = seal_nomiss, spcov_type = \"car\")\nprint(sealmod)\n#> \n#> Call:\n#> spautor(formula = log_trend ~ 1, data = seal, spcov_type = \"car\")\n#> \n#> \n#> Coefficients (fixed):\n#> (Intercept)  \n#>     -0.0709  \n#> \n#> \n#> Coefficients (car spatial covariance):\n#>      de    range    extra  \n#> 0.03252  0.42037  0.02177\nprint(sealmod_nomiss)\n#> \n#> Call:\n#> spautor(formula = log_trend ~ 1, data = seal_nomiss, spcov_type = \"car\")\n#> \n#> \n#> Coefficients (fixed):\n#> (Intercept)  \n#>    -0.08152  \n#> \n#> \n#> Coefficients (car spatial covariance):\n#>      de    range    extra  \n#> 0.02297  0.41280  0.01958\n```\n:::\n\n\n:::\n\n## Simulating Spatial Gaussian Data {#sec-simulate-gauss}\n\nWe simulate Gaussian spatial data using `sprnorm()`. `sprnorm()` is similar in structure to `rnorm()` for simulating non-spatial Gaussian data. The first argument to `sprnorm()` is `spcov_params`, which is a spatial covariance parameter object created with `spcov_params()`:\n\n::: {.cell}\n\n```{.r .cell-code}\nparams <- spcov_params(\"exponential\", de = 1, ie = 0.5, range = 5e5)\n```\n:::\n\n\n::: {.callout-note}\n When the `type` argument to `coef()` is `\"spcov\"`, the estimated spatial covariance parameters are returned as an `spcov_params` object, naturally usable simulation-based contexts that require conditioning on these estimated parameters.\n:::\n\n`sprnorm()` simulates data at each location in `data` for each of `n` samples (specified via `n`) with some mean vector (specified via `mean`). We simulate one realization of zero-mean Gaussian data with spatial covariance structure from `params` at each location in the `sulfate` data by running\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1)\nsulfate$z <- sprnorm(params, data = sulfate)\n```\n:::\n\n\nWe visualize this realization by running\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(sulfate, aes(color = z)) +\n  geom_sf() +\n  scale_color_viridis_c() +\n  theme_gray(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](splm-additional_files/figure-html/unnamed-chunk-44-1.png){width=672}\n:::\n:::\n\n\nWe visualize an empirical semivariogram of this realization by running\n\n::: {.cell}\n\n```{.r .cell-code}\nesv_out <- esv(z ~ 1, sulfate)\nggplot(esv_out, aes(x = dist, y = gamma, size = np)) +\n  geom_point() +\n  lims(y = c(0, NA)) +\n  theme_gray(base_size = 14)\n```\n\n::: {.cell-output-display}\n![](splm-additional_files/figure-html/unnamed-chunk-45-1.png){width=672}\n:::\n:::\n\n\n::: {.callout-caution}\n### Caution\n\nSimulating spatial data in `spmodel` requires the inverse (more rigorously, the Cholesky decomposition) of the covariance matrix, which can take awhile for sample sizes exceeding 10,000. Regardless of the number of realizations simulated, this inverse is only needed once, which means that simulating many realizations (via `samples`) takes nearly the same time as simulating just one.\n\n:::\n\n\n## R Code Appendix\n\n\n::: {.cell}\n\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(spmodel)\nlibrary(ggplot2)\nset.seed(06022024)\nsim_params <- spcov_params(\"exponential\", de = 7, ie = 2, range = 0.7)\n\nn <- 5000\nx <- runif(n)\ny <- runif(n)\nsim_coords <- data.frame(x, y)\n\nsim_response <- sprnorm(sim_params, data = sim_coords, xcoord = x, ycoord = y)\nsim_data <- data.frame(sim_coords, sim_response)\nggplot(sim_data, aes(x = x, y = y, color = sim_response)) +\n  geom_point() +\n  scale_color_viridis_c(limits = c(-8, 9)) +\n  theme_gray(base_size = 14)\nfit_start_time <- proc.time()\nbdmod <- splm(sim_response ~ 1, data = sim_data,\n     spcov_type = \"exponential\",\n     xcoord = x, ycoord = y,\n     local = TRUE)\nfit_end_time <- proc.time()\nfit_end_time - fit_start_time\nsummary(bdmod)\nn_pred <- 3000\nx_pred <- runif(n_pred)\ny_pred <- runif(n_pred)\nsim_preds <- tibble::tibble(x = x_pred, y = y_pred)\npred_start_time <- proc.time()\nsim_preds$preds <- predict(bdmod, newdata = sim_preds, local = TRUE)\npred_end_time <- proc.time()\npred_end_time - pred_start_time\nggplot(sim_preds, aes(x = x, y = y, color = preds)) +\n  geom_point() +\n  scale_color_viridis_c(limits = c(-8, 9)) +\n  theme_gray(base_size = 14)\nspmods <- splm(formula = log_Zn ~ log_dist2road, data = moss,\n              spcov_type = c(\"exponential\", \"gaussian\"))\nnames(spmods)\nglances(spmods)\ntidy(spmods$gaussian, conf.int = TRUE, conf.level = 0.90)\nconfint(spmods$gaussian, level = 0.90)\nmoss\nrandint <- splm(log_Zn ~ log_dist2road,\n                data = moss, spcov_type = \"exponential\",\n                random = ~ (1 | sample))\nrandint <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ sample)\nsummary(randint)\nspmod <- splm(log_Zn ~ log_dist2road,\n              data = moss, spcov_type = \"exponential\")\nglances(spmod, randint)\nyearint <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ (1 | sample + year))\nyearsl <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ (1 | sample) + \n                       (log_dist2road | year))\nglances(spmod, randint, yearint, yearsl)\nnospcov <- splm(log_Zn ~ log_dist2road,\n                    data = moss, spcov_type = \"none\",\n                    random = ~ (1 | sample) + \n                      (log_dist2road | year))\nglances(spmod, randint, yearint, yearsl, nospcov)\n## the model with no explicit spatial covariance has the worst fit \n## of the five models.\naniso <- splm(log_Zn ~ log_dist2road,\n              data = moss, spcov_type = \"exponential\",\n              anisotropy = TRUE)\naniso\nglances(spmod, aniso)\nplot(aniso, which = 8)\npart <- splm(log_Zn ~ log_dist2road,\n             data = moss, spcov_type = \"exponential\",\n             partition_factor = ~ year)\ninit_spher <- spcov_initial(\"spherical\", range = 20000, known = \"range\")\ninit_spher\nsplm(log_Zn ~ log_dist2road, data = moss,\n     spcov_initial = init_spher)\ninit_no_ie <- spcov_initial(\"spherical\", ie = 0, known = \"ie\")\nno_ie <- splm(log_Zn ~ log_dist2road, data = moss,\n              spcov_initial = init_no_ie)\nsummary(no_ie)\nset.seed(1)\nn <- NROW(moss)\nn_train <- round(0.75 * n)\nn_test <- n - n_train\ntrain_index <- sample(n, size = n_train)\nmoss_train <- moss[train_index, , ]\nmoss_test <- moss[-train_index, , ]\nrfsrmod <- splmRF(log_Zn ~ log_dist2road, moss_train,\n                  spcov_type = \"exponential\")\n# results omitted\npredict(rfsrmod, moss_test)\nrf_preds <- predict(rfsrmod, newdata = moss_test)\nrf_errors <- moss_test$log_Zn - rf_preds\nmean(rf_errors^2)\n\nsplmmod <- splm(log_Zn ~ log_dist2road, moss_train,\n                  spcov_type = \"exponential\")\nsplm_preds <- predict(splmmod, newdata = moss_test)\nsplm_errors <- moss_test$log_Zn - splm_preds\nmean(splm_errors^2)\nggplot(seal, aes(fill = log_trend)) +\n  geom_sf() +\n  scale_fill_viridis_c() +\n  theme_bw(base_size = 14) \nsealmod <- spautor(log_trend ~ 1, data = seal, spcov_type = \"car\")\nsummary(sealmod)\nspcov_params_car <- coef(sealmod, type = \"spcov\")\nde_car <- as.vector(round(spcov_params_car[[\"de\"]], digits = 3))\nrange_car <- as.vector(round(spcov_params_car[[\"range\"]], digits = 3))\nsealmod$W\nsealmod$W[1, ]\nwhich(sealmod$W[1, ] > 0)\nAIC(sealmod)\nfitted(sealmod)\nsealmod <- spautor(log_trend ~ 1, data = seal, spcov_type = \"car\")\nsummary(sealmod)\n\npredict(sealmod)\naugment(sealmod, newdata = sealmod$newdata)\nis_missing <- is.na(seal$log_trend)\nseal_nomiss <- seal[!is_missing, , ]\nsealmod_nomiss <- spautor(log_trend ~ 1,\n                          data = seal_nomiss, spcov_type = \"car\")\nprint(sealmod)\nprint(sealmod_nomiss)\nparams <- spcov_params(\"exponential\", de = 1, ie = 0.5, range = 5e5)\nset.seed(1)\nsulfate$z <- sprnorm(params, data = sulfate)\nggplot(sulfate, aes(color = z)) +\n  geom_sf() +\n  scale_color_viridis_c() +\n  theme_gray(base_size = 14)\nesv_out <- esv(z ~ 1, sulfate)\nggplot(esv_out, aes(x = dist, y = gamma, size = np)) +\n  geom_point() +\n  lims(y = c(0, NA)) +\n  theme_gray(base_size = 14)\n```\n:::\n",
    "supporting": [
      "splm-additional_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}