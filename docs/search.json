[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Spatial Analysis and Statistical Modeling with R and spmodel",
    "section": "",
    "text": "Welcome\nHello 👋 and welcome! The purpose of this website is to provide workshop materials for the “Spatial Analysis and Statistical Modeling with R and spmodel” workshop at the 2024 Society for Freshwater Science Conference in Philadelphia, PA, USA. To view the workshop’s accompanying workbook, visit here. To download the workshop’s slides, visit here. Slides are downloaded by clicking the “Download raw file” button via the ellipsis or downward arrow symbol on the right side of the screen.\nIt is our hope that this workshop will provide attendees with the conceptual and practical knowledge to incorporate geospatial analyses into their R workflow to improve the interpretation of spatial data. In particular, this workshop will focus on the R package spmodel.\nWhat is spmodel? The spmodel R package (Dumelle, Higham, and Ver Hoef 2023) can be used to fit, summarize, and predict for a variety of spatial statistical models. Some of the things that spmodel can do include:\nWhy use spmodel? There are many great spatial modeling packages in R. A few reasons to use spmodel for spatial analysis are that:\nThroughout the rest of these materials, we introduce spmodel through a few applied examples. We connect basic summary output with the spatial linear model for both point-referenced and areal (lattice) data. We discuss prediction and generalized linear spatial models for response variables whose distribution is not Gaussian. Along the way, we mention a few other advanced spmodel features. Then we show how to use spmodel to analyze several real freshwater data sets.\nWorkshop Summary. The spmodel R package can be used to fit, summarize, and predict for a variety of spatial statistical models for both point-referenced and areal spatial data. What distinguishes spmodel from many other R packages for modeling spatial data is (1) a syntactic structure similar to the syntactic structure of base R functions lm() and glm() that makes spmodel relatively easy to learn, (2) the breadth of options that give the user a high amount of control over the model being fit, and (3) compatibility with other modern R packages like broom and sf. By the end of this workshop, participants can expect to be able to use spmodel to fit spatial linear models for point-referenced and areal (lattice) data, make predictions for unobserved spatial locations, fit anisotropic models for point-referenced data, fit spatial models with additional non-spatial random effects, fit generalized linear models for spatial data, and use big data methods to analyze large spatial data sets. More information on spmodel can be found on our website at https://usepa.github.io/spmodel/."
  },
  {
    "objectID": "index.html#workshop-agenda",
    "href": "index.html#workshop-agenda",
    "title": "Spatial Analysis and Statistical Modeling with R and spmodel",
    "section": "Workshop Agenda",
    "text": "Workshop Agenda\n\n9:00am - 10:00am EDT: XYZ\n10:00am - 11:00am EDT: XYZ"
  },
  {
    "objectID": "index.html#author-introduction",
    "href": "index.html#author-introduction",
    "title": "Spatial Analysis and Statistical Modeling with R and spmodel",
    "section": "Author Introduction",
    "text": "Author Introduction\nMichael Dumelle (he/him/his) is a statistician for the United States Environmental Protection Agency (USEPA). He works primarily on facilitating the survey design and analysis of USEPA’s National Aquatic Resource Surveys (NARS), which characterize the condition of waters across the United States. His primary research interests are in spatial statistics, survey design, environmental and ecological applications, and software development.\nRyan Hill (he/him/his) is an aquatic ecologist with the U.S. EPA Office of Research and Development. He is interested in how watershed conditions drive differences in freshwater diversity and water quality across the United States. He has worked extensively with federal physical, chemical, and biological datasets to gain insights into the factors affecting water quality and biotic condition of freshwaters across the conterminous US. He has also worked to develop and distribute large datasets of geospatial watershed metrics of streams and lakes for the Agency (EPA’s StreamCat and LakeCat datasets)."
  },
  {
    "objectID": "index.html#support-introduction",
    "href": "index.html#support-introduction",
    "title": "Spatial Analysis and Statistical Modeling with R and spmodel",
    "section": "Support Introduction",
    "text": "Support Introduction\nLara Jansen (she/her/hers) is an aquatic community ecologist and an ORISE postdoctoral fellow working on predictive models of benthic macroinvertebrate communities across the conterminous US in relation to watershed factors. Lara completed her PhD in Environmental Science at Portland State University in 2023, studying the drivers and dynamics of harmful algal blooms in mountain lakes with Dr. Angela Strecker.She obtained a MS in Natural Resource Sciences at Cal Poly Humboldt University with a thesis focused on the downstream impacts of dam flow regulation on benthic macroinvertebrate and algal communities.\nWade Boys (he/him/his) is a graduate student at the University of Arkansas broadly interested in understanding how aquatic ectotherms will respond to climate change, especially the role of phenotypic plasticity in adapting to a warming world. Wade is a firm believer that science is not finished until it is communicated. In addition to research, he finds great purpose in cultivating community and connecting science to our everyday experiences as humans."
  },
  {
    "objectID": "index.html#setup",
    "href": "index.html#setup",
    "title": "Spatial Analysis and Statistical Modeling with R and spmodel",
    "section": "Setup",
    "text": "Setup\nInstall R and RTools\nPrior to the start of the workshop everyone will need to have the software installed and tested. You will need to have R, Rtools, and RStudio. Please install at least R version >= 4.0 and the compatible version of RTools.RTools will be necessary to install non-CRAN repositories from GitHub (see below).\nInstall RStudio\nThere are many graphical user interfaces (GUIs) that make it easier to interact with R. We recommend using Posit’s RStudio](https://posit.co/products/open-source/rstudio/).\nInstall R Packages\nThe packages that we use throughout this workshop are listed below. To install them run:\nCommon Packages\n\ninstall.packages('tidyverse')\ninstall.packages('ggplot2')\ninstall.packages('data.table')\ninstall.packages('tictoc')\ninstall.packages(\"remotes\")\ninstall.packages(\"devtools\")\n\nGeospatial Packages\nIn addition to these core R packages, we’ll be using several CRAN packages specifically developed for GIS tasks or handling/obtaining spatial data:\n\ninstall.packages('sf')\ninstall.packages('terra')\ninstall.packages('prism')\ninstall.packages('tigris')\ninstall.packages('nhdplusTools')\ninstall.packages('mapview')\ninstall.packages('FedData')\ninstall.packages('tidyterra')\ninstall.packages('jsonlite')\ninstall.packages('geojson')\ninstall.packages('geojsonio')\ninstall.packages('maps')\n\nNew EPA Packages\nFinally, we will use three new R packages developed by researchers at the U.S. Environmental Protection Agency. The package spmodel, in particular, will form the basis of this workshop.\nspmodel - The spmodel package is the basis of this workshop.\n\n\n\n\nStreamCatTools - The StreamCatTools package retrieves StreamCat and LakeCat data via an API.\n\n\n\n\nfinsyncR - The finsyncR package greatly facilitates the retrieval and harmonization of EPA and USGS stream macroinvertebrate and fish data.\n\n\n\n\nTo install these packages:\n\n# Install spmodel from CRAN\ninstall.packages('spmodel')\n\n# Install remotes package to allow GitHub installation of finsyncR and StreamCatTools\nremotes::install_github('USEPA/finsyncR')\nremotes::install_github('USEPA/StreamCatTools')"
  },
  {
    "objectID": "index.html#how-to-follow-along-with-material",
    "href": "index.html#how-to-follow-along-with-material",
    "title": "Spatial Analysis and Statistical Modeling with R and spmodel",
    "section": "How to follow along with material",
    "text": "How to follow along with material\nThis workshop was built using Quarto and rendered to html. If you are familiar with using git and GitHub, you can fork and clone this repository, or simply clone directly and open the corresponding .qmd files to follow along with material in RStudio. You can also copy code snippets from the rendered book site and paste into your code files in RStudio."
  },
  {
    "objectID": "index.html#citation-information",
    "href": "index.html#citation-information",
    "title": "Spatial Analysis and Statistical Modeling with R and spmodel",
    "section": "Citation Information",
    "text": "Citation Information\nIf you use these software packages in a formal report or publication, please cite them. For example, the spmodel citation is available by running:\n\ncitation(package = \"spmodel\")\n\n\nTo cite spmodel in publications use:\n\n  Dumelle M, Higham M, Ver Hoef JM (2023). spmodel: Spatial statistical\n  modeling and prediction in R. PLOS ONE 18(3): e0282524.\n  https://doi.org/10.1371/journal.pone.0282524\n\nA BibTeX entry for LaTeX users is\n\n  @Article{,\n    title = {{spmodel}: Spatial statistical modeling and prediction in {R}},\n    author = {Michael Dumelle and Matt Higham and Jay M. {Ver Hoef}},\n    journal = {PLOS ONE},\n    year = {2023},\n    volume = {18},\n    number = {3},\n    pages = {1--32},\n    doi = {10.1371/journal.pone.0282524},\n    url = {https://doi.org/10.1371/journal.pone.0282524},\n  }"
  },
  {
    "objectID": "index.html#disclaimer",
    "href": "index.html#disclaimer",
    "title": "Spatial Analysis and Statistical Modeling with R and spmodel",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe views expressed in this manuscript are those of the authors and do not necessarily represent the views or policies of the U.S. Environmental Protection Agency or the U.S. National Oceanic and Atmospheric Administration. Any mention of trade names, products, or services does not imply an endorsement by the U.S. government, the U.S. Environmental Protection Agency, or the U.S. National Oceanic and Atmospheric Administration. The U.S. Environmental Protection Agency and the U.S. National Oceanic and Atmospheric Administration do not endorse any commercial products, services, or enterprises.\n\n\n\n\nDumelle, Michael, Matt Higham, and Jay M. Ver Hoef. 2023. “spmodel: Spatial Statistical Modeling and Prediction in R.” PLOS ONE 18 (3): 1–32. https://doi.org/10.1371/journal.pone.0282524."
  },
  {
    "objectID": "splm.html#the-spatial-linear-model",
    "href": "splm.html#the-spatial-linear-model",
    "title": "1  Spatial Linear Models in spmodel",
    "section": "\n1.1 The Spatial Linear Model",
    "text": "1.1 The Spatial Linear Model\n\n1.1.1 Reviewing the Nonspatial Linear Model\nBefore we describe the spatial linear model, we review nonspatial linear models, which many of us are already familiar with (whether we realize it or not). They incredibly flexible statistical tools that encompass all sorts of model types. In fact, multiple linear regression, analysis of variance (ANOVA), splines, polynomial regression, additive models, and mixed effect models are all linear models! Linear models are designed to relate a response variable (i.e., dependent variable) to one or more explanatory variables (i.e., independent variable, predictor variable, covariate) while accounting for random error. More formally, the linear model may be written (for a single observation) as \\[\n\\begin{split} \\text{y} & = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\dots + \\beta_k + \\epsilon \\\\\ni & = 1, 2, \\dots, k\n\\end{split}\n\\tag{1.1}\\]\nwhere \\(\\text{y}\\) is the value of the response variable, \\(\\beta_0\\) is the overall intercept, \\(\\beta_i\\) is the \\(i\\)th fixed effect (sometimes called a “slope”) parameter, which captures the average effect on \\(\\text{y}\\) resulting from a one-unit increase in \\(x_i\\), the value of the \\(i\\)th (of \\(k\\)) explanatory variable, and \\(\\epsilon\\) is random error. Generalizing this model to \\(n\\) distinct observations yields \\[\n\\begin{split} \\text{y}_j & = \\beta_0 + \\beta_1 x_{1, j} + \\beta_2 x_{2, j} + \\dots + \\beta_k x_{k, j} + \\epsilon_j \\\\\ni & = 1, 2, \\dots, k \\\\\nj & = 1, 2, \\dots, n\n\\end{split}\n\\tag{1.2}\\] where the model terms from Equation 1.1 are now indexed via a subscript \\(j\\) that ranges from one to \\(n\\), the sample size. The index \\(i\\) still ranges from one to \\(k\\), the number of explanatory variables. Linear models are commonly fit in R using the lm() function.\nThe model in Equation 1.2 is sometimes written in matrix notation instead of index notation. Let\n\\[\n\\mathbf{y} = \\begin{bmatrix}\n  \\text{y}_1 \\\\\n  \\text{y}_2 \\\\\n  \\vdots \\\\\n  \\text{y}_j \\\\\n\\end{bmatrix},\n\\mathbf{X} =\n\\begin{bmatrix}\n1 & x_{1, 1} & x_{1, 2} & \\dots & x_{1, k} \\\\\n1 & x_{2, 1} & x_{2, 2} & \\dots & x_{2, k} \\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n1 & x_{n, 1} & x_{n, 2} & \\dots & x_{n, k} \\\\\n\\end{bmatrix},\n\\boldsymbol{\\beta} =\n\\begin{bmatrix}\n\\beta_0 \\\\\n\\beta_1 \\\\\n\\vdots \\\\\n\\beta_k\n\\end{bmatrix}, \\text{ and }\n\\boldsymbol{\\epsilon} =\n\\begin{bmatrix}\n\\epsilon_0 \\\\\n\\epsilon_1 \\\\\n\\vdots \\\\\n\\epsilon_j\n\\end{bmatrix},\n\\] where for a sample size \\(n\\), \\(\\mathbf{y}\\) is the \\(n \\times 1\\) column vector of response variables, \\(\\mathbf{X}\\) is the \\(n \\times p\\) matrix of explanatory variables (sometimes called the “design” or “model” matrix), \\(\\boldsymbol{\\beta}\\) is the \\(p \\times 1\\) column vector of fixed effects, and \\(\\boldsymbol{\\epsilon}\\) is the \\(n \\times 1\\) column vector of random errors.\nThen Equation 1.2 in matrix notation is written as \\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}.\n\\tag{1.3}\\]\nThere are typically a few assumptions inherent in models built using Equation 1.3. First, we typically assume that \\(\\text{E}(\\boldsymbol{\\epsilon}) = \\mathbf{0}\\), where \\(\\text{E}(\\cdot)\\) denotes expectation. Less formally, this means that the average of all random errors is zero. Second, we typically assume \\(\\text{Cov}(\\boldsymbol{\\epsilon}) = \\sigma^2_\\epsilon \\mathbf{I}\\), where \\(\\text{Cov}(\\cdot)\\) denotes covariance, \\(\\sigma^2_\\epsilon\\) denotes a variance parameter, and \\(\\mathbf{I}\\) denotes the identity matrix. Less formally, this means that each random error is independent of other random errors (i.e., we gain no information about observation two’s random error by knowing observation one’s random error). Moreover, we also usually assume these random errors are normally distributed.\n\n\n\n\n\n\nLinear Modeling Misconceptions\n\n\n\nMisconception one: That linear models can only be used if there are linear relationships between a response variable and an explanatory variable – this is untrue! Linear models can be constructed to handle nonlinear relationships by leveraging special tools like splines, polynomial regression, and additive models.\nMisconception two: That linear models can only be used if the response variable is normally distributed – this is untrue! Our normality assumption is on the random errors, not the response itself (this is a very important point). We can investigate the plausibility of normally distributed random errors by investigating the residuals of a model (our best guess at these random errors).\n\n\n\n1.1.2 Introducing the Spatial Linear Model\nAs mentioned previously, nonspatial linear models are very flexible tools that can capture all sorts of interesting processes. Unfortunately, they do assume that random errors are independent of one another, often unreasonable for spatial data which tend to be correlated in space. Ignoring this spatial dependence and fitting nonspatial linear models generally leads to invalid fixed effect inference and poor predictions. Spatial linear models leverage spatial dependence in the random error structure of a linear model, creating models that more accurately reflect the spatial process in study and perform substantially better than nonspatial linear models. So why have they not been more commonly used in ecological settings? These models are challenging to implement, both theoretically and computationally, and software has not always been available that uses standard R linear modeling conventions. Fortunately, incorporating spatial dependence is now straightforward using spmodel, which we discuss shortly.\nMore formally, we accommodate spatial dependence in linear models by adding an \\(n \\times 1\\) spatial random effect, \\(\\boldsymbol{\\tau}\\), to Equation 1.3, yielding the model\n\\[\n\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\tau} + \\boldsymbol{\\epsilon},\n\\tag{1.4}\\]\nwhere \\(\\boldsymbol{\\tau}\\) is independent of \\(\\boldsymbol{\\epsilon}\\), \\(\\text{E}(\\boldsymbol{\\tau}) = \\mathbf{0}\\), \\(\\text{Cov}(\\boldsymbol{\\tau}) = \\sigma^2_\\tau \\mathbf{R}\\), and \\(\\mathbf{R}\\) is a matrix that determines the spatial dependence structure in \\(\\mathbf{y}\\) and depends on a range parameter, \\(\\phi\\), which controls the behavior of the spatial covariance as a function of distance. We discuss \\(\\mathbf{R}\\) in more detail shortly. The parameter \\(\\sigma^2_\\tau\\) is called the spatially dependent random error variance or partial sill. The parameter \\(\\sigma^2_\\epsilon\\) is called the spatially independent random error variance or nugget. These two variance parameters are henceforth more intuitively written as \\(\\sigma^2_{de}\\) and \\(\\sigma^2_{ie}\\), respectively. The covariance of \\(\\mathbf{y}\\) is denoted \\(\\boldsymbol{\\Sigma}\\) and given by \\(\\sigma^2_{de} \\mathbf{R} + \\sigma^2_{ie} \\mathbf{I}\\). The parameters that compose this covariance are contained in the vector \\(\\boldsymbol{\\theta}\\), which is called the covariance parameter vector.\nEquation 1.4 is called the spatial linear model. The spatial linear model applies to both point-referenced and areal (i.e., lattice) data. Spatial data are point-referenced when the elements in \\(\\mathbf{y}\\) are observed at point-locations indexed by x-coordinates and y-coordinates on a spatially continuous surface with an infinite number of locations. For example, consider sampling soil at any point-location in a field. Spatial data are areal when the elements in \\(\\mathbf{y}\\) are observed as part of a finite network of polygons whose connections are indexed by a neighborhood structure. For example, the polygons may represent states in a country who are neighbors if they share at least one boundary.\n\n1.1.3 Modeling Covariance in the Spatial Linear Model\nA primary way in which the model in Equation 1.4 differs for point-referenced and areal data is the way in which \\(\\mathbf{R}\\) in \\(\\text{Cov}(\\boldsymbol{\\tau}) = \\sigma^2_{de} \\mathbf{R}\\) is modeled. For point-referenced data, the \\(\\mathbf{R}\\) matrix is generally constructed using the Euclidean distance between spatial locations. For example, the exponential spatial covariance function generates an \\(\\mathbf{R}\\) matrix given by\n\\[\n\\mathbf{R} = \\exp(-\\mathbf{H} / \\phi),\n\\tag{1.5}\\]\nwhere \\(\\mathbf{H}\\) is a matrix of Euclidean distances among observations and \\(\\phi\\) is the range parameter. Some spatial covariance functions have an extra parameter – one example is the Matérn covariance. Spatial models for point-referenced data are fit in spmodel using the splm() function.\nOn the other hand, \\(\\mathbf{R}\\) for areal data is often constructed from how the areal polygons are oriented in space. Commonly, a neighborhood structure is used to construct \\(\\mathbf{R}\\), where two observations are considered to be “neighbors” if they share a common boundary. In the simultaneous auto-regressive (SAR) model,\n\\[\n\\mathbf{R} = [(\\mathbf{I} - \\phi \\mathbf{W}) (\\mathbf{I} - \\phi \\mathbf{W}^\\top)]^{-1}\n\\tag{1.6}\\]\nwhere \\(\\mathbf{I}\\) is the identity matrix and \\(\\mathbf{W}\\) is a weight matrix that describes the neighborhood structure among observations. A popular neighborhood structure is queen contiguity, in which two polygons are neighbors if they share a boundary. It is important to clarify that observations are not considered neighbors with themselves. Spatial models for areal data are fit in spmodel using the spautor() function.\n\n\n\n\n\n\nExercise\n\n\n\nNavigate to the Help file for splm by running ?splm or by visiting this link and scroll down to “Details.” Examine the spatial linear model description in the Help file and relate some of the syntax used to the syntax in Equation 1.4 and Equation 1.5.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe form of the spatial linear model (\\(\\mathbf{y} = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\tau} + \\boldsymbol{\\epsilon}\\)) is the same in the Help file as the form in Equation Equation 1.4. In the help file, \\(de\\) refers to \\(\\sigma^2_{de}\\), \\(ie\\) refers to \\(\\sigma^2_{ie}\\), and \\(range\\) refers to \\(\\phi\\). Finally, in the help file \\(h\\) refers to distance between observations while, in Equation 1.5, \\(\\mathbf{H}\\) refers to a matrix of these distances for all pairs of observations."
  },
  {
    "objectID": "splm.html#model-fitting",
    "href": "splm.html#model-fitting",
    "title": "1  Spatial Linear Models in spmodel",
    "section": "\n1.2 Model Fitting",
    "text": "1.2 Model Fitting\n\n1.2.1 Data Introduction\nThe moss data in the spmodel package is an sf (simple features) object (Pebesma 2018) that contains observations on heavy metals in mosses near a mining road in Alaska. An sf object is a special data.frame built for storing spatial information and contains a column called geometry. We can view the first few rows of moss by running\n\nmoss\n#> Simple feature collection with 365 features and 7 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: -445884.1 ymin: 1929616 xmax: -383656.8 ymax: 2061414\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 365 × 8\n#>    sample field_dup lab_rep year  sideroad log_dist2road log_Zn\n#>    <fct>  <fct>     <fct>   <fct> <fct>            <dbl>  <dbl>\n#>  1 001PR  1         1       2001  N                 2.68   7.33\n#>  2 001PR  1         2       2001  N                 2.68   7.38\n#>  3 002PR  1         1       2001  N                 2.54   7.58\n#>  4 003PR  1         1       2001  N                 2.97   7.63\n#>  5 004PR  1         1       2001  N                 2.72   7.26\n#>  6 005PR  1         1       2001  N                 2.76   7.65\n#>  7 006PR  1         1       2001  S                 2.30   7.59\n#>  8 007PR  1         1       2001  N                 2.78   7.16\n#>  9 008PR  1         1       2001  N                 2.93   7.19\n#> 10 009PR  1         1       2001  N                 2.79   8.07\n#> # ℹ 355 more rows\n#> # ℹ 1 more variable: geometry <POINT [m]>\n\nMore information about moss can be found by running help(\"moss\", \"spmodel\").\nOur goal is to model the distribution of log zinc concentration (log_Zn) using a spatial linear model. We can visualize the distribution of log zinc concentration (log_Zn) in moss by running\n\nggplot(moss, aes(color = log_Zn)) +\n  geom_sf(size = 2) +\n  scale_color_viridis_c() +\n  scale_x_continuous(breaks = seq(-163, -164, length.out = 2)) +\n  theme_gray(base_size = 14)\n\n\n\nDistribution of log zinc concentration in the moss data.\n\n\n\n\nAn important predictor variable may be the log of the distance to the haul road, log_dist2road, which is measured in meters. Later we use spmodel to fit a spatial linear model with with log_Zn as the response and log_dist2road as a predictor.\n\n1.2.2 splm() Syntax and Output Interpretation\nThe splm() function shares similar syntactic structure with the lm() function used to fit linear models without spatial dependence (Equation 1.3). splm() generally requires at least three arguments\n\n\nformula: a formula that describes the relationship between the response variable (\\(\\mathbf{y}\\)) and explanatory variables (\\(\\mathbf{X}\\))\n\n\nformula in splm() is the same as formula in lm()\n\n\n\n\ndata: a data.frame or sf object that contains the response variable, explanatory variables, and spatial information.\n\nspcov_type: the spatial covariance type (\"exponential\", \"matern\", \"spherical\", etc)\n\nThere are 17 different types\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf data is an sf object, then spatial information is stored in the object’s geometry. However, if data is a data.frame or tibble (a special data.frame), then the names of the variables that represent the x-coordinates and y-coordinates must also be provided as two additional arguments via xcoord and ycoord.\n\n\nWe fit a spatial linear model regressing log zinc concentration (log_Zn) on log distance to a haul road (log_dist2road) using an exponential spatial covariance function by running\n\nspmod <- splm(formula = log_Zn ~ log_dist2road, data = moss,\n              spcov_type = \"exponential\")\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe estimation method in splm() is specified by estmethod. The default estimation method is restricted maximum likelihood (\"reml\"). Additional options include maximum likelihood \"ml\", semivariogram-based composite likelihood (\"sv-cl\") (Curriero and Lele 1999), and semivariogram-based weighted least squares (\"sv-wls\") (Cressie 1985). When the estimation method is semivariogram-based weighted least squares, the weights are specified by weights with a default of Cressie weights (“cressie\").\n\n\nWe summarize the model fit by running\n\nsummary(spmod)\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_type = \"exponential\")\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.6801 -1.3606 -0.8103 -0.2485  1.1298 \n#> \n#> Coefficients (fixed):\n#>               Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)    9.76825    0.25216   38.74   <2e-16 ***\n#> log_dist2road -0.56287    0.02013  -27.96   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Pseudo R-squared: 0.683\n#> \n#> Coefficients (exponential spatial covariance):\n#>        de        ie     range \n#> 3.595e-01 7.897e-02 8.237e+03\n\nThe fixed effects coefficient table contains estimates, standard errors, z-statistics, and asymptotic p-values for each fixed effect. From this table, we notice there is evidence that mean log zinc concentration significantly decreases with distance from the haul road (p-value < 2e-16).\nWe can relate some of the components in the summary output to the model in Equation 1.4:\n\nThe values in the Estimate column of the Coefficients (fixed) table form \\(\\boldsymbol{\\hat{\\beta}}\\), an estimate of \\(\\boldsymbol{\\beta}\\).\nThe de value of 0.36 in the Coefficients (exponential spatial covariance) table is \\(\\hat{\\sigma}^2_{de}\\), which is an estimate of \\(\\sigma^2_{de}\\), the variance of \\(\\boldsymbol{\\tau}\\) (commonly called the partial sill).\nThe ie value of 0.079 in the Coefficients (exponential spatial covariance) table is \\(\\hat{\\sigma}^2_{ie}\\), which is an estimate of \\(\\sigma^2_{ie}\\), the variance of \\(\\boldsymbol{\\epsilon}\\) (commonly called the nugget).\nThe range value of 8,237 in the Coefficients (exponential spatial covariance) table is \\(\\hat{\\phi}\\), which is an estimate of \\(\\phi\\) (recall \\(\\phi\\) is the range parameter in Equation 1.5 that controls the behavior of the spatial covariance as a function of distance).\n\nThe pseudo R-squared emulates the R-squared from nonspatial linear models, quantifying the proportion of variability in the model explained by the fixed effects. Via varcomp(), we can identify how much variability is attributed to distinct parts of the model:\n\nvarcomp(spmod)\n#> # A tibble: 3 × 2\n#>   varcomp            proportion\n#>   <chr>                   <dbl>\n#> 1 Covariates (PR-sq)     0.683 \n#> 2 de                     0.260 \n#> 3 ie                     0.0571\n\nWe see most of the variability is explained by the fixed effects (pseudo R-squared) and the spatially dependent random error (de), while little variability is independent (ie).\nThe summary() output, while useful, is printed to the R console and not easy to manipulate. The tidy() function turns the coefficient table into a tibble (i.e., a special data.frame) that is easy to manipulate. We tidy the fixed effects by running\n\ntidy(spmod)\n#> # A tibble: 2 × 5\n#>   term          estimate std.error statistic p.value\n#>   <chr>            <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)      9.77     0.252       38.7       0\n#> 2 log_dist2road   -0.563    0.0201     -28.0       0\n\nWe tidy the spatial covariance parameters by running\n\ntidy(spmod, effects = \"spcov\")\n#> # A tibble: 3 × 3\n#>   term   estimate is_known\n#>   <chr>     <dbl> <lgl>   \n#> 1 de       0.360  FALSE   \n#> 2 ie       0.0790 FALSE   \n#> 3 range 8237.     FALSE\n\nThe is_known column indicates whether the parameter is assumed known. By default, all parameters are assumed unknown. We discuss this more in Chapter 2.\n\n\n\n\n\n\nExercise\n\n\n\nAnother data set contained within the spmodel package is the caribou data set. Read about the caribou data with ?caribou. Then, fit a spatial linear model with\n\n\nz as the response and tarp, water, and the interaction between tarp and water as predictors\na spatial covariance model for the errors of your choosing. You can examine the spatial covariance models available to use in the spcov_type argument of splm() in the Arguments section of ?splm.\n\nx as the xcoord and y as the ycoord (note that the xcoord and ycoord arguments now need to be specified because caribou is a data.frame object, not an sf object).\n\nAfter fitting the model, perform an analysis of variance using anova() to assess the importance of tarp, water, and tarp:water.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\ncaribou_mod <- splm(z ~ tarp + water + tarp:water,\n                    data = caribou, spcov_type = \"spherical\",\n                    xcoord = x, ycoord = y)\nsummary(caribou_mod)\n#> \n#> Call:\n#> splm(formula = z ~ tarp + water + tarp:water, data = caribou, \n#>     spcov_type = \"spherical\", xcoord = x, ycoord = y)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.35216 -0.20785 -0.14179 -0.01131  0.38485 \n#> \n#> Coefficients (fixed):\n#>                  Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)       1.98916    0.18716  10.628  < 2e-16 ***\n#> tarpnone          0.17326    0.10644   1.628 0.103555    \n#> tarpshade         0.39640    0.10463   3.788 0.000152 ***\n#> waterY            0.04699    0.10396   0.452 0.651269    \n#> tarpnone:waterY  -0.18499    0.14781  -1.252 0.210724    \n#> tarpshade:waterY -0.21835    0.14451  -1.511 0.130812    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Pseudo R-squared: 0.4573\n#> \n#> Coefficients (spherical spatial covariance):\n#>       de       ie    range \n#>  0.04794  0.02209 13.15323\nanova(caribou_mod)\n#> Analysis of Variance Table\n#> \n#> Response: z\n#>             Df     Chi2 Pr(>Chi2)    \n#> (Intercept)  1 112.9630 < 2.2e-16 ***\n#> tarp         2  14.5326 0.0006987 ***\n#> water        1   0.2043 0.6512687    \n#> tarp:water   2   2.6071 0.2715643    \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\ntidy(anova(caribou_mod))\n#> # A tibble: 4 × 4\n#>   effects        df statistic  p.value\n#>   <chr>       <int>     <dbl>    <dbl>\n#> 1 (Intercept)     1   113.    2.20e-26\n#> 2 tarp            2    14.5   6.99e- 4\n#> 3 water           1     0.204 6.51e- 1\n#> 4 tarp:water      2     2.61  2.72e- 1\n\n\n\n\n\n1.2.3 Model Fit and Diagnostics\nThe quality of model fit can be assessed using a variety of statistics readily available in spmodel, including AIC, AICc, and pseudo R-squared. Additionally, model diagnostics such as leverage, fitted values, residuals (several types), and Cook’s distance. While both the model fit statistics and the diagnostics can be found with individual functions like AIC(), residuals(), cooks.distance(), etc., they can also be computed using glance() (for the model fit statistics) and augment() (for the diagnostics).\n\nglance(spmod)\n#> # A tibble: 1 × 9\n#>       n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1   365     2     3  367.  373.  373.  -184.     363.            0.683\n\nThe output from glance() shows model fit statistics for the spatial linear model with an exponential covariance structure for the errors.\nThe augment() function provides many model diagnostics statistics in a single tibble:\n\naugment(spmod)\n#> Simple feature collection with 365 features and 7 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: -445884.1 ymin: 1929616 xmax: -383656.8 ymax: 2061414\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 365 × 8\n#>    log_Zn log_dist2road .fitted .resid   .hat .cooksd .std.resid\n#>  *  <dbl>         <dbl>   <dbl>  <dbl>  <dbl>   <dbl>      <dbl>\n#>  1   7.33          2.68    8.26 -0.928 0.0200 0.0142       1.18 \n#>  2   7.38          2.68    8.26 -0.880 0.0200 0.0186       1.35 \n#>  3   7.58          2.54    8.34 -0.755 0.0225 0.00482      0.647\n#>  4   7.63          2.97    8.09 -0.464 0.0197 0.0305       1.74 \n#>  5   7.26          2.72    8.24 -0.977 0.0215 0.131        3.45 \n#>  6   7.65          2.76    8.21 -0.568 0.0284 0.0521       1.89 \n#>  7   7.59          2.30    8.47 -0.886 0.0300 0.0591       1.96 \n#>  8   7.16          2.78    8.20 -1.05  0.0335 0.00334      0.439\n#>  9   7.19          2.93    8.12 -0.926 0.0378 0.0309       1.26 \n#> 10   8.07          2.79    8.20 -0.123 0.0314 0.00847      0.723\n#> # ℹ 355 more rows\n#> # ℹ 1 more variable: geometry <POINT [m]>\n\naugment() returns a tibble with many model diagnostics statistics, including\n\n\n.fitted, the fitted value, calculated from the estimated fixed effects in the model\n\n.hat, the Mahalanobis distance, a metric of leverage\n\n.cooksd, the Cook’s distance, a metric of influence\n\n.std.resid, the standardized residual\n\nIf the model is correct, then the standardized residuals have mean 0, standard deviation 1, and are uncorrelated.\nThe plot() function can be used on a fitted model object to construct a few pre-specified plots of these model diagnostics. For example, the following code plots the Cook’s distance, a measure of influence, which quantifies each observation’s impact on model fit:\n\nplot(spmod, which = 4)\n\n\n\n\nThe other 7 plots for model objects fit with splm() can be read about in the help: ?plot.spmodel.\nIf the grammar of graphics plotting syntax in ggplot2 is more familiar, then we can also construct plots with the augmented model:\n\naug_df <- augment(spmod)\nggplot(data = aug_df, aes(x = seq_len(nrow(aug_df)), y = .cooksd)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Row Number\")\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse spmodel’s plot function on the spmod object to construct a plot of the fitted spatial covariance vs spatial distance. To learn more about the options for spmodel’s plot function, run ?plot.spmodel or visit this link.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nplot(spmod, which = 7)\n\n\n\n\n\n\n\n\n1.2.4 Model Comparison\nSo far we have relied on the intuition that the spatial model performs better than a nonspatial one, but we have not yet communicated this empirically. We fit a nonspatial linear model using splm() with spcov_type = \"none\" (this is equivalent to a model fit using lm(), but using splm() provides access to other helper functions in spmodel):\n\nnone <- splm(formula = log_Zn ~ log_dist2road, data = moss,\n              spcov_type = \"none\")\n\n\n\n\n\n\n\nExercise\n\n\n\nCompare the output of the none model fit using splm() to a model via using lm(). Do you get the same estimates and standard errors?\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nsummary(none)\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_type = \"none\")\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.45814 -0.37942 -0.05889  0.38035  1.77185 \n#> \n#> Coefficients (fixed):\n#>               Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)    7.33423    0.09960   73.64   <2e-16 ***\n#> log_dist2road -0.34460    0.01267  -27.20   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Pseudo R-squared: 0.6709\n#> \n#> Coefficients (none spatial covariance):\n#>     ie \n#> 0.3214\nlmod <- lm(formula = log_Zn ~ log_dist2road, data = moss)\nsummary(lmod)\n#> \n#> Call:\n#> lm(formula = log_Zn ~ log_dist2road, data = moss)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -1.45814 -0.37942 -0.05889  0.38035  1.77185 \n#> \n#> Coefficients:\n#>               Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)    7.33423    0.09960   73.64   <2e-16 ***\n#> log_dist2road -0.34460    0.01267  -27.20   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.5669 on 363 degrees of freedom\n#> Multiple R-squared:  0.6709, Adjusted R-squared:   0.67 \n#> F-statistic: 740.1 on 1 and 363 DF,  p-value: < 2.2e-16\n\nThe estimates and standard errors are the same. For small sample sizes, the p-values may be slightly different because spmodel uses a reference z-distribution while lm() uses a reference t-distribution.\n\n\n\nThe glances() function allows us to compare the model fit statistics for a few different models simultaneously. Two of these fit statistics are AIC and AICc (Akaike 1974), which are commonly used to select a “best” model – the lower the AIC/AICc, the better the model. The AICc is more appropriate for small samples, but for large samples AIC/AICc are nearly equivalent. glances() automatically orders the models by AICc:\n\nglances(spmod, none)\n#> # A tibble: 2 × 10\n#>   model     n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr> <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 spmod   365     2     3  367.  373.  373.  -184.     363.            0.683\n#> 2 none    365     2     1  634.  636.  636.  -317.     363.            0.671\n\nThe AIC and AICc are significantly lower for the spatial model, which suggest the spatial model fits the data better than the nonspatial model.\n\n\n\n\n\n\nCaution\n\n\n\nThe default estimation method is restricted maximum likelihood (estmethod = \"reml\"). Models fit using REML can only be compared using AIC/AICc when the models have the same set of explanatory variables. Models with different sets of explanatory variables can be compared via AIC/AICc when fitting using maximum likelihood (estmethod = \"ml\"). AIC/AICc are not defined for models fit using estmethod = \"sv-wls\" or estmethod = \"sv-cl\".\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse the likelihood ratio test (run help(\"anova.spmod\", \"spmodel\")) to determine whether the spatial model outperforms the nonspatial model.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\nThe likelihood ratio test can be used here because 1) the models are “nested” within one another (i.e., the nonspatial linear model is a special case of the spatial linear model with de set to zero) and 2) because estmethod = \"reml\", the fixed effect structure is the same (like AIC/AICc, to compare models with varying fixed effect structures, use estmethod = \"ml\").\n\nanova(spmod, none)\n#> Likelihood Ratio Test\n#> \n#> Response: log_Zn\n#>               Df   Chi2 Pr(>Chi2)    \n#> spmod vs none  2 266.85 < 2.2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nThe small p-value (\\(p < 0.001\\)) indicates that the spatial model is a significantly better fit to the data.\n\n\n\nAnother approach to model selection is leave-one-out cross validation (Hastie et al. 2009). In leave-one-out cross validation, a single observation is removed from the data, the model is re-fit, and a prediction is made for the held-out observation. Then, a loss metric like mean-squared-prediction error (MSPE) is computed and used to evaluate model fit. The lower the mean-squared-prediction error, the better the model fit.\n\nloocv(spmod)\n#> # A tibble: 1 × 4\n#>      bias  MSPE RMSPE  cor2\n#>     <dbl> <dbl> <dbl> <dbl>\n#> 1 0.00655 0.111 0.333 0.886\nloocv(none)\n#> # A tibble: 1 × 4\n#>       bias  MSPE RMSPE  cor2\n#>      <dbl> <dbl> <dbl> <dbl>\n#> 1 0.000644 0.324 0.569 0.667\n\nloocv() returns several useful fit statistics:\n\n\nbias: The average difference between the observed value and its leave-one-out prediction. This should be close to zero for well-fitting models.\n\nMSPE: The mean squared prediction error between the observed value and its leave-one-out prediction.\n\nRMSPE: The square root of MSPE.\n\ncor2: The squared correlation between the observed value and its leave-one-out prediction. This can be viewed as a “predictive” R-squared, emulating the “squared correlation” interpretation of R-squared in nonspatial linear models.\n\n\n\n\n\n\n\nCaution\n\n\n\nWe do not recommend using pseudo R-squared as a criteria by which to compare models, as it is simply a descriptive statistic that provides insight model fit. The “predictive” R-squared statistic (cor2 in loocv()) is a type of R-squared statistic that can be compared across models.Instead, consider the cor2 statistic, which is a type of R-squared statistic that is generally comparable across models.\n\n\n\n1.2.4.1 Model Selection Strategies\nWe use models to simplify and explain incredibly complex systems. As George Box noted, all models are wrong but some are useful. In short, model selection is a challenging problem, and people approach it with varying perspectives. Some prefer a single model be hypothesized and fit based on first principles, some prefer using algorithmic approaches like stepwise regression to find the most appropriate model, and some people are in between. We think it is reasonable to use your expert knowledge of the system being studied to determine a set of candidate models. Then, you can combine this expert knowledge with empirical tools like AIC, AICc, likelihood ratio tests, and cross validation to distinguish among these candidate models and determine the most appropriate model. We note that even if model selection is performed on fixed effects and covariance structures simultaneously using maximum likelihood (estmethod = \"ml\") and AIC, AICc, and/or likelihood ratio tests, that the most appropriate model should then be refit using restricted maximum likelihood (estmethod = \"reml\") before proceeding with model interpretation and, eventually, spatial prediction at unobserved locations. Zuur et al. (2009), Johnson and Omland (2004), and Zimmerman and Ver Hoef (2024) provide further insights.\n\n\n\n\n\n\nTip\n\n\n\nIf you don’t know which spatial covariance types to start with, try spcov_type = \"exponential\" and spcov_type = \"gaussian\". These two are well-understood and have notably different covariance behavior for observations that are close to one another. It is not uncommon to see little difference in model fit between spatial models while still seeing a dramatic difference in model fit between spatial models and nonspatial models. To see a list of all spatial covariance types, run help(\"splm\", \"spmodel\") or visit this link."
  },
  {
    "objectID": "splm.html#sec-sp-pred",
    "href": "splm.html#sec-sp-pred",
    "title": "1  Spatial Linear Models in spmodel",
    "section": "\n1.3 Spatial Prediction",
    "text": "1.3 Spatial Prediction\nWe could use spmod from the moss data to make spatial predictions of log Zinc at different locations of interest near the haul road. However, to familiarize ourselves with more of the example data sets in spmodel, we will switch gears and use the moose data to build a model and make spatial predictions at the locations in moose_preds.\n\n1.3.1 Data Introduction\nThe moose data in the spmodel package contains observations from a moose survey in Alaska. The Alaska Department of Fish and Game performed the survey on 218 spatial locations throughout the region of interest. Our goal is to predict the moose count in 100 spatial locations in the moose_pred data frame that were not surveyed. Both elev, the elevation of the spatial location, and strat, a stratification variable based on landscape metrics that is either \"L\" for Low or \"M\" for medium, are possible predictors for moose count.\n\nmoose\n#> Simple feature collection with 218 features and 4 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 269085 ymin: 1416151 xmax: 419976.2 ymax: 1541763\n#> Projected CRS: NAD83 / Alaska Albers\n#> First 10 features:\n#>        elev strat count presence                 geometry\n#> 1  468.9167     L     0        0 POINT (293542.6 1541016)\n#> 2  362.3125     L     0        0 POINT (298313.1 1533972)\n#> 3  172.7500     M     0        0 POINT (281896.4 1532516)\n#> 4  279.6250     L     0        0 POINT (298651.3 1530264)\n#> 5  619.6000     L     0        0 POINT (311325.3 1527705)\n#> 6  164.1250     M     0        0 POINT (291421.5 1518398)\n#> 7  163.5000     M     0        0 POINT (287298.3 1518035)\n#> 8  186.3500     L     0        0 POINT (279050.9 1517324)\n#> 9  362.3125     L     0        0 POINT (346145.9 1512479)\n#> 10 430.5000     L     0        0 POINT (321354.6 1509966)\n\nWe visualize the moose counts by running\n\nggplot(data = moose, aes(colour = count)) +\n  geom_sf() +\n  scale_colour_viridis_c(limits = c(0, 40)) +\n  theme_minimal()\n\n\n\n\nFrom our plot, we see that there are a large number of observed moose counts at or near 0. Therefore, perhaps a generalized linear model in the Poisson or negative binomial family might be more appropriate for this particular data set. We will come back to this issue in Chapter 3; however, for this section, we assume that a standard spatial linear model is appropriate.\n\n\n\n\n\n\nNote\n\n\n\nWe also see in the plot that the spatial locations in the survey were clearly not randomly selected. Random selection of spatial locations is only required for inference in design-based analyses. For model-based analyses, random selection of spatial locations is not necessarily an assumption. See Brus (2021) and Dumelle et al. (2022) for more.\n\n\n\n1.3.2 Moose Count Predictions\nIn this section, we show how to use predict() and augment() to perform spatial prediction (also called Kriging) for point-referenced data from a model fit with splm(). First, we fit a spatial model to the moose data with a \"spherical\" spatial covariance and elev, strat, and their interaction as predictors in the model:\n\nmoosemod <- splm(count ~ elev * strat, data = moose,\n                  spcov_type = \"spherical\")\ntidy(moosemod)\n#> # A tibble: 4 × 5\n#>   term        estimate std.error statistic p.value\n#>   <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n#> 1 (Intercept)   0.310    9.02       0.0344 0.973  \n#> 2 elev          0.0141   0.00806    1.76   0.0792 \n#> 3 stratM        6.93     2.26       3.07   0.00217\n#> 4 elev:stratM  -0.0273   0.0130    -2.10   0.0357\n\n\n\n\n\n\n\nTip\n\n\n\nelev * strat is shorthand for elev + strat + elev:strat.\n\n\nWe then use predict() to predict the moose count at the spatial locations in moose_preds. The predict() function for models fit with splm() works in the same way as it does for models fit with lm(). We provide predict() with the fitted model object, along with a newdata argument that is an sf object, data.frame, or tibble that contains the locations at which to predict. newdata must have the same predictors as those used to fit the spatial model. We see that moose_preds contains the predictors (elev and strat) and the locations at which to predict:\n\nmoose_preds\n#> Simple feature collection with 100 features and 2 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 269085 ymin: 1416151 xmax: 419976.2 ymax: 1541763\n#> Projected CRS: NAD83 / Alaska Albers\n#> First 10 features:\n#>        elev strat                 geometry\n#> 1  143.4000     L POINT (401239.6 1436192)\n#> 2  324.4375     L POINT (352640.6 1490695)\n#> 3  158.2632     L POINT (360954.9 1491590)\n#> 4  221.3125     M POINT (291839.8 1466091)\n#> 5  208.6875     M POINT (310991.9 1441630)\n#> 6  218.3333     L POINT (304473.8 1512103)\n#> 7  126.8125     L POINT (339011.1 1459318)\n#> 8  122.0833     L POINT (342827.3 1463452)\n#> 9  191.0000     L POINT (284453.8 1502837)\n#> 10 105.3125     L POINT (391343.9 1483791)\n\n\n# results omitted\npredict(moosemod, newdata = moose_preds)\n\nThe output of predict() (not rendered in this document) gives predicted moose counts for the 100 unobserved spatial locations in moose_preds.\n\n\n\n\n\n\nNote\n\n\n\nExamining some of the predictions, we see that a few are negative. These unreasonable negative values are a further indication that we should use a spatial generalized linear model in Chapter 3.\n\n\nThe augment() function can also be used to obtain predictions for unobserved locations. While the required arguments to augment() are the same as the arguments used in predict() (the name of the fitted model object along with a newdata data frame), the output of augment() is an sf object with predictions in the .fitted column. Often, using augment() is more convenient than using predict(), as augment() returns an object with predictions alongside the spatial locations and any predictors used in the model.\n\nmoose_aug <- augment(moosemod, newdata = moose_preds)\nmoose_aug\n#> Simple feature collection with 100 features and 3 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 269386.2 ymin: 1418453 xmax: 419976.2 ymax: 1541763\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 100 × 4\n#>     elev strat .fitted           geometry\n#>  * <dbl> <chr>   <dbl>        <POINT [m]>\n#>  1  143. L       3.45  (401239.6 1436192)\n#>  2  324. L       1.59  (352640.6 1490695)\n#>  3  158. L      -0.267 (360954.9 1491590)\n#>  4  221. M       2.39  (291839.8 1466091)\n#>  5  209. M       7.62  (310991.9 1441630)\n#>  6  218. L      -1.02  (304473.8 1512103)\n#>  7  127. L      -1.23  (339011.1 1459318)\n#>  8  122. L      -1.43  (342827.3 1463452)\n#>  9  191  L      -0.239 (284453.8 1502837)\n#> 10  105. L       0.657 (391343.9 1483791)\n#> # ℹ 90 more rows\n\nWe can construct a plot of the predictions with\n\nggplot(data = moose, aes(colour = count)) +\n  geom_sf(alpha = 0.4) +\n  geom_sf(data = moose_aug, aes(colour = .fitted)) +\n  scale_colour_viridis_c(limits = c(0, 40)) +\n  theme_minimal()\n\n\n\n\nIn the plot, the observed counts are also shown with faded points. We see that, most of the predictions are at or near 0, but spatial locations that are close in proximity to observed counts that are very large have a higher predicted count (for example, the point in the southwest region that is directly south of the observed count coloured yellow is predicted to be around 10).\n\n\n\n\n\n\nExercise\n\n\n\nExamine the help file ?augment.spmodel or by visiting this link and create site-wise 99% prediction intervals for the unsampled locations found in moose_preds.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\naugment(moosemod, newdata = moose_preds, interval = \"prediction\",\n        level = 0.99)\n#> Simple feature collection with 100 features and 5 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 269386.2 ymin: 1418453 xmax: 419976.2 ymax: 1541763\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 100 × 6\n#>     elev strat .fitted .lower .upper           geometry\n#>  * <dbl> <chr>   <dbl>  <dbl>  <dbl>        <POINT [m]>\n#>  1  143. L       3.45  -11.1    18.0 (401239.6 1436192)\n#>  2  324. L       1.59  -13.4    16.5 (352640.6 1490695)\n#>  3  158. L      -0.267 -15.1    14.5 (360954.9 1491590)\n#>  4  221. M       2.39  -12.2    17.0 (291839.8 1466091)\n#>  5  209. M       7.62   -6.99   22.2 (310991.9 1441630)\n#>  6  218. L      -1.02  -15.9    13.9 (304473.8 1512103)\n#>  7  127. L      -1.23  -15.9    13.5 (339011.1 1459318)\n#>  8  122. L      -1.43  -16.1    13.3 (342827.3 1463452)\n#>  9  191  L      -0.239 -15.2    14.8 (284453.8 1502837)\n#> 10  105. L       0.657 -14.0    15.3 (391343.9 1483791)\n#> # ℹ 90 more rows\n\n\n\n\n\n1.3.3 Cross Validation\nRecall the loocv() function can be used to perform leave-one-out cross validation on a fitted model object.\n\nloocv(moosemod)\n#> # A tibble: 1 × 4\n#>        bias  MSPE RMSPE  cor2\n#>       <dbl> <dbl> <dbl> <dbl>\n#> 1 -0.000201  32.2  5.67 0.120\n\n\n\n\n\n\n\nExercise\n\n\n\nFit a model with count as the response variable from the moose data with a \"spherical\" spatial covariance model for the random errors but no predictors as fixed effects. Compare the MSPE from leave-one-out cross-validation for this model with the previously fit moosemod. Which model is better, according to the leave-one-out cross-validation criterion?\nThen, for the model with the lower MSPE, obtain the leave-one-out cross validation predictions and their standard errors. Hint: run ?loocv or visit this link.\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nmoose_int <- splm(count ~ 1, data = moose,\n                  spcov_type = \"spherical\")\nloocv(moose_int)\n#> # A tibble: 1 × 4\n#>       bias  MSPE RMSPE   cor2\n#>      <dbl> <dbl> <dbl>  <dbl>\n#> 1 -0.00798  33.6  5.79 0.0801\n\n\n# results omitted\nloocv(moosemod, cv_predict = TRUE, se.fit = TRUE)"
  },
  {
    "objectID": "splm.html#r-code-appendix",
    "href": "splm.html#r-code-appendix",
    "title": "1  Spatial Linear Models in spmodel",
    "section": "\n1.4 R Code Appendix",
    "text": "1.4 R Code Appendix\n\n\n\n\nlibrary(spmodel)\nlibrary(ggplot2)\nmoss\nggplot(moss, aes(color = log_Zn)) +\n  geom_sf(size = 2) +\n  scale_color_viridis_c() +\n  scale_x_continuous(breaks = seq(-163, -164, length.out = 2)) +\n  theme_gray(base_size = 14)\nspmod <- splm(formula = log_Zn ~ log_dist2road, data = moss,\n              spcov_type = \"exponential\")\nsummary(spmod)\nvarcomp(spmod)\ntidy(spmod)\ntidy(spmod, effects = \"spcov\")\ncaribou_mod <- splm(z ~ tarp + water + tarp:water,\n                    data = caribou, spcov_type = \"spherical\",\n                    xcoord = x, ycoord = y)\nsummary(caribou_mod)\nanova(caribou_mod)\ntidy(anova(caribou_mod))\nglance(spmod)\naugment(spmod)\nplot(spmod, which = 4)\naug_df <- augment(spmod)\nggplot(data = aug_df, aes(x = seq_len(nrow(aug_df)), y = .cooksd)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"Row Number\")\nplot(spmod, which = 7)\nnone <- splm(formula = log_Zn ~ log_dist2road, data = moss,\n              spcov_type = \"none\")\nsummary(none)\nlmod <- lm(formula = log_Zn ~ log_dist2road, data = moss)\nsummary(lmod)\nglances(spmod, none)\nanova(spmod, none)\nloocv(spmod)\nloocv(none)\nmoose\nggplot(data = moose, aes(colour = count)) +\n  geom_sf() +\n  scale_colour_viridis_c(limits = c(0, 40)) +\n  theme_minimal()\nmoosemod <- splm(count ~ elev * strat, data = moose,\n                  spcov_type = \"spherical\")\ntidy(moosemod)\nmoose_preds\n# results omitted\npredict(moosemod, newdata = moose_preds)\nmoose_aug <- augment(moosemod, newdata = moose_preds)\nmoose_aug\nggplot(data = moose, aes(colour = count)) +\n  geom_sf(alpha = 0.4) +\n  geom_sf(data = moose_aug, aes(colour = .fitted)) +\n  scale_colour_viridis_c(limits = c(0, 40)) +\n  theme_minimal()\naugment(moosemod, newdata = moose_preds, interval = \"prediction\",\n        level = 0.99)\nloocv(moosemod)\nmoose_int <- splm(count ~ 1, data = moose,\n                  spcov_type = \"spherical\")\nloocv(moose_int)\n# results omitted\nloocv(moosemod, cv_predict = TRUE, se.fit = TRUE)\n\n\n\n\n\nAkaike, Hirotugu. 1974. “A New Look at the Statistical Model Identification.” IEEE Transactions on Automatic Control 19 (6): 716–23.\n\n\nBrus, Dick J. 2021. “Statistical Approaches for Spatial Sample Survey: Persistent Misconceptions and New Developments.” European Journal of Soil Science 72 (2): 686–703.\n\n\nCressie, Noel. 1985. “Fitting Variogram Models by Weighted Least Squares.” Journal of the International Association for Mathematical Geology 17 (5): 563–86.\n\n\nCurriero, Frank C, and Subhash Lele. 1999. “A Composite Likelihood Approach to Semivariogram Estimation.” Journal of Agricultural, Biological, and Environmental Statistics, 9–28.\n\n\nDumelle, Michael, Matt Higham, Jay M Ver Hoef, Anthony R Olsen, and Lisa Madsen. 2022. “A Comparison of Design-Based and Model-Based Approaches for Finite Population Spatial Sampling and Inference.” Methods in Ecology and Evolution 13 (9): 2018–29.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. 2009. The Elements of Statistical Learning: Data Mining, Inference, and Prediction. Vol. 2. Springer.\n\n\nJohnson, Jerald B, and Kristian S Omland. 2004. “Model Selection in Ecology and Evolution.” Trends in Ecology & Evolution 19 (2): 101–8.\n\n\nPebesma, Edzer. 2018. “Simple Features for R: Standardized Support for Spatial Vector Data.” The R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nZimmerman, Dale L, and Jay M Ver Hoef. 2024. Spatial Linear Models for Environmental Data. CRC Press.\n\n\nZuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M Smith, et al. 2009. Mixed Effects Models and Extensions in Ecology with r. Vol. 574. Springer."
  },
  {
    "objectID": "splm-additional.html#big-spatial-data",
    "href": "splm-additional.html#big-spatial-data",
    "title": "2  Additional spmodel Features",
    "section": "\n2.1 Big Spatial Data",
    "text": "2.1 Big Spatial Data\nFor large observed data sets, fitting spatial linear models or making predictions is challenging because these operations require a covariance matrix inverse, which are computationally challenging to obtain. Typically, observed data samples sizes approaching around 10,000 make model fitting or prediction infeasible on a standard computer in a reasonable amount of time (your definition of this may vary). This necessitates the use of model fitting and prediction tools that work for large data sets. spmodel offers big data methods for model fitting and prediction for point-referenced data via the local argument to splm() and predict().\n\n2.1.1 Model Fitting\nspmodel implements “local” spatial indexing as described by Ver Hoef et al. (2023). Observations are first assigned an index. Then for the purposes of model fitting, observations with different indexes are assumed uncorrelated. Assuming observations with different indexes are uncorrelated induces sparsity in the covariance matrix, which greatly reduces the computational time required for operations that involve its inverse. Models fit using spatial indexing are capable of fitting models with hundreds of thousands of observations relatively quickly. Ver Hoef et al. (2023) showed that in a variety of scenarios, spatial indexing yielded fixed effect confidence intervals with proper coverage.\nTo illustrate spatial indexing in spmodel, we first simulate a response variable sim_response with 5000 observations at random spatial locations in the unit square (sim_coords). Then we place the response and coordinates in a data.frame:\n\nset.seed(06022024)\nsim_params <- spcov_params(\"exponential\", de = 7, ie = 2, range = 0.7)\n\nn <- 5000\nx <- runif(n)\ny <- runif(n)\nsim_coords <- data.frame(x, y)\n\nsim_response <- sprnorm(sim_params, data = sim_coords, xcoord = x, ycoord = y)\nsim_data <- data.frame(sim_coords, sim_response)\n\nWe visualize the data by running\n\nggplot(sim_data, aes(x = x, y = y, color = sim_response)) +\n  geom_point() +\n  scale_color_viridis_c(limits = c(-8, 9)) +\n  theme_gray(base_size = 14)\n\n\n\nFigure 2.1: Distribution of simulated data\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe provide more detail regarding using spmodel to simulate data later on in this section.\n\n\nWe then use splm() to fit a spatial model to sim_data, providing the xcoord and ycoord arguments because sim_data is a data.frame, not an sf object. To implement spatial indexing, we use the local argument to splm(). Setting local to TRUE chooses default spatial indexing settings. We fit the model and time it by running\n\nfit_start_time <- proc.time()\nbdmod <- splm(sim_response ~ 1, data = sim_data,\n     spcov_type = \"exponential\",\n     xcoord = x, ycoord = y,\n     local = TRUE)\nfit_end_time <- proc.time()\nfit_end_time - fit_start_time\n#>    user  system elapsed \n#>    6.86    0.93   10.88\n\nThe model with 5000 observations is fit in just 10.88 seconds.\n\n\n\n\n\n\nNote\n\n\n\nWhen the sample size is larger than 5000 observations, splm() implements spatial indexing by default, as fitting time without spatial indexing becomes lengthy. This behavior can be overridden by explicitly setting local to FALSE.\n\n\nA summary of the model fit yields\n\nsummary(bdmod)\n#> \n#> Call:\n#> splm(formula = sim_response ~ 1, data = sim_data, spcov_type = \"exponential\", \n#>     xcoord = x, ycoord = y, local = TRUE)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -8.4301 -1.8626  0.1926  1.8081  7.9181 \n#> \n#> Coefficients (fixed):\n#>             Estimate Std. Error z value Pr(>|z|)\n#> (Intercept)   0.5384     1.2610   0.427    0.669\n#> \n#> Coefficients (exponential spatial covariance):\n#>     de     ie  range \n#> 4.3557 1.9964 0.4643\n\nThe other way to specify local in splm() is via a list object, which offers much more control and customization over the spatial indexing. To learn more, read about local in splm()’s help page by running ?splm.\n\n\n\n\n\n\nNote\n\n\n\nEven for two separate data sets with the same sample size fit on the same machine, the computational time required to fit models via spatial indexing varies, depending on many factors like the number of iterations required for convergence and the number of observations assigned to each spatial index.\n\n\n\n2.1.2 Local Prediction\nUsing the fitted model, Ver Hoef et al. (2023) evaluates the performance of local neighborhood prediction. Local neighborhood prediction only uses some of the observed data to predict for an unobserved location of interest. Local neighborhood prediction is capable of making predictions of hundreds of thousands of observations relatively quickly. Ver Hoef et al. (2023) showed that in a variety of scenarios, local neighborhood prediction yielded prediction intervals with proper coverage.\nTo illustrate local neighborhood prediction in spmodel, we first simulate 3000 new random spatial locations in the unit square (sim_coords). Then we place the coordinates in a data.frame and visualize:\n\nn_pred <- 3000\nx_pred <- runif(n_pred)\ny_pred <- runif(n_pred)\nsim_preds <- tibble::tibble(x = x_pred, y = y_pred)\n\nTo implement local neighborhood prediction, we use the local argument to predict() (or augment()). Setting local in predict() (or augment()) to TRUE chooses default local neighborhood prediction settings. We compute local neighborhood predictions at the unobserved locations in sim_preds and time it by running\n\npred_start_time <- proc.time()\nsim_preds$preds <- predict(bdmod, newdata = sim_preds, local = TRUE)\npred_end_time <- proc.time()\npred_end_time - pred_start_time\n#>    user  system elapsed \n#>   20.42    1.35   28.65\n\nThe 3000 predictions are computed in just 28.65 seconds. We visualize them by running\n\nggplot(sim_preds, aes(x = x, y = y, color = preds)) +\n  geom_point() +\n  scale_color_viridis_c(limits = c(-8, 9)) +\n  theme_gray(base_size = 14)\n\n\n\nFigure 2.2: Distribution of local neighborhood predictions using the model fit to the large simulated data set.\n\n\n\n\nThese predictions at the unobserved locations closely match the pattern of the observed data.\nThe other way to specify local in predict() (or augment()) is via a list object, which offers much more control and customization over the local neighborhood prediction. To learn more, read about local in predict()’s (or augment()’s) help page by running ?predict.spmodel (or ?augment.spmodel).\n\n\n\n\n\n\nNote\n\n\n\nMost of the computational burden associated with prediction is actually from the observed data sample size used to fit the model (because an inverse is needed). As long as the observed data sample sizes are a few thousand or fewer, local prediction is not imperative, no matter the size of the prediction data. Note that parallel process can be used whether or not local prediction is implemented.\n\n\n\n\n\n\n\n\nTip\n\n\n\nloocv() also has a local argument for large data sets that is structured the same as local for predict() (and augment())."
  },
  {
    "objectID": "splm-additional.html#additional-arguments",
    "href": "splm-additional.html#additional-arguments",
    "title": "2  Additional spmodel Features",
    "section": "\n2.2 Additional Arguments",
    "text": "2.2 Additional Arguments\n\n2.2.1 Multiple Models\nsplm() fits multiple models simultaneously when spcov_type is a vector with more than one element:\n\nspmods <- splm(formula = log_Zn ~ log_dist2road, data = moss,\n              spcov_type = c(\"exponential\", \"gaussian\"))\n\nspmods is a list with two elements: exponential, using the exponential spatial covariance; and gaussian, using the Gaussian spatial covariance.\n\nnames(spmods)\n#> [1] \"exponential\" \"gaussian\"\n\nspmods is natural to combine with glances() to glance at each model fit:\n\nglances(spmods)\n#> # A tibble: 2 × 10\n#>   model      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 expon…   365     2     3  367.  373.  373.  -184.     363.            0.683\n#> 2 gauss…   365     2     3  435.  441.  441.  -218.     363.            0.686\n\nand to combine with predict() to predict for each model fit.\n\n\n\n\n\n\nExercise\n\n\n\nWork with a neighbor to find 90% confidence intervals for the fixed effects in the Gaussian model using either (1) tidy() or (2) confint(). Before beginning, decide with your neighbor who will begin working on (1) tidy() and who will begin working on (2) confint().\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\ntidy(spmods$gaussian, conf.int = TRUE, conf.level = 0.90)\n#> # A tibble: 2 × 7\n#>   term          estimate std.error statistic p.value conf.low conf.high\n#>   <chr>            <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n#> 1 (Intercept)      9.21     0.196       46.9       0    8.89      9.53 \n#> 2 log_dist2road   -0.519    0.0184     -28.2       0   -0.549    -0.489\nconfint(spmods$gaussian, level = 0.90)\n#>                      5 %       95 %\n#> (Intercept)    8.8853387  9.5310288\n#> log_dist2road -0.5493091 -0.4887111\n\n\n\n\n\n2.2.2 Non-Spatial Random Effects\nIn the moss data, there are actually some spatial locations that have more than one measurement due to multiple samples being collected at a single location or due to a single sample being tested multiple times in the laboratory. The sample variable indexes the spatial location:\n\nmoss\n#> Simple feature collection with 365 features and 7 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: -445884.1 ymin: 1929616 xmax: -383656.8 ymax: 2061414\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 365 × 8\n#>    sample field_dup lab_rep year  sideroad log_dist2road log_Zn\n#>    <fct>  <fct>     <fct>   <fct> <fct>            <dbl>  <dbl>\n#>  1 001PR  1         1       2001  N                 2.68   7.33\n#>  2 001PR  1         2       2001  N                 2.68   7.38\n#>  3 002PR  1         1       2001  N                 2.54   7.58\n#>  4 003PR  1         1       2001  N                 2.97   7.63\n#>  5 004PR  1         1       2001  N                 2.72   7.26\n#>  6 005PR  1         1       2001  N                 2.76   7.65\n#>  7 006PR  1         1       2001  S                 2.30   7.59\n#>  8 007PR  1         1       2001  N                 2.78   7.16\n#>  9 008PR  1         1       2001  N                 2.93   7.19\n#> 10 009PR  1         1       2001  N                 2.79   8.07\n#> # ℹ 355 more rows\n#> # ℹ 1 more variable: geometry <POINT [m]>\n\nWe might expect Zinc concentration to be correlated within a spatial location; therefore, we might want to add sample as a non-spatial random effect (here, an intercept random effect) to the model with log_Zn as the response and log_dist2road as the predictor. The splm() function allows non-spatial random effects to be incorporated with the random argument, which takes a formula specification that is similar in syntax as the nlme (Pinheiro and Bates 2006) and lme4 (Bates et al. 2015) packages.\n\nrandint <- splm(log_Zn ~ log_dist2road,\n                data = moss, spcov_type = \"exponential\",\n                random = ~ (1 | sample))\n\n\n\n\n\n\n\nTip\n\n\n\nFor the randint model, in the random argument, sample is shorthand for (1 | sample). So the randint model could be written more concisely as\n\nrandint <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ sample)\n\n\n\nThe summary output now shows an estimate of the variance of the random intercepts, in addition to the estimated fixed effects and estimated spatial covariance parameters.\n\nsummary(randint)\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_type = \"exponential\", \n#>     random = ~sample)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -2.6234 -1.3228 -0.8026 -0.2642  1.0998 \n#> \n#> Coefficients (fixed):\n#>               Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)    9.66066    0.26770   36.09   <2e-16 ***\n#> log_dist2road -0.55028    0.02071  -26.58   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Pseudo R-squared: 0.6605\n#> \n#> Coefficients (exponential spatial covariance):\n#>        de        ie     range \n#> 3.153e-01 2.094e-02 1.083e+04 \n#> \n#> Coefficients (random effects):\n#> 1 | sample \n#>    0.07995\n\nAnd, glances() shows that the model with the random intercepts is a better fit to the data than the model without random intercepts.\n\nspmod <- splm(log_Zn ~ log_dist2road,\n              data = moss, spcov_type = \"exponential\")\nglances(spmod, randint)\n#> # A tibble: 2 × 10\n#>   model      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 randi…   365     2     4  335.  343.  343.  -168.     363.            0.661\n#> 2 spmod    365     2     3  367.  373.  373.  -184.     363.            0.683\n\nAs another example, we might consider a model that also has random intercepts for year, or, a model that also has both random intercepts for year and random slopes for log_dist2road within year:\n\nyearint <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ (1 | sample + year))\nyearsl <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ (1 | sample) + \n                       (log_dist2road | year))\n\nglances() shows that, of these four models, the model that includes random intercepts for sample, random intercepts for year, and random slopes for year is best, according to the AIC and AICc metrics.\n\nglances(spmod, randint, yearint, yearsl)\n#> # A tibble: 4 × 10\n#>   model      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 yearsl   365     2     6  190.  202.  202.  -94.9     363.            0.215\n#> 2 yeari…   365     2     4  230.  238.  238. -115.      363.            0.729\n#> 3 randi…   365     2     4  335.  343.  343. -168.      363.            0.661\n#> 4 spmod    365     2     3  367.  373.  373. -184.      363.            0.683\n\n\n\n\n\n\n\nNote\n\n\n\nThe syntax ~ (log_dist2road | year) specifies that both random intercepts for year and random slopes for log_dist2road within year should be included in the model. If only random slopes are desired, then we should set random to ~ (-1 + log_dist2road | year).\n\n\n\n\n\n\n\n\nExercise\n\n\n\nPerhaps a model with random intercepts for sample and random intercepts and slopes for year but without any spatial covariance is an even better fit to the data. Fit such a model by specifying spcov_type to be \"none\". Then, use glances() to see how well this non-spatial model fits the moss data compared to the spatially explicit models.\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\nnospcov <- splm(log_Zn ~ log_dist2road,\n                    data = moss, spcov_type = \"none\",\n                    random = ~ (1 | sample) + \n                      (log_dist2road | year))\nglances(spmod, randint, yearint, yearsl, nospcov)\n#> # A tibble: 5 × 10\n#>   model      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 yearsl   365     2     6  190.  202.  202.  -94.9     363.            0.215\n#> 2 yeari…   365     2     4  230.  238.  238. -115.      363.            0.729\n#> 3 randi…   365     2     4  335.  343.  343. -168.      363.            0.661\n#> 4 spmod    365     2     3  367.  373.  373. -184.      363.            0.683\n#> 5 nospc…   365     2     4  456.  464.  464. -228.      363             0.119\n## the model with no explicit spatial covariance has the worst fit \n## of the five models.\n\n\n\n\n\n2.2.3 Anisotropy\nBy default, splm() uses isotropic spatial covariance. Spatial covariance is isotropic if it behaves similarly in all directions. A spatial covariance is (geometrically) anisotropic if it does not behave similarly in all directions. Anisotropic models require estimation of two additional parameters: rotate and scale, which control the behavior of the spatial covariance as a function of distance and direction.\n\naniso <- splm(log_Zn ~ log_dist2road,\n              data = moss, spcov_type = \"exponential\",\n              anisotropy = TRUE)\naniso\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_type = \"exponential\", \n#>     anisotropy = TRUE)\n#> \n#> \n#> Coefficients (fixed):\n#>   (Intercept)  log_dist2road  \n#>         9.548         -0.546  \n#> \n#> \n#> Coefficients (exponential spatial covariance):\n#>        de         ie      range     rotate      scale  \n#> 3.561e-01  6.812e-02  8.732e+03  2.435e+00  4.753e-01\n\nWe can again use glances to compare the model that allows for anisotropy with the isotropic model:\n\nglances(spmod, aniso)\n#> # A tibble: 2 × 10\n#>   model     n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr> <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 aniso   365     2     5  362.  372.  372.  -181.     363             0.705\n#> 2 spmod   365     2     3  367.  373.  373.  -184.     363.            0.683\n\nThe anisotropic model does have lower AIC and AICc than the isotropic model, indicating a better fit. However, the reduction in AIC and AICc is quite small, so we may still prefer the isotropic model for simplicity and interpretability.\n\n\n\n\n\n\nExercise\n\n\n\nVisualize the anisotropic level curve for aniso using plot(). Hint: Run ?plot.spmodel or visit this link. Which direction does the model predict two responses will be more correlated?\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\nplot(aniso, which = 8)\n\n\n\n\nA clockwise rotation of this level curve by rotate followed by a scaling of the minor axis by the reciprocal of scale yields a spatial covariance that is isotropic.\n\n\n\n\n2.2.4 Partition Factors\nA partition factor is a categorical (or factor) variable that forces observations in different levels of the partition factor to be uncorrelated. The year variable in moss has two levels, 2001 and 2006, which correspond to the year of measurement. Suppose the goal is to fit a model that assumes observations from the same year are spatially correlated but observations from different years are not spatially correlated. In this context, year is a partition factor. We fit this model by running\n\npart <- splm(log_Zn ~ log_dist2road,\n             data = moss, spcov_type = \"exponential\",\n             partition_factor = ~ year)\n\nLike the formula and random arguments, the partition_factor argument requires a formula object.\n\n2.2.5 Fixing Covariance Parameters\nBy default, splm() estimates all unknown covariance parameters. However, we can also fix covariance parameters at known values with the spcov_initial argument for spatial covariance parameters and with the randcov_initial argument for non-spatial covariance parameters.\nAs an example, suppose that we want to fit a \"spherical\" covariance model to the moss data, but that, we want to fix the range at 20000 units so that errors from spatial locations more than 20000 units apart are not spatially correlated. We first create an spcov_initial object with the spcov_initial() function:\n\ninit_spher <- spcov_initial(\"spherical\", range = 20000, known = \"range\")\ninit_spher\n#> $initial\n#> range \n#> 20000 \n#> \n#> $is_known\n#> range \n#>  TRUE \n#> \n#> attr(,\"class\")\n#> [1] \"spherical\"\n\nWithin the function call, we specify that, for a \"spherical\" covariance, we would like to set the range parameter to 20000 and for that value to be known and therefore fixed in any subsequent estimation. We then provide init_spher as an argument to spcov_initial in splm():\n\nsplm(log_Zn ~ log_dist2road, data = moss,\n     spcov_initial = init_spher)\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_initial = init_spher)\n#> \n#> \n#> Coefficients (fixed):\n#>   (Intercept)  log_dist2road  \n#>        9.7194        -0.5607  \n#> \n#> \n#> Coefficients (spherical spatial covariance):\n#>        de         ie      range  \n#> 4.545e-01  8.572e-02  2.000e+04\n\nWhen spcov_initial is provided, spcov_type is not a necessary argument to splm().\n\n\n\n\n\n\nExercise\n\n\n\nFit a \"spherical\" spatial covariance model to the moss data set without a nugget effect (i.e., the model should have the ie independent variance parameter set to 0 and treated as known). Verify in the summary output that the ie is indeed 0 for this model.\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\ninit_no_ie <- spcov_initial(\"spherical\", ie = 0, known = \"ie\")\nno_ie <- splm(log_Zn ~ log_dist2road, data = moss,\n              spcov_initial = init_no_ie)\nsummary(no_ie)\n#> \n#> Call:\n#> splm(formula = log_Zn ~ log_dist2road, data = moss, spcov_initial = init_no_ie)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -3.1766 -1.8420 -1.2975 -0.7249  0.6577 \n#> \n#> Coefficients (fixed):\n#>               Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)   10.27912   28.99660   0.354    0.723    \n#> log_dist2road -0.56642    0.01974 -28.693   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Pseudo R-squared: 0.6967\n#> \n#> Coefficients (spherical spatial covariance):\n#>        de        ie     range \n#> 8.433e+02 0.000e+00 3.699e+07\n\n\n\n\n\n2.2.6 Random Forest Spatial Residual Models\nRandom forests are a popular machine-learning modeling tool. The random forest spatial residual model available in spmodel combines random forest modeling and spatial linear models. First, the model is fit using random forests and fitted values are obtained. Then the response residuals are used to fit a spatial linear model. Predictions at unobserved locations are computed as the sum of the random forest prediction and the predicted (i.e., Kriged) response residual from the spatial linear model. Suppose we split the moss data into training and test data sets, with the goal of predicting log_Zn in the test data.\n\nset.seed(1)\nn <- NROW(moss)\nn_train <- round(0.75 * n)\nn_test <- n - n_train\ntrain_index <- sample(n, size = n_train)\nmoss_train <- moss[train_index, , ]\nmoss_test <- moss[-train_index, , ]\n\nWe fit a random forest spatial residual model to the test data by running\n\nrfsrmod <- splmRF(log_Zn ~ log_dist2road, moss_train,\n                  spcov_type = \"exponential\")\n\nWe make predictions for the test data by running\n\n# results omitted\npredict(rfsrmod, moss_test)\n\n\n\n\n\n\n\nExercise\n\n\n\nUse predict() to store the random forest spatial residual predictions of log_Zn at locations in the test data and then compute the mean-squared prediction error (MSPE). Compare this MSPE to the MSPE from fitting a spatial linear model with an exponential covariance function.\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\nrf_preds <- predict(rfsrmod, newdata = moss_test)\nrf_errors <- moss_test$log_Zn - rf_preds\nmean(rf_errors^2)\n#> [1] 0.1849228\n\nsplmmod <- splm(log_Zn ~ log_dist2road, moss_train,\n                  spcov_type = \"exponential\")\nsplm_preds <- predict(splmmod, newdata = moss_test)\nsplm_errors <- moss_test$log_Zn - splm_preds\nmean(splm_errors^2)\n#> [1] 0.1514774\n\nFor these data, the spatial linear model yielded more accurate predictions (lower MSPE)."
  },
  {
    "objectID": "splm-additional.html#areal-data",
    "href": "splm-additional.html#areal-data",
    "title": "2  Additional spmodel Features",
    "section": "\n2.3 Areal Data",
    "text": "2.3 Areal Data\n\n2.3.1 Data Introduction\nThroughout the section, we will use the seal data in the spmodel package. The seal data is an sf object with a POLYGON geometry. There are 62 polygons in the data, some of which have non-missing values of log_trend, which is the log of the estimated harbor-seal trends that were calculated from abundance data.\nThe following code generates a visualization of the seal data: polygons that are grey have a missing value for log_trend.\n\nggplot(seal, aes(fill = log_trend)) +\n  geom_sf() +\n  scale_fill_viridis_c() +\n  theme_bw(base_size = 14) \n\n\n\n\nOur goal is to fit a spatial autoregressive model (Equation 1.4 and Equation 1.6) to the log_trend response variable with the spautor() function. Then, we will use the fitted model to predict the log_trend for sites where log_trend is not recorded.\n\n2.3.2 spautor() Syntax and Output Interpretation\nThe syntax for fitting a model to areal data with spautor() is very similar to that used for splm(). Again, there are generally at least three required arguments:\n\n\nformula: a formula that describes the relationship between the response variable (\\(\\mathbf{y}\\)) and explanatory variables (\\(\\mathbf{X}\\))\n\n\nformula in spautor() is the same as formula in lm() and splm()\n\n\n\n\ndata: a data.frame or sf object that contains the response variable, explanatory variables, and spatial information. Note that if data is a data.frame, then W is an additional required argument to spautor().\n\nspcov_type: the spatial covariance type (\"car\" or \"sar\")\n\nWe can fit a conditional auto-regressive (CAR) model with\n\nsealmod <- spautor(log_trend ~ 1, data = seal, spcov_type = \"car\")\nsummary(sealmod)\n#> \n#> Call:\n#> spautor(formula = log_trend ~ 1, data = seal, spcov_type = \"car\")\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.34455 -0.10417  0.04410  0.07338  0.20475 \n#> \n#> Coefficients (fixed):\n#>             Estimate Std. Error z value Pr(>|z|)   \n#> (Intercept) -0.07090    0.02497  -2.839  0.00452 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Coefficients (car spatial covariance):\n#>      de   range   extra \n#> 0.03252 0.42037 0.02177\n\n\n\n\nWe can relate some of the components in the summary output to the model in Equation 1.4 and Equation 1.6:\n\nthe value in the Estimate column of the Coefficients (fixed) table form \\(\\boldsymbol{\\hat{\\beta}}\\), an estimate of \\(\\boldsymbol{\\beta}\\).\nthe de value of 0.033 in the Coefficients (car spatial covariance) table is \\(\\hat{\\sigma}^2_{de}\\), which is an estimate of \\(\\sigma^2_{de}\\), the variance of \\(\\boldsymbol{\\tau}\\).\nthe range value of 0.42 in the Coefficients (car spatial covariance) table is \\(\\hat{\\phi}\\), an estimate of \\(\\phi\\) in Equation 1.6.\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, \\(\\sigma^2_{ie}\\) is assumed to be 0 for autoregressive models and hence, ie is omitted from the summary output.\n\n\nThough the weight matrix \\(\\mathbf{W}\\) in Equation 1.6 used in the model does not appear in the summary output, we can pull the weight matrix from the sealmod object with\n\nsealmod$W\n\nBy default, spautor() uses queen contiguity to form the weight matrix: observations are “neighbors” if they share at least one boundary (even if that boundary is a single point). Recall that observations are not considered neighbors with themselves. Also by default, spautor() row standardizes the weight matrix so that each of the rows in \\(\\mathbf{W}\\) sum to \\(1\\). Row standardization of the weight matrix is performed by default because doing so results in “nice” properties of the resulting covariance matrix (Ver Hoef et al. 2018). The first row of the weight matrix is\n\nsealmod$W[1, ]\n#>  [1] 0.0000000 0.3333333 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#>  [8] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [15] 0.0000000 0.3333333 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [22] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.3333333\n#> [29] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [36] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [43] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [50] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n#> [57] 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000 0.0000000\n\nThe output indicates that the first observation is neighbors with the second observation, the sixteenth observation, and the twenty-eighth observation.\n\nwhich(sealmod$W[1, ] > 0)\n#> [1]  2 16 28\n\nFinally, if we re-examine \\(\\mathbf{W}\\), we can note that some rows of \\(\\mathbf{W}\\) do not have any positive values, indicating that some observations in the data have no neighbors. Looking back on the plot of the data, we see that there are indeed a few “island” sites that do not share a boundary with any other polygons. The errors for these spatial locations are assumed to be uncorrelated with all other random errors, and, they are given a unique variance parameter that is the extra spatial covariance estimate in the summary output of the model.\n\n2.3.3 Additional Analysis\nMost of the helper functions for models fit with splm() are also useful for models fit with spautor(). Additionally, most of the additional arguments for splm() are also additional arguments for spautor().\nAll helper functions available for splm() model objects are also available for spautor() model objects:\n\n\naugment(), glance(), and glances()\n\nmodel fit statistics with AIC(), AICc() and GR2()\n\nmodel diagnostics statistics with cooks.distance(), residuals(), fitted(), etc.\n\nspautor() model objects accommodate all additional arguments previously mentioned except big data sets and anisotropy.\n\n\n\n\n\n\nNote\n\n\n\nBig data applications are not available because the models are parameterized in terms of their inverse covariance matrix, not the covariance matrix, which makes the “local” approach infeasible. The anisotropy argument is not available for spautor() because the covariance for an autoregressive model is based on the neighborhood structure of the spatial locations, not on distance.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nChoose a couple of the helper functions that you would like to explore and apply those functions to the fitted seal model.\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\nAIC(sealmod)\n#> [1] -30.87584\nfitted(sealmod)\n#>           2           3           4           5           6           7 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>           8          10          11          12          14          16 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>          17          20          21          22          23          24 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>          25          26          28          29          30          31 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>          33          34          35          37          38          39 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 -0.07090121 \n#>          41          45          59          60 \n#> -0.07090121 -0.07090121 -0.07090121 -0.07090121\n\n\n\n\n\n2.3.4 Prediction with Areal Data\nPrediction of response values for unobserved polygons with areal data requires that the polygons with missing response values be included in the data argument supplied to spautor(). The reason for this requirement is that exclusion of these polygons changes the underlying neighborhood structure of the data, and, therefore changes the covariance matrix.\nFor areal data, we can obtain predictions for unobserved polygons using predict() on the fitted model object or augment() on the fitted model object, specifying the newdata argument to be mod$newdata. Both approaches are given below:\n\nsealmod <- spautor(log_trend ~ 1, data = seal, spcov_type = \"car\")\nsummary(sealmod)\n\npredict(sealmod)\n\n\naugment(sealmod, newdata = sealmod$newdata)\n#> Simple feature collection with 28 features and 2 fields\n#> Geometry type: POLYGON\n#> Dimension:     XY\n#> Bounding box:  xmin: 913618.8 ymin: 1007542 xmax: 1115097 ymax: 1132682\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 28 × 3\n#>    log_trend  .fitted                                                geometry\n#>  *     <dbl>    <dbl>                                           <POLYGON [m]>\n#>  1        NA -0.115   ((1035002 1054710, 1035002 1054542, 1035002 1053542, 1…\n#>  2        NA -0.00908 ((1043093 1020553, 1043097 1020550, 1043101 1020550, 1…\n#>  3        NA -0.0602  ((1099737 1054310, 1099752 1054262, 1099788 1054278, 1…\n#>  4        NA -0.0359  ((1099002 1036542, 1099134 1036462, 1099139 1036431, 1…\n#>  5        NA -0.0723  ((1076902 1053189, 1076912 1053179, 1076931 1053179, 1…\n#>  6        NA -0.0548  ((1070501 1046969, 1070317 1046598, 1070308 1046542, 1…\n#>  7        NA -0.0976  ((1072995 1054942, 1072996 1054910, 1072997 1054878, 1…\n#>  8        NA -0.0714  ((960001.5 1127667, 960110.8 1127542, 960144.1 1127495…\n#>  9        NA -0.0825  ((1031308 1079817, 1031293 1079754, 1031289 1079741, 1…\n#> 10        NA -0.0592  ((998923.7 1053647, 998922.5 1053609, 998950 1053631, …\n#> # ℹ 18 more rows\n\n\n\n\n\n\n\nNote\n\n\n\nThe mod$newdata syntax also works for models fit with splm(), where the data used contains missing values for the response variable at any unobserved locations.\n\n\n\n\n\n\n\n\nExercise\n\n\n\nVerify that the fitted autoregressive model with the seal data changes when the polygons with missing response values are excluded from the data argument in spautor(). The following code creates a data without the polygons with missing values:\n\nis_missing <- is.na(seal$log_trend)\nseal_nomiss <- seal[!is_missing, , ]\n\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\nsealmod_nomiss <- spautor(log_trend ~ 1,\n                          data = seal_nomiss, spcov_type = \"car\")\nprint(sealmod)\n#> \n#> Call:\n#> spautor(formula = log_trend ~ 1, data = seal, spcov_type = \"car\")\n#> \n#> \n#> Coefficients (fixed):\n#> (Intercept)  \n#>     -0.0709  \n#> \n#> \n#> Coefficients (car spatial covariance):\n#>      de    range    extra  \n#> 0.03252  0.42037  0.02177\nprint(sealmod_nomiss)\n#> \n#> Call:\n#> spautor(formula = log_trend ~ 1, data = seal_nomiss, spcov_type = \"car\")\n#> \n#> \n#> Coefficients (fixed):\n#> (Intercept)  \n#>    -0.08152  \n#> \n#> \n#> Coefficients (car spatial covariance):\n#>      de    range    extra  \n#> 0.02297  0.41280  0.01958"
  },
  {
    "objectID": "splm-additional.html#sec-simulate-gauss",
    "href": "splm-additional.html#sec-simulate-gauss",
    "title": "2  Additional spmodel Features",
    "section": "\n2.4 Simulating Spatial Gaussian Data",
    "text": "2.4 Simulating Spatial Gaussian Data\nWe simulate Gaussian spatial data using sprnorm(). sprnorm() is similar in structure to rnorm() for simulating non-spatial Gaussian data. The first argument to sprnorm() is spcov_params, which is a spatial covariance parameter object created with spcov_params():\n\nparams <- spcov_params(\"exponential\", de = 1, ie = 0.5, range = 5e5)\n\n\n\n\n\n\n\nNote\n\n\n\nWhen the type argument to coef() is \"spcov\", the estimated spatial covariance parameters are returned as an spcov_params object, naturally usable simulation-based contexts that require conditioning on these estimated parameters.\n\n\nsprnorm() simulates data at each location in data for each of n samples (specified via n) with some mean vector (specified via mean). We simulate one realization of zero-mean Gaussian data with spatial covariance structure from params at each location in the sulfate data by running\n\nset.seed(1)\nsulfate$z <- sprnorm(params, data = sulfate)\n\n\n\n\n\n\n\nCaution\n\n\n\nSimulating spatial data in spmodel requires the inverse (more rigorously, the Cholesky decomposition) of the covariance matrix, which can take awhile for sample sizes exceeding 10,000. Regardless of the number of realizations simulated, this inverse is only needed once, which means that simulating many realizations (via samples) takes nearly the same time as simulating just one.\n\n\nWe visualize this realization by running\n\nggplot(sulfate, aes(color = z)) +\n  geom_sf() +\n  scale_color_viridis_c() +\n  theme_gray(base_size = 14)\n\n\n\n\nWe visualize an empirical semivariogram of this realization by running\n\nesv_out <- esv(z ~ 1, sulfate)\nggplot(esv_out, aes(x = dist, y = gamma, size = np)) +\n  geom_point() +\n  lims(y = c(0, NA)) +\n  theme_gray(base_size = 14)\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe empirical semivariogram is a diagnostic tool that can be used to characterize spatial dependence. On the x-axis is distance between observations. On the y-axis is the average squared difference between response variables (semivariance) in different distance “bins”. Typically when spatial dependence exists, the semivariance is smaller at short distances and larger at far distances. In the figure above, the size of the circle is proportional to the number of unique response variable pairs used in the distance bin. To assess leftover spatial dependence in a model, typically semivariograms are constructed on residuals from the nonspatial fitted model. The empirical semivariogram is intimately connected to empirical correlations/correllograms."
  },
  {
    "objectID": "splm-additional.html#r-code-appendix",
    "href": "splm-additional.html#r-code-appendix",
    "title": "2  Additional spmodel Features",
    "section": "\n2.5 R Code Appendix",
    "text": "2.5 R Code Appendix\n\n\n\n\nlibrary(spmodel)\nlibrary(ggplot2)\nset.seed(06022024)\nsim_params <- spcov_params(\"exponential\", de = 7, ie = 2, range = 0.7)\n\nn <- 5000\nx <- runif(n)\ny <- runif(n)\nsim_coords <- data.frame(x, y)\n\nsim_response <- sprnorm(sim_params, data = sim_coords, xcoord = x, ycoord = y)\nsim_data <- data.frame(sim_coords, sim_response)\nggplot(sim_data, aes(x = x, y = y, color = sim_response)) +\n  geom_point() +\n  scale_color_viridis_c(limits = c(-8, 9)) +\n  theme_gray(base_size = 14)\nfit_start_time <- proc.time()\nbdmod <- splm(sim_response ~ 1, data = sim_data,\n     spcov_type = \"exponential\",\n     xcoord = x, ycoord = y,\n     local = TRUE)\nfit_end_time <- proc.time()\nfit_end_time - fit_start_time\nsummary(bdmod)\nn_pred <- 3000\nx_pred <- runif(n_pred)\ny_pred <- runif(n_pred)\nsim_preds <- tibble::tibble(x = x_pred, y = y_pred)\npred_start_time <- proc.time()\nsim_preds$preds <- predict(bdmod, newdata = sim_preds, local = TRUE)\npred_end_time <- proc.time()\npred_end_time - pred_start_time\nggplot(sim_preds, aes(x = x, y = y, color = preds)) +\n  geom_point() +\n  scale_color_viridis_c(limits = c(-8, 9)) +\n  theme_gray(base_size = 14)\nspmods <- splm(formula = log_Zn ~ log_dist2road, data = moss,\n              spcov_type = c(\"exponential\", \"gaussian\"))\nnames(spmods)\nglances(spmods)\ntidy(spmods$gaussian, conf.int = TRUE, conf.level = 0.90)\nconfint(spmods$gaussian, level = 0.90)\nmoss\nrandint <- splm(log_Zn ~ log_dist2road,\n                data = moss, spcov_type = \"exponential\",\n                random = ~ (1 | sample))\nrandint <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ sample)\nsummary(randint)\nspmod <- splm(log_Zn ~ log_dist2road,\n              data = moss, spcov_type = \"exponential\")\nglances(spmod, randint)\nyearint <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ (1 | sample + year))\nyearsl <- splm(log_Zn ~ log_dist2road,\n                      data = moss, spcov_type = \"exponential\",\n                      random = ~ (1 | sample) + \n                       (log_dist2road | year))\nglances(spmod, randint, yearint, yearsl)\nnospcov <- splm(log_Zn ~ log_dist2road,\n                    data = moss, spcov_type = \"none\",\n                    random = ~ (1 | sample) + \n                      (log_dist2road | year))\nglances(spmod, randint, yearint, yearsl, nospcov)\n## the model with no explicit spatial covariance has the worst fit \n## of the five models.\naniso <- splm(log_Zn ~ log_dist2road,\n              data = moss, spcov_type = \"exponential\",\n              anisotropy = TRUE)\naniso\nglances(spmod, aniso)\nplot(aniso, which = 8)\npart <- splm(log_Zn ~ log_dist2road,\n             data = moss, spcov_type = \"exponential\",\n             partition_factor = ~ year)\ninit_spher <- spcov_initial(\"spherical\", range = 20000, known = \"range\")\ninit_spher\nsplm(log_Zn ~ log_dist2road, data = moss,\n     spcov_initial = init_spher)\ninit_no_ie <- spcov_initial(\"spherical\", ie = 0, known = \"ie\")\nno_ie <- splm(log_Zn ~ log_dist2road, data = moss,\n              spcov_initial = init_no_ie)\nsummary(no_ie)\nset.seed(1)\nn <- NROW(moss)\nn_train <- round(0.75 * n)\nn_test <- n - n_train\ntrain_index <- sample(n, size = n_train)\nmoss_train <- moss[train_index, , ]\nmoss_test <- moss[-train_index, , ]\nrfsrmod <- splmRF(log_Zn ~ log_dist2road, moss_train,\n                  spcov_type = \"exponential\")\n# results omitted\npredict(rfsrmod, moss_test)\nrf_preds <- predict(rfsrmod, newdata = moss_test)\nrf_errors <- moss_test$log_Zn - rf_preds\nmean(rf_errors^2)\n\nsplmmod <- splm(log_Zn ~ log_dist2road, moss_train,\n                  spcov_type = \"exponential\")\nsplm_preds <- predict(splmmod, newdata = moss_test)\nsplm_errors <- moss_test$log_Zn - splm_preds\nmean(splm_errors^2)\nggplot(seal, aes(fill = log_trend)) +\n  geom_sf() +\n  scale_fill_viridis_c() +\n  theme_bw(base_size = 14) \nsealmod <- spautor(log_trend ~ 1, data = seal, spcov_type = \"car\")\nsummary(sealmod)\nspcov_params_car <- coef(sealmod, type = \"spcov\")\nde_car <- as.vector(round(spcov_params_car[[\"de\"]], digits = 3))\nrange_car <- as.vector(round(spcov_params_car[[\"range\"]], digits = 3))\nsealmod$W\nsealmod$W[1, ]\nwhich(sealmod$W[1, ] > 0)\nAIC(sealmod)\nfitted(sealmod)\nsealmod <- spautor(log_trend ~ 1, data = seal, spcov_type = \"car\")\nsummary(sealmod)\n\npredict(sealmod)\naugment(sealmod, newdata = sealmod$newdata)\nis_missing <- is.na(seal$log_trend)\nseal_nomiss <- seal[!is_missing, , ]\nsealmod_nomiss <- spautor(log_trend ~ 1,\n                          data = seal_nomiss, spcov_type = \"car\")\nprint(sealmod)\nprint(sealmod_nomiss)\nparams <- spcov_params(\"exponential\", de = 1, ie = 0.5, range = 5e5)\nset.seed(1)\nsulfate$z <- sprnorm(params, data = sulfate)\nggplot(sulfate, aes(color = z)) +\n  geom_sf() +\n  scale_color_viridis_c() +\n  theme_gray(base_size = 14)\nesv_out <- esv(z ~ 1, sulfate)\nggplot(esv_out, aes(x = dist, y = gamma, size = np)) +\n  geom_point() +\n  lims(y = c(0, NA)) +\n  theme_gray(base_size = 14)\n\n\n\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015. “Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical Software 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nPinheiro, José, and Douglas Bates. 2006. Mixed-Effects Models in S and S-PLUS. Springer science & business media.\n\n\nVer Hoef, Jay M, Michael Dumelle, Matt Higham, Erin E Peterson, and Daniel J Isaak. 2023. “Indexing and Partitioning the Spatial Linear Model for Large Data Sets.” Plos One 18 (11): e0291906.\n\n\nVer Hoef, Jay M, Erin E Peterson, Mevin B Hooten, Ephraim M Hanks, and Marie-Josèe Fortin. 2018. “Spatial Autoregressive Models for Statistical Inference from Ecological Data.” Ecological Monographs 88 (1): 36–59."
  },
  {
    "objectID": "spglm.html#the-spatial-generalized-linear-model",
    "href": "spglm.html#the-spatial-generalized-linear-model",
    "title": "3  Spatial Generalized Linear Models in spmodel",
    "section": "\n3.1 The Spatial Generalized Linear Model",
    "text": "3.1 The Spatial Generalized Linear Model\nAs with spatial linear models, spatial generalized linear models can be fit in spmodel for point-referenced and areal data. A generalized linear model essentially uses the right-hand-side of Equation 1.4 as a model for a function of the mean of the response vector \\(\\mathbf{y}\\). More formally, the spatial generalized linear model can be written as \\[\ng(\\boldsymbol{\\mu}) = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\tau} + \\boldsymbol{\\epsilon},\n\\] where \\(g(\\boldsymbol{\\mu})\\) is the link function that “links” a function of the mean of \\(\\mathbf{y}\\) to \\(\\mathbf{X} \\boldsymbol{\\beta}\\), \\(\\boldsymbol{\\tau}\\), and \\(\\boldsymbol{\\epsilon}\\). For example, in a spatial Poisson generalized linear model, each element of \\(\\mathbf{y}\\), \\(\\text{y}_i\\), is modeled as a Poisson random variable with mean \\(\\mu_i\\). Denoting the vector of means as \\(\\boldsymbol{\\mu}\\), the log of the mean vector of \\(\\mathbf{y}\\) is then modeled as\n\\[\n\\text{log}(\\boldsymbol{\\mu}) = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\tau} + \\boldsymbol{\\epsilon},\n\\]\nwhere the \\(\\text{log()}\\) function is applied element-wise over the mean vector \\(\\boldsymbol{\\mu}\\), which is the expected value of \\(\\mathbf{y}\\). In this example, the link function used is the log link. In the binomial generalized linear model family, a popular link function is the logit link, so that the model for the mean of a binomial response vector is\n\\[\n\\text{log}\\left(\\frac{\\boldsymbol{\\mu}}{1 - \\boldsymbol{\\mu}}\\right) = \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{\\tau} + \\boldsymbol{\\epsilon},\n\\] Table 3.1 shows the response distributions, data types, and link functions available in spmodel.\n\n\nTable 3.1: Response distributions and link functions available in spmodel\n\n\nDistribution\nData Type\nLink Function\n\n\n\nPoisson\nCount\nLog\n\n\nNegative Binomial\nCount\nLog\n\n\nBinomial\nBinary\nLogit\n\n\nBeta\nProportion\nLogit\n\n\nGamma\nSkewed\nLog\n\n\nInverse Gaussian\nSkewed\nLog"
  },
  {
    "objectID": "spglm.html#model-fitting",
    "href": "spglm.html#model-fitting",
    "title": "3  Spatial Generalized Linear Models in spmodel",
    "section": "\n3.2 Model Fitting",
    "text": "3.2 Model Fitting\nThe spglm() function is used to fit spatial generalized linear models for point-referenced data, and the spgautor() function is used to fit spatial generalized linear models for areal data. spglm() and spgautor() share similar syntax with splm() and spautor(), respectively, though one additional argument is required:\n\n\nfamily: The generalized linear model family (i.e., the distribution of \\(\\mathbf{y}\\)). The family argument can be binomial, beta, Poisson, nbinomial, Gamma, or inverse.gaussian.\n\n\n\n\n\n\n\nNote\n\n\n\nThe family argument in spglm() and spgautor() uses similar syntax as the family argument in glm(). One difference, however, is that the link function for the spmodel functions is fixed. For binomial and beta responses, that link is the logit link function, while for Poisson, negative binomial, gamma, and inverse gaussian responses, that link is the log link function.\n\n\nWhile spatial generalized linear models can be fit to both point-referenced and areal data, we focus only on fitting spatial generalized linear models to point-referenced data with spglm() for the remainder of this section. Models are fit using a novel application of the Laplace approximation – Ver Hoef et al. (2023) provide further details.\nWe observed in Section 1.3 that a generalized linear model may be a better choice for the count data in the Alaska moose. We specify a Poisson spatial generalized linear model with the following:\n\npoismod <- spglm(count ~ elev * strat, data = moose,\n               family = poisson, spcov_type = \"spherical\")\nsummary(poismod)\n#> \n#> Call:\n#> spglm(formula = count ~ elev * strat, family = poisson, data = moose, \n#>     spcov_type = \"spherical\")\n#> \n#> Deviance Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.4245 -0.7783 -0.3653  0.1531  0.5900 \n#> \n#> Coefficients (fixed):\n#>              Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept) -2.230575   0.958201  -2.328 0.019919 *  \n#> elev         0.007623   0.003129   2.437 0.014820 *  \n#> stratM       2.752234   0.782853   3.516 0.000439 ***\n#> elev:stratM -0.010248   0.004472  -2.292 0.021928 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Pseudo R-squared: 0.09573\n#> \n#> Coefficients (spherical spatial covariance):\n#>        de        ie     range \n#>     3.892     1.163 51204.657 \n#> \n#> Coefficients (Dispersion for poisson family):\n#> dispersion \n#>          1\n\nAs with spatial linear models, the broom functions tidy(), glance() and augment(), as well as many other generic functions like plot(), are available for spatial generalized linear models. For example, we glance at the fitted model by running\n\nglance(poismod)\n#> # A tibble: 1 × 9\n#>       n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1   218     4     3 1332. 1338. 1338.  -666.     84.3           0.0957\n\n\n\n\n\n\n\nExercise\n\n\n\nFit a spatial negative binomial model to the moose data with count as the response and elev, strat, and their interaction as predictors. The negative binomial model relaxes the assumption in the spatial Poisson generalized linear model that the mean of a response variable \\(\\text{y}_i\\) and the variance of a response variable \\(text{y}_i\\) must be equal. Obtain a summary of the fitted model. Then compare their fits using loocv(). Which model is preferable based on the leave-one-out MSPE?\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\nnbmod <- spglm(count ~ elev * strat, data = moose,\n               family = nbinomial, spcov_type = \"spherical\")\nloocv(poismod)\n#> # A tibble: 1 × 3\n#>    bias  MSPE RMSPE\n#>   <dbl> <dbl> <dbl>\n#> 1  1.35  32.1  5.67\nloocv(nbmod)\n#> # A tibble: 1 × 3\n#>    bias  MSPE RMSPE\n#>   <dbl> <dbl> <dbl>\n#> 1 0.845  27.9  5.28\n\nnbmod has the lower loocv() error, suggesting it is a better fit to the data."
  },
  {
    "objectID": "spglm.html#spatial-prediction",
    "href": "spglm.html#spatial-prediction",
    "title": "3  Spatial Generalized Linear Models in spmodel",
    "section": "\n3.3 Spatial Prediction",
    "text": "3.3 Spatial Prediction\nWe can also make predictions of the mean function at unobserved locations. For example, we can use poismod to predict the mean number of moose (on the link scale) at the spatial locations in moose_preds using predict() by running:\n\n# results omitted\npredict(poismod, newdata = moose_preds)\n\nWe can also use augment():\n\naugment(poismod, newdata = moose_preds)\n#> Simple feature collection with 100 features and 3 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 269386.2 ymin: 1418453 xmax: 419976.2 ymax: 1541763\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 100 × 4\n#>     elev strat .fitted           geometry\n#>  * <dbl> <chr>   <dbl>        <POINT [m]>\n#>  1  143. L      0.207  (401239.6 1436192)\n#>  2  324. L     -0.0563 (352640.6 1490695)\n#>  3  158. L     -1.24   (360954.9 1491590)\n#>  4  221. M     -1.16   (291839.8 1466091)\n#>  5  209. M      1.78   (310991.9 1441630)\n#>  6  218. L     -1.84   (304473.8 1512103)\n#>  7  127. L     -2.80   (339011.1 1459318)\n#>  8  122. L     -2.45   (342827.3 1463452)\n#>  9  191  L     -0.409  (284453.8 1502837)\n#> 10  105. L     -1.10   (391343.9 1483791)\n#> # ℹ 90 more rows\n\nBy default, predict() and augment() return predictions on the link scale. We return predictions on the response scale by running\n\naugmod <- augment(poismod, newdata = moose_preds, type = \"response\")\n\nAnd we can visualize these predictions by running\n\nggplot(augmod, aes(color = .fitted)) +\n  geom_sf() +\n  scale_color_viridis_c(limits = c(0, 40)) +\n  theme_gray(base_size = 14)\n\n\n\n\n\n\n\n\n\n\nExercise\n\n\n\nUse spglm() to fit a spatial logistic regression model to the moose data using presence as the response variable and a Cauchy covariance function. Then, find the predicted probabilities that moose are present at the spatial locations in moose_preds (Hint: Use the type argument in predict() or augment()).\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\nbinmod <- spglm(presence ~ elev * strat, data = moose,\n               family = binomial, spcov_type = \"cauchy\")\naugment(binmod, newdata = moose_preds, type = \"response\")\n#> Simple feature collection with 100 features and 3 fields\n#> Geometry type: POINT\n#> Dimension:     XY\n#> Bounding box:  xmin: 269386.2 ymin: 1418453 xmax: 419976.2 ymax: 1541763\n#> Projected CRS: NAD83 / Alaska Albers\n#> # A tibble: 100 × 4\n#>     elev strat .fitted           geometry\n#>  * <dbl> <chr>   <dbl>        <POINT [m]>\n#>  1  143. L      0.489  (401239.6 1436192)\n#>  2  324. L      0.376  (352640.6 1490695)\n#>  3  158. L      0.0945 (360954.9 1491590)\n#>  4  221. M      0.229  (291839.8 1466091)\n#>  5  209. M      0.804  (310991.9 1441630)\n#>  6  218. L      0.0411 (304473.8 1512103)\n#>  7  127. L      0.0234 (339011.1 1459318)\n#>  8  122. L      0.0432 (342827.3 1463452)\n#>  9  191  L      0.391  (284453.8 1502837)\n#> 10  105. L      0.197  (391343.9 1483791)\n#> # ℹ 90 more rows"
  },
  {
    "objectID": "spglm.html#additional-spmodel-features",
    "href": "spglm.html#additional-spmodel-features",
    "title": "3  Spatial Generalized Linear Models in spmodel",
    "section": "\n3.4 Additional spmodel Features",
    "text": "3.4 Additional spmodel Features\nAll advanced features available in spmodel for spatial linear models are also available for spatial generalized linear models. This means that spatial generalized linear models in spmodel can accommodate big spatial data (though far less than in the spatial linear model case), fixing spatial covariance parameters, fitting and predicting for multiple models, non-spatial random effects (on the link scale), partition factors, anisotropy (on the link scale), and prediction. spmodel has a variety of additional simulation functions used to simulate binary, proportion, count, and skewed data:\n\nBinary data: sprbinom()\n\nProportion data: sprbeta()\n\nCount data: sprpois() and sprnbinom()\n\nSkewed data: sprgamma() and sprinvgauss()\n\n\nWith these simulation functions, the spatial covariance parameters and mean vector are specified on the appropriate link scale. For sprbinom() and sprbeta(), this is the logit link scale. For the other functions, this is the log link scale. We simulate one realization of Poisson data where on the link scale, the mean is zero and the spatial covariance structure is specified via params, by running\n\nparams <- spcov_params(\"exponential\", de = 1, ie = 0.5, range = 5e5)\nsulfate$p <- sprpois(params, data = sulfate)\n\nWe visualize this realization by running\n\nggplot(sulfate, aes(color = p)) +\n  geom_sf() +\n  scale_color_viridis_c() +\n  theme_gray(base_size = 14)"
  },
  {
    "objectID": "spglm.html#spatial-generalized-autoregressive-mdoels",
    "href": "spglm.html#spatial-generalized-autoregressive-mdoels",
    "title": "3  Spatial Generalized Linear Models in spmodel",
    "section": "\n3.5 Spatial Generalized Autoregressive Mdoels",
    "text": "3.5 Spatial Generalized Autoregressive Mdoels\nSo far we discussed how to formulate spatial generalized linear models using spglm(), building off the construction of spatial linear models fit using splm(). Similarly, we can formulate spatial generalized autoregressive models using spgautor(), building off the construction of spatial autoregressive models fit using spautor(). Spatial generalized autoregressive models can be fit to binomial, proportion, count, and skewed data by providing the family argument, as with the models fit using splm(). For example, we model exp(log_trend) (which is strictly positive) as a gamma random variable with the simultaneous autoregressive spatial covariance by running\n\nsealgmod <- spgautor(exp(log_trend) ~ 1, family = Gamma,\n                  data = seal, spcov_type = \"sar\")\nsummary(sealgmod)\n#> \n#> Call:\n#> spgautor(formula = exp(log_trend) ~ 1, family = Gamma, data = seal, \n#>     spcov_type = \"sar\")\n#> \n#> Deviance Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -0.31970 -0.10003  0.04354  0.07266  0.20661 \n#> \n#> Coefficients (fixed):\n#>             Estimate Std. Error z value Pr(>|z|)   \n#> (Intercept) -0.07106    0.02203  -3.226  0.00126 **\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Coefficients (sar spatial covariance):\n#>        de     range     extra \n#> 3.783e-04 2.164e-03 4.106e-05 \n#> \n#> Coefficients (Dispersion for Gamma family):\n#> dispersion \n#>      61.89"
  },
  {
    "objectID": "spglm.html#r-code-appendix",
    "href": "spglm.html#r-code-appendix",
    "title": "3  Spatial Generalized Linear Models in spmodel",
    "section": "\n3.6 R Code Appendix",
    "text": "3.6 R Code Appendix\n\n\n\n\nlibrary(spmodel)\nlibrary(ggplot2)\npoismod <- spglm(count ~ elev * strat, data = moose,\n               family = poisson, spcov_type = \"spherical\")\nsummary(poismod)\nglance(poismod)\nnbmod <- spglm(count ~ elev * strat, data = moose,\n               family = nbinomial, spcov_type = \"spherical\")\nloocv(poismod)\nloocv(nbmod)\n# results omitted\npredict(poismod, newdata = moose_preds)\naugment(poismod, newdata = moose_preds)\naugmod <- augment(poismod, newdata = moose_preds, type = \"response\")\nggplot(augmod, aes(color = .fitted)) +\n  geom_sf() +\n  scale_color_viridis_c(limits = c(0, 40)) +\n  theme_gray(base_size = 14)\nbinmod <- spglm(presence ~ elev * strat, data = moose,\n               family = binomial, spcov_type = \"cauchy\")\naugment(binmod, newdata = moose_preds, type = \"response\")\nparams <- spcov_params(\"exponential\", de = 1, ie = 0.5, range = 5e5)\nsulfate$p <- sprpois(params, data = sulfate)\nggplot(sulfate, aes(color = p)) +\n  geom_sf() +\n  scale_color_viridis_c() +\n  theme_gray(base_size = 14)\nsealgmod <- spgautor(exp(log_trend) ~ 1, family = Gamma,\n                  data = seal, spcov_type = \"sar\")\nsummary(sealgmod)\n\n\n\n\n\nVer Hoef, Jay M, Eryn Blagg, Michael Dumelle, Philip M Dixon, Dale L Zimmerman, and Paul Conn. 2023. “Marginal Inference for Hierarchical Generalized Linear Mixed Models with Patterned Covariance Matrices Using the Laplace Approximation.” arXiv Preprint arXiv:2305.02978."
  },
  {
    "objectID": "gis-in-r.html#goals-and-motivation",
    "href": "gis-in-r.html#goals-and-motivation",
    "title": "4  GIS in R",
    "section": "\n4.1 Goals and Motivation",
    "text": "4.1 Goals and Motivation\nMaintaining all analyses within a single software (R) can greatly simplify your research workflow. In this section, we’ll cover the basics of doing GIS in R.\nBy the end of this lesson, you should be able to:\n\nUnderstand the main features and types of vector data.\nGenerate point data from a set of latitudes and longitudes, such as from fields sites.\nRead, write, query, and manipulate vector data using the sf package.\nUnderstand the main features of raster data.\nAccess, manipulate, and stack raster layers.\nGenerate summaries of raster data within vector polygon layers."
  },
  {
    "objectID": "gis-in-r.html#points-lines-and-polygons",
    "href": "gis-in-r.html#points-lines-and-polygons",
    "title": "4  GIS in R",
    "section": "\n4.2 Points, lines, and polygons",
    "text": "4.2 Points, lines, and polygons\nThese data are a way of representing real-world features on a landscape in a highly simplified way. The simplest of these features is a point, which is a 0-dimensional feature that can be used to represent a specific location on the earth, such as a single tree or an entire city. Linear, 1-dimensional features can be represented with points (or vertices) that are connected by a path to form a line and when many points are connected these form a polyline. Finally, when a polyline’s path returns to its origin to represent an enclosed space, such as a forest, watershed boundary, or lake, this forms a polygon.\n\n\nFigure 4.1: Vector data. Image from: https://earthdatascience.org/courses/earth-analytics/spatial-data-r/intro-vector-data-r/\n\nWe can represent these features in R without actually using GIS packages. In this example, we’ll represent several cities in Oregon with common R data structures that you are probably already familiar with.\n\n\nlibrary(ggplot2)\n\nid &lt;- c(1:5)\ncities &lt;- c('Ashland','Corvallis','Bend','Portland','Newport')\nlongitude &lt;- c(-122.699, -123.275, -121.313, -122.670, -124.054)\nlatitude &lt;- c(42.189, 44.57, 44.061, 45.523, 44.652)\npopulation &lt;- c(20062, 50297, 61362, 537557, 9603)\n\noregon_cities &lt;- data.frame(id, cities, longitude, latitude, population)\n\n\nggplot(\n  data = oregon_cities, \n  aes(x = longitude, y = latitude, size = population, label = cities)\n) +\n  geom_point() +\n  geom_text(hjust = 1, vjust = 1) +\n  theme_bw()\n\n\n\nFigure 4.2: Oregon cities plotted from data frame.\n\n\n\nSo, is this sufficient for working with spatial data in R and doing spatial analysis? What are we missing?\nIf you have worked with vector data before, you may know that these data also usually have:\n\nA coordinate reference system\nA bounding box or extent\nPlot order\nAdditional data\n\nIn the next section we will introduce the sf package that will allow us to take fuller advantage of spatial features in R.\n\n4.2.1 Exploring the Simple Features (sf) package\nThe sf package provides simple features access for R. sf fits in within the “tidy” approach to data of Hadley Wickham’s tidyverse and is an ever-expanding package with 113 contributors in GitHub. In short, much of what used to require ArcGIS license can now be done in R with sf:\n\nlibrary(sf)\nls(\"package:sf\")\n#&gt;   [1] \"%&gt;%\"                          \"as_Spatial\"                  \n#&gt;   [3] \"dbDataType\"                   \"dbWriteTable\"                \n#&gt;   [5] \"gdal_addo\"                    \"gdal_create\"                 \n#&gt;   [7] \"gdal_crs\"                     \"gdal_extract\"                \n#&gt;   [9] \"gdal_inv_geotransform\"        \"gdal_metadata\"               \n#&gt;  [11] \"gdal_polygonize\"              \"gdal_rasterize\"              \n#&gt;  [13] \"gdal_read\"                    \"gdal_read_mdim\"              \n#&gt;  [15] \"gdal_subdatasets\"             \"gdal_utils\"                  \n#&gt;  [17] \"gdal_write\"                   \"gdal_write_mdim\"             \n#&gt;  [19] \"get_key_pos\"                  \"NA_agr_\"                     \n#&gt;  [21] \"NA_bbox_\"                     \"NA_crs_\"                     \n#&gt;  [23] \"NA_m_range_\"                  \"NA_z_range_\"                 \n#&gt;  [25] \"pivot_wider.sf\"               \"plot_sf\"                     \n#&gt;  [27] \"rawToHex\"                     \"read_sf\"                     \n#&gt;  [29] \"sf.colors\"                    \"sf_add_proj_units\"           \n#&gt;  [31] \"sf_extSoftVersion\"            \"sf_proj_info\"                \n#&gt;  [33] \"sf_proj_network\"              \"sf_proj_pipelines\"           \n#&gt;  [35] \"sf_proj_search_paths\"         \"sf_project\"                  \n#&gt;  [37] \"sf_use_s2\"                    \"st_agr\"                      \n#&gt;  [39] \"st_agr&lt;-\"                     \"st_area\"                     \n#&gt;  [41] \"st_as_binary\"                 \"st_as_grob\"                  \n#&gt;  [43] \"st_as_s2\"                     \"st_as_sf\"                    \n#&gt;  [45] \"st_as_sfc\"                    \"st_as_text\"                  \n#&gt;  [47] \"st_axis_order\"                \"st_bbox\"                     \n#&gt;  [49] \"st_bind_cols\"                 \"st_boundary\"                 \n#&gt;  [51] \"st_break_antimeridian\"        \"st_buffer\"                   \n#&gt;  [53] \"st_can_transform\"             \"st_cast\"                     \n#&gt;  [55] \"st_centroid\"                  \"st_collection_extract\"       \n#&gt;  [57] \"st_combine\"                   \"st_concave_hull\"             \n#&gt;  [59] \"st_contains\"                  \"st_contains_properly\"        \n#&gt;  [61] \"st_convex_hull\"               \"st_coordinates\"              \n#&gt;  [63] \"st_covered_by\"                \"st_covers\"                   \n#&gt;  [65] \"st_crop\"                      \"st_crosses\"                  \n#&gt;  [67] \"st_crs\"                       \"st_crs&lt;-\"                    \n#&gt;  [69] \"st_delete\"                    \"st_difference\"               \n#&gt;  [71] \"st_dimension\"                 \"st_disjoint\"                 \n#&gt;  [73] \"st_distance\"                  \"st_drivers\"                  \n#&gt;  [75] \"st_drop_geometry\"             \"st_equals\"                   \n#&gt;  [77] \"st_equals_exact\"              \"st_filter\"                   \n#&gt;  [79] \"st_geometry\"                  \"st_geometry_type\"            \n#&gt;  [81] \"st_geometry&lt;-\"                \"st_geometrycollection\"       \n#&gt;  [83] \"st_graticule\"                 \"st_inscribed_circle\"         \n#&gt;  [85] \"st_interpolate_aw\"            \"st_intersection\"             \n#&gt;  [87] \"st_intersects\"                \"st_is\"                       \n#&gt;  [89] \"st_is_empty\"                  \"st_is_longlat\"               \n#&gt;  [91] \"st_is_simple\"                 \"st_is_valid\"                 \n#&gt;  [93] \"st_is_within_distance\"        \"st_jitter\"                   \n#&gt;  [95] \"st_join\"                      \"st_layers\"                   \n#&gt;  [97] \"st_length\"                    \"st_line_merge\"               \n#&gt;  [99] \"st_line_sample\"               \"st_linestring\"               \n#&gt; [101] \"st_m_range\"                   \"st_make_grid\"                \n#&gt; [103] \"st_make_valid\"                \"st_minimum_rotated_rectangle\"\n#&gt; [105] \"st_multilinestring\"           \"st_multipoint\"               \n#&gt; [107] \"st_multipolygon\"              \"st_nearest_feature\"          \n#&gt; [109] \"st_nearest_points\"            \"st_node\"                     \n#&gt; [111] \"st_normalize\"                 \"st_overlaps\"                 \n#&gt; [113] \"st_perimeter\"                 \"st_point\"                    \n#&gt; [115] \"st_point_on_surface\"          \"st_polygon\"                  \n#&gt; [117] \"st_polygonize\"                \"st_precision\"                \n#&gt; [119] \"st_precision&lt;-\"               \"st_read\"                     \n#&gt; [121] \"st_read_db\"                   \"st_relate\"                   \n#&gt; [123] \"st_reverse\"                   \"st_sample\"                   \n#&gt; [125] \"st_segmentize\"                \"st_set_agr\"                  \n#&gt; [127] \"st_set_crs\"                   \"st_set_geometry\"             \n#&gt; [129] \"st_set_precision\"             \"st_sf\"                       \n#&gt; [131] \"st_sfc\"                       \"st_shift_longitude\"          \n#&gt; [133] \"st_simplify\"                  \"st_snap\"                     \n#&gt; [135] \"st_sym_difference\"            \"st_touches\"                  \n#&gt; [137] \"st_transform\"                 \"st_triangulate\"              \n#&gt; [139] \"st_triangulate_constrained\"   \"st_union\"                    \n#&gt; [141] \"st_viewport\"                  \"st_voronoi\"                  \n#&gt; [143] \"st_within\"                    \"st_wrap_dateline\"            \n#&gt; [145] \"st_write\"                     \"st_write_db\"                 \n#&gt; [147] \"st_z_range\"                   \"st_zm\"                       \n#&gt; [149] \"vec_cast.sfc\"                 \"vec_ptype2.sfc\"              \n#&gt; [151] \"write_sf\"\n\nImportant for us today - the sf package is fast and simple to use. To get a sense of why simple features are simple, let’s make one from our Oregon cities.\nIn this code we will…\n\nConvert an existing data frame to a simple feature by:\nSupply the longitude and latitude (x and y).\nDefine the (geographic) coordinate reference system (crs).\n\n\n\nprint(oregon_cities)\n#&gt;   id    cities longitude latitude population\n#&gt; 1  1   Ashland  -122.699   42.189      20062\n#&gt; 2  2 Corvallis  -123.275   44.570      50297\n#&gt; 3  3      Bend  -121.313   44.061      61362\n#&gt; 4  4  Portland  -122.670   45.523     537557\n#&gt; 5  5   Newport  -124.054   44.652       9603\n\noregon_cities &lt;- oregon_cities %&gt;%\n  st_as_sf(coords = c('longitude', 'latitude'), crs = 4269)\n\nprint(oregon_cities)\n#&gt; Simple feature collection with 5 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.054 ymin: 42.189 xmax: -121.313 ymax: 45.523\n#&gt; Geodetic CRS:  NAD83\n#&gt;   id    cities population                geometry\n#&gt; 1  1   Ashland      20062 POINT (-122.699 42.189)\n#&gt; 2  2 Corvallis      50297  POINT (-123.275 44.57)\n#&gt; 3  3      Bend      61362 POINT (-121.313 44.061)\n#&gt; 4  4  Portland     537557  POINT (-122.67 45.523)\n#&gt; 5  5   Newport       9603 POINT (-124.054 44.652)\n\n\n\n\n\n\n\nReading and Writing vector files\n\n\n\nFor reference, sf makes it very easy to write and read spatial objects in R, such as ESRI shapefiles:\nTo write an sf object out as an ESRI shapefile:\nst_write(oregon_cities, dsn = 'oregon_cities.shp')\nTo read an ESRI shapefile:\noregon_cities &lt;- st_read('oregon_cities.shp')\nThese operations can also be done with write_sf and read_sf but with slightly different options.\n\n\nWhen we print the oregon_cities object, we can see it has changed from being a standard data frame. It now includes features that are required for a true spatial object:\n\nGeometry type\nBounding box\nCoordinate reference system\n\nIn addition, we see that the latitude and longitude columns have been moved to a new column called \"geometry\" that appears in parentheses. Unlike sp, simple features contain the geometric information in a single column. An important note about the geometry column is that it is “sticky”, meaning that we can manipulate the sf object, such as with select in tidyverse (e.g., with dplyr) without explicitly referencing the geometry column without losing it:\n\nlibrary(tidyverse)\n\noregon_cities %&gt;% \n  select(cities)\n#&gt; Simple feature collection with 5 features and 1 field\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -124.054 ymin: 42.189 xmax: -121.313 ymax: 45.523\n#&gt; Geodetic CRS:  NAD83\n#&gt;      cities                geometry\n#&gt; 1   Ashland POINT (-122.699 42.189)\n#&gt; 2 Corvallis  POINT (-123.275 44.57)\n#&gt; 3      Bend POINT (-121.313 44.061)\n#&gt; 4  Portland  POINT (-122.67 45.523)\n#&gt; 5   Newport POINT (-124.054 44.652)\n\nIn the above example, we selected the cities column and the returned feature still included the \"geometry\" column.\n\n4.2.2 Coordinate Reference Systems\nsf also has functionality to re-project and manipulate spatial objects. For example, the coordinate reference system of the sf object is currently in degrees. For many applications, we may want to transform the data to have an equal area projection and to have x and y units of meters, such as the USGS’s Albers Equal-Area Conic Projection.\nAlthough a full discussion of geographic projections is beyond the scope of this workshop, it’s worth understanding how they affect geographic data.\n\n\nImage from: https://nceas.github.io/oss-lessons/spatial-data-gis-law/1-mon-spatial-data-intro.html\n\nWhen we created the oregon_cities object, we defined the coordinate reference system with crs = 4269. Likewise, we can use the EPSG reference number of the Albers Equal-Area Conic Projection to transform the spatial object. At epsg.org, we can see that this code is 5070.\nIn this code we:\n\nCheck to see if the current CRS is equal to the Albers Equal-Area Conic Projection.\nTransform oregon_cities to CRS 5070.\nPlot the results with ggplot using the geom_sf_text() and geom_sf() functions.\n\n\nst_crs(oregon_cities) == st_crs(5070)\n#&gt; [1] FALSE\n\noregon_cities &lt;- \n  oregon_cities %&gt;% \n  st_transform(crs = 5070)\n\nprint(oregon_cities)\n#&gt; Simple feature collection with 5 features and 3 fields\n#&gt; Geometry type: POINT\n#&gt; Dimension:     XY\n#&gt; Bounding box:  xmin: -2188387 ymin: 2437735 xmax: -1997273 ymax: 2794360\n#&gt; Projected CRS: NAD83 / Conus Albers\n#&gt;   id    cities population                 geometry\n#&gt; 1  1   Ashland      20062 POINT (-2161785 2437735)\n#&gt; 2  2 Corvallis      50297 POINT (-2131908 2705899)\n#&gt; 3  3      Bend      61362 POINT (-1997273 2608702)\n#&gt; 4  4  Portland     537557 POINT (-2056514 2794360)\n#&gt; 5  5   Newport       9603 POINT (-2188387 2732355)\n\n\nggplot(data = oregon_cities) +\n  geom_sf_text(aes(label = cities),\n               hjust=0, vjust=1.5) +\n  geom_sf(aes(size = population)) + \n  xlab('Longitude') +\n  ylab('Latitude') +\n  theme_bw()\n\n\n\nFigure 4.3: Oregon cities plotted from sf object.\n\n\n\n\n\n\n\n\n\nEPSG Codes\n\n\n\nEPSG codes are unique identifiers that represent coordinate reference systems. Some commonly used EPSG codes include:\n\n4326: The WGS84 geographic coordinate reference system.\n4269: The NAD83 geographic coordinate reference system.\n5070: The NAD83 projected coordinate reference system.\n\n\n\n\n4.2.3 It all feels like R\n\nThere can be huge advantages to doing GIS tasks in R. First, there are huge advantages to keeping workflows within one software system. Second, if you are familiar with R, the leap to doing GIS in R will feel small.\nAs noted above, the sf package provides a variety of GIS functions, such as buffers, intersection, centroids, etc. In addition, these functions can be combined with tidyverse functions in piped procedures.\n\n# Plot 1:\n\n# Add 100 Km buffer to cities\ncities_buffer &lt;- \n  oregon_cities %&gt;% \n  st_buffer(100000)\n\n# Plot, color by city, and add centroids\n\n#| label: buffered-cities1\n#| fig-cap: \"Buffered cities w/ overlapping buffers.\"\nggplot(data = cities_buffer) +\n  geom_sf(aes(fill = cities), alpha = 0.5) +\n  geom_sf(data = st_centroid(oregon_cities)) +\n  theme_bw()\n\n\n\n\n# Plot 2:\n\noregon_cities %&gt;% \n  \n  # 100 Km buffer\n  \n  st_buffer(100000) %&gt;%\n  \n  # Split polygons where buffers intersect\n  \n  st_intersection() %&gt;%\n  \n  # Add area to table, but drop units (causes issues with ggplot)\n  \n  mutate(area = st_area(.) %&gt;% \n           units::drop_units(),\n         id = as.factor(1:nrow(.))) %&gt;% \n  \n  # Plot and color by id of polygons\n  \n#| label: buffered-cities2\n#| fig-cap: \"Buffered cities w/ overlapping buffers split.\"\n\nggplot() +\n  geom_sf(aes(fill = id), alpha = 0.5) +\n  theme_bw()"
  },
  {
    "objectID": "gis-in-r.html#raster-data",
    "href": "gis-in-r.html#raster-data",
    "title": "4  GIS in R",
    "section": "\n4.3 Raster data",
    "text": "4.3 Raster data\nAnother fundamental data type in GIS is the raster. Rasters are a way of displaying gridded or an array of data, where each member of the grid represents a landscape feature (e.g., elevation) and each element also has a given resolution (e.g., 30m x 30m).\nThere are several packages available to work with rasters in R, including the original raster, but also now terra, and stars. We’ll use the terra package since it has an active develoment community on GitHub and has taken over as the primary package for handling and viewing raster data. It is reported to be up to 7x faster than the original raster package for some processes. If you are familiar with raster it will feel very similar.\nMuch like sf, terra has a large number of functions for working with raster data.\n\nlibrary(terra)\nls(\"package:terra\")\n#&gt;   [1] \"%in%\"                  \"activeCat\"             \"activeCat&lt;-\"          \n#&gt;   [4] \"add_legend\"            \"add&lt;-\"                 \"addCats\"              \n#&gt;   [7] \"adjacent\"              \"aggregate\"             \"align\"                \n#&gt;  [10] \"all.equal\"             \"allNA\"                 \"animate\"              \n#&gt;  [13] \"app\"                   \"approximate\"           \"area\"                 \n#&gt;  [16] \"Arith\"                 \"as.array\"              \"as.bool\"              \n#&gt;  [19] \"as.contour\"            \"as.data.frame\"         \"as.factor\"            \n#&gt;  [22] \"as.int\"                \"as.lines\"              \"as.list\"              \n#&gt;  [25] \"as.matrix\"             \"as.points\"             \"as.polygons\"          \n#&gt;  [28] \"as.raster\"             \"atan_2\"                \"atan2\"                \n#&gt;  [31] \"autocor\"               \"barplot\"               \"blocks\"               \n#&gt;  [34] \"boundaries\"            \"boxplot\"               \"buffer\"               \n#&gt;  [37] \"cartogram\"             \"catalyze\"              \"categories\"           \n#&gt;  [40] \"cats\"                  \"cbind2\"                \"cellFromRowCol\"       \n#&gt;  [43] \"cellFromRowColCombine\" \"cellFromXY\"            \"cells\"                \n#&gt;  [46] \"cellSize\"              \"centroids\"             \"clamp\"                \n#&gt;  [49] \"clamp_ts\"              \"classify\"              \"clearance\"            \n#&gt;  [52] \"click\"                 \"colFromCell\"           \"colFromX\"             \n#&gt;  [55] \"colorize\"              \"coltab\"                \"coltab&lt;-\"             \n#&gt;  [58] \"combineGeoms\"          \"compare\"               \"Compare\"              \n#&gt;  [61] \"compareGeom\"           \"concats\"               \"contour\"              \n#&gt;  [64] \"convHull\"              \"costDist\"              \"countNA\"              \n#&gt;  [67] \"cover\"                 \"crds\"                  \"crop\"                 \n#&gt;  [70] \"crosstab\"              \"crs\"                   \"crs&lt;-\"                \n#&gt;  [73] \"datatype\"              \"deepcopy\"              \"delaunay\"             \n#&gt;  [76] \"densify\"               \"density\"               \"depth\"                \n#&gt;  [79] \"depth&lt;-\"               \"describe\"              \"diff\"                 \n#&gt;  [82] \"direction\"             \"disagg\"                \"distance\"             \n#&gt;  [85] \"dots\"                  \"draw\"                  \"droplevels\"           \n#&gt;  [88] \"elongate\"              \"emptyGeoms\"            \"erase\"                \n#&gt;  [91] \"expanse\"               \"ext\"                   \"ext&lt;-\"                \n#&gt;  [94] \"extend\"                \"extract\"               \"extractAlong\"         \n#&gt;  [97] \"extractRange\"          \"fileBlocksize\"         \"fillHoles\"            \n#&gt; [100] \"fillTime\"              \"flip\"                  \"focal\"                \n#&gt; [103] \"focal3D\"               \"focalCor\"              \"focalCpp\"             \n#&gt; [106] \"focalMat\"              \"focalPairs\"            \"focalReg\"             \n#&gt; [109] \"focalValues\"           \"forceCCW\"              \"free_RAM\"             \n#&gt; [112] \"freq\"                  \"gaps\"                  \"gdal\"                 \n#&gt; [115] \"gdalCache\"             \"geom\"                  \"geomtype\"             \n#&gt; [118] \"getGDALconfig\"         \"global\"                \"graticule\"            \n#&gt; [121] \"gridDist\"              \"gridDistance\"          \"halo\"                 \n#&gt; [124] \"has.colors\"            \"has.RGB\"               \"has.time\"             \n#&gt; [127] \"hasMinMax\"             \"hasValues\"             \"head\"                 \n#&gt; [130] \"hist\"                  \"ifel\"                  \"image\"                \n#&gt; [133] \"impose\"                \"inext\"                 \"init\"                 \n#&gt; [136] \"inMemory\"              \"inset\"                 \"interpIDW\"            \n#&gt; [139] \"interpNear\"            \"interpolate\"           \"intersect\"            \n#&gt; [142] \"is.bool\"               \"is.empty\"              \"is.factor\"            \n#&gt; [145] \"is.int\"                \"is.lines\"              \"is.lonlat\"            \n#&gt; [148] \"is.points\"             \"is.polygons\"           \"is.related\"           \n#&gt; [151] \"is.rotated\"            \"is.valid\"              \"isFALSE\"              \n#&gt; [154] \"isTRUE\"                \"k_means\"               \"lapp\"                 \n#&gt; [157] \"layerCor\"              \"levels\"                \"linearUnits\"          \n#&gt; [160] \"lines\"                 \"logic\"                 \"Logic\"                \n#&gt; [163] \"longnames\"             \"longnames&lt;-\"           \"makeNodes\"            \n#&gt; [166] \"makeTiles\"             \"makeValid\"             \"makeVRT\"              \n#&gt; [169] \"map.pal\"               \"mask\"                  \"match\"                \n#&gt; [172] \"math\"                  \"Math\"                  \"Math2\"                \n#&gt; [175] \"mean\"                  \"median\"                \"mem_info\"             \n#&gt; [178] \"merge\"                 \"mergeLines\"            \"mergeTime\"            \n#&gt; [181] \"meta\"                  \"metags\"                \"metags&lt;-\"             \n#&gt; [184] \"minCircle\"             \"minmax\"                \"minRect\"              \n#&gt; [187] \"modal\"                 \"mosaic\"                \"na.omit\"              \n#&gt; [190] \"NAflag\"                \"NAflag&lt;-\"              \"names\"                \n#&gt; [193] \"ncell\"                 \"ncol\"                  \"ncol&lt;-\"               \n#&gt; [196] \"nearby\"                \"nearest\"               \"nlyr\"                 \n#&gt; [199] \"nlyr&lt;-\"                \"noNA\"                  \"normalize.longitude\"  \n#&gt; [202] \"north\"                 \"not.na\"                \"nrow\"                 \n#&gt; [205] \"nrow&lt;-\"                \"nsrc\"                  \"origin\"               \n#&gt; [208] \"origin&lt;-\"              \"pairs\"                 \"panel\"                \n#&gt; [211] \"patches\"               \"perim\"                 \"persp\"                \n#&gt; [214] \"plet\"                  \"plot\"                  \"plotRGB\"              \n#&gt; [217] \"points\"                \"polys\"                 \"predict\"              \n#&gt; [220] \"princomp\"              \"project\"               \"quantile\"             \n#&gt; [223] \"query\"                 \"rangeFill\"             \"rapp\"                 \n#&gt; [226] \"rast\"                  \"rasterize\"             \"rasterizeGeom\"        \n#&gt; [229] \"rasterizeWin\"          \"rcl\"                   \"readRDS\"              \n#&gt; [232] \"readStart\"             \"readStop\"              \"readValues\"           \n#&gt; [235] \"rectify\"               \"regress\"               \"relate\"               \n#&gt; [238] \"removeDupNodes\"        \"res\"                   \"res&lt;-\"                \n#&gt; [241] \"resample\"              \"rescale\"               \"rev\"                  \n#&gt; [244] \"RGB\"                   \"RGB&lt;-\"                 \"roll\"                 \n#&gt; [247] \"rotate\"                \"round\"                 \"rowColCombine\"        \n#&gt; [250] \"rowColFromCell\"        \"rowFromCell\"           \"rowFromY\"             \n#&gt; [253] \"same.crs\"              \"sapp\"                  \"saveRDS\"              \n#&gt; [256] \"sbar\"                  \"scale\"                 \"scoff\"                \n#&gt; [259] \"scoff&lt;-\"               \"sds\"                   \"segregate\"            \n#&gt; [262] \"sel\"                   \"selectHighest\"         \"selectRange\"          \n#&gt; [265] \"serialize\"             \"set.cats\"              \"set.crs\"              \n#&gt; [268] \"set.ext\"               \"set.names\"             \"set.RGB\"              \n#&gt; [271] \"set.values\"            \"setGDALconfig\"         \"setMinMax\"            \n#&gt; [274] \"setValues\"             \"shade\"                 \"sharedPaths\"          \n#&gt; [277] \"shift\"                 \"sieve\"                 \"simplifyGeom\"         \n#&gt; [280] \"size\"                  \"snap\"                  \"sort\"                 \n#&gt; [283] \"sources\"               \"spatSample\"            \"spin\"                 \n#&gt; [286] \"split\"                 \"sprc\"                  \"stdev\"                \n#&gt; [289] \"stretch\"               \"subset\"                \"subst\"                \n#&gt; [292] \"summary\"               \"Summary\"               \"svc\"                  \n#&gt; [295] \"symdif\"                \"t\"                     \"tail\"                 \n#&gt; [298] \"tapp\"                  \"terrain\"               \"terraOptions\"         \n#&gt; [301] \"text\"                  \"tighten\"               \"time\"                 \n#&gt; [304] \"time&lt;-\"                \"timeInfo\"              \"tmpFiles\"             \n#&gt; [307] \"trans\"                 \"trim\"                  \"union\"                \n#&gt; [310] \"unique\"                \"units\"                 \"units&lt;-\"              \n#&gt; [313] \"unserialize\"           \"unwrap\"                \"update\"               \n#&gt; [316] \"values\"                \"values&lt;-\"              \"varnames\"             \n#&gt; [319] \"varnames&lt;-\"            \"vect\"                  \"vector_layers\"        \n#&gt; [322] \"viewshed\"              \"voronoi\"               \"vrt\"                  \n#&gt; [325] \"vrt_tiles\"             \"weighted.mean\"         \"where.max\"            \n#&gt; [328] \"where.min\"             \"which.lyr\"             \"which.max\"            \n#&gt; [331] \"which.min\"             \"width\"                 \"window\"               \n#&gt; [334] \"window&lt;-\"              \"wrap\"                  \"wrapCache\"            \n#&gt; [337] \"writeCDF\"              \"writeRaster\"           \"writeStart\"           \n#&gt; [340] \"writeStop\"             \"writeValues\"           \"writeVector\"          \n#&gt; [343] \"xFromCell\"             \"xFromCol\"              \"xmax\"                 \n#&gt; [346] \"xmax&lt;-\"                \"xmin\"                  \"xmin&lt;-\"               \n#&gt; [349] \"xres\"                  \"xyFromCell\"            \"yFromCell\"            \n#&gt; [352] \"yFromRow\"              \"ymax\"                  \"ymax&lt;-\"               \n#&gt; [355] \"ymin\"                  \"ymin&lt;-\"                \"yres\"                 \n#&gt; [358] \"zonal\"                 \"zoom\"\n\nLet’s create an empty RasterLayer object - we have to define the matrix (rows and columns) the spatial bounding box, and then we provide values to the cells using the runif function to derive random values from the uniform distribution.\n\nr &lt;- rast(ncol=10, nrow = 10)\n\nr[] &lt;- runif(n=ncell(r))\n\nr\n#&gt; class       : SpatRaster \n#&gt; dimensions  : 10, 10, 1  (nrow, ncol, nlyr)\n#&gt; resolution  : 36, 18  (x, y)\n#&gt; extent      : -180, 180, -90, 90  (xmin, xmax, ymin, ymax)\n#&gt; coord. ref. : lon/lat WGS 84 (CRS84) (OGC:CRS84) \n#&gt; source(s)   : memory\n#&gt; name        :        lyr.1 \n#&gt; min value   : 0.0002964453 \n#&gt; max value   : 0.9977271722\n\n#| label: basic-raster\n#| fig-cap: \"Basic raster dataset in R.\"\nplot(r)\n\n\n\n\nThe raster we made from scratch is in memory but very small. An important feature of the terra package is that when you load a raster from file, it is not actually loaded into memory. Instead, terra reads information about the raster into memory, such as its dimensions, its extent, and more. This makes working with rasters lightweight, as it will only ever load into memory what is needed for a specific task. For example, you can access raster values via direct indexing or line/column indexing. The terra package accesses just those locations on the disk. Take a minute to see how this works using the raster r that we just created - the syntax is:\n\n# Access data from the ith location in a raster\nr[12]\n#&gt;          lyr.1\n#&gt; 1 0.0002964453\n\n# Access data based on row and column\nr[2, 2] \n#&gt;          lyr.1\n#&gt; 1 0.0002964453\n\n\n4.3.1 Stacking raster data\nOften, it can be very efficient to work with mutliple rasters at once. Raster data is often distributed in stacks, such as climate data as a NetCDF file.\n\n# Create 2 new rasters based on raster r\n\nr2 &lt;- r * 50\nr3 &lt;- sqrt(r * 5)\n\n# Stack rasters and rename to be unique\n\ns &lt;- c(r, r2, r3)\nnames(s) &lt;- c('r1', 'r2', 'r3')\n\n#| label: stacked-rasters\n#| fig-cap: \"Plot of stacked rasters in R.\"\nplot(s)\n\n\n\n\n# Manipulate all rasters in stack\n\ns2 &lt;- (s + 5) / 3\n\n# Give modified rasters a new name (add \"modified\")\n\nnames(s2) &lt;- paste0(names(s), \" modified\")\n\n#| label: modified-stacked-rasters\n#| fig-cap: \"Plot of stacked rasters in R.\"\nplot(c(s, s2))\n\n\n\n\n\n\n\n\n\n\nReclassifying rasters\n\n\n\nYou can also relcassify rasters values with terra::classify()\n\n\n# Create a matrix where first 2 values are raster values and third value is the class value:\nclass_table &lt;-  c(0, 0.5, 1,\n                  0.5, 1, 2) %&gt;% \n  matrix(ncol = 3, byrow = TRUE)\n\n# Reclassify r based on the reclass matrix\nrcls &lt;- r %&gt;%\n  terra::classify(class_table) \n\nplot(c(r, rcls))"
  },
  {
    "objectID": "gis-in-r.html#working-with-real-data",
    "href": "gis-in-r.html#working-with-real-data",
    "title": "4  GIS in R",
    "section": "\n4.4 Working with real data",
    "text": "4.4 Working with real data\nThere are several (often very new) R packages for accessing and working with spatial data. In this code, we will use theFedData R package to download National Elevation Data. Although this example focuses on digital elevation data, the FedData package provides access to several federal datasets, including the National Land Cover Database (NLCD).\n\nlibrary(FedData)\n\n# Select just Corvallis and calculate a 10,000-m buffer\ncorvallis &lt;- \n  oregon_cities %&gt;%\n  filter(cities == 'Corvallis') %&gt;% \n  st_buffer(10000)\n\n# Download national elevation data (ned)\nned &lt;- FedData::get_ned(\n  template = corvallis,\n  label = \"corvallis\")\n\nnames(ned) &lt;- \"elevation\"\n\n# Check whether the elevation and buffer data share the same geographic reference system\nst_crs(ned) == st_crs(corvallis)\n#&gt; [1] FALSE\n\nWe can transform the projection of the raster with terra::project(). Because the raster represents continuous data, we will use the bilinear interpolation method. For categorical data (e.g., land cover), it is best to use method = 'near'\n\nned &lt;- terra::project(ned, \n                      'epsg:5070',\n                      method = 'bilinear')\n\nThe tidyterra package gives us a new “geometry” for ggplot called geom_spatraster().\n\nlibrary(tidyterra)\n\n#| label: elevation-ggplot\n#| fig-cap: \"ggplot of elevation data.\"\nggplot() + \n  tidyterra::geom_spatraster(data = ned) + \n  geom_sf(data = corvallis, \n          fill = NA, \n          color = 'white', \n          lwd = 2) +\n  theme_bw()\n\n\n\n\nIn addition to elevation, we can derive several topographic indices, including slope and roughness.\n\n# Calculate several terrain metrics\n?terra::terrain\n\nTo calculate these metrics on the elevation raster, we can use the terrain() function from the terra package.\n\n# Calculate several terrain metrics\nterrain_metrics &lt;- terrain(ned, \n                           v = c('slope', 'roughness', 'TRI'))\n\n#| label: terrain-plots\n#| fig-cap: \"Plot of elevation-derived terrain metrics.\"\nplot(c(ned, terrain_metrics))\n\n\n\n\n#| label: terrain-ggplots\n#| fig-cap: \"ggplots of elevation-derived terrain metrics.\"\nggplot() + \n  geom_spatraster(data = terrain_metrics) + \n  facet_wrap(~lyr, ncol = 2) + \n  scale_fill_whitebox_c(\n    palette = \"muted\",\n    n.breaks = 12,\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  theme_bw()\n\n\n\n\nWhat if we want to know the average of elevation, slope, etc. within the buffered area of Corvallis?\n\n# First, let's combine into a single stack of rasters\nterrain_metrics &lt;- c(terrain_metrics, ned)\n\n# zonal function in terra to calculate zonal statistics\nterra::zonal(terrain_metrics, \n             \n             # Need to convert corvallis `sf` object to terra vector\n             terra::vect(corvallis), \n             \n             # Metric to be calculated\n             mean, na.rm = T)\n#&gt;      slope roughness      TRI elevation\n#&gt; 1 4.638052  5.604068 1.717456  119.6678\n\n\n\n\n\n\n\nExercise\n\n\n\n\nCreate a layer of U.S. cities with data('us.cities') from the maps library.\nSelect the city of your choice and buffer it by 10Km. (We suggest converting to an equal area projection first).\nRead in NLCD land cover data for your city with the FedData package.\nCalculate the % of the buffer around your city that is composed of row crop.\n\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\nlibrary(FedData)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(maps)\n\ndata('us.cities')\n\nmy_city &lt;- us.cities %&gt;% \n  filter(name == 'Idaho Falls ID') %&gt;% \n  st_as_sf(coords = c('long', 'lat'), crs = 4269) %&gt;% \n  st_transform(crs = 5070) %&gt;% \n  st_buffer(10000)\n\nnlcd &lt;- get_nlcd(\n  template = my_city,\n  year = 2021,\n  label = 'city') %&gt;%\nterra::project('epsg:5070',\n               method = 'near')\n\ncity_nlcd &lt;- terra::extract(nlcd, \n                            terra::vect(my_city))\n\ncity_nlcd %&gt;% \n  group_by(Class) %&gt;% \n  summarise(count = n()) %&gt;% \n  mutate(percent = (count / sum(count)) * 100)\n#&gt; # A tibble: 13 × 3\n#&gt;    Class                         count  percent\n#&gt;    &lt;fct&gt;                         &lt;int&gt;    &lt;dbl&gt;\n#&gt;  1 Open Water                     4189  1.20   \n#&gt;  2 Developed, Open Space         31025  8.89   \n#&gt;  3 Developed, Low Intensity      43040 12.3    \n#&gt;  4 Developed, Medium Intensity   38470 11.0    \n#&gt;  5 Developed High Intensity       6778  1.94   \n#&gt;  6 Barren Land (Rock/Sand/Clay)   1511  0.433  \n#&gt;  7 Evergreen Forest                  9  0.00258\n#&gt;  8 Shrub/Scrub                    4991  1.43   \n#&gt;  9 Grassland/Herbaceous           2633  0.755  \n#&gt; 10 Pasture/Hay                      98  0.0281 \n#&gt; 11 Cultivated Crops             215924 61.9    \n#&gt; 12 Woody Wetlands                   88  0.0252 \n#&gt; 13 Emergent Herbaceous Wetlands    147  0.0421\n\n# Bonus: mapping land cover w/ city radius\n\n# Mask then crop the NLCD to the buffer\nnlcd_crop &lt;- terra::mask(nlcd, my_city) %&gt;% \n  terra::crop(my_city)\n\nggplot() + \n  geom_spatraster(data = nlcd_crop) +\n  geom_sf(data = my_city,\n          fill = NA,\n          lwd = 2) +\n  theme_bw()\n\n\n\n\n\n\n\nZonal statistics are fundamental to generating data summaries that are useful for ecological/statistical analyses, and the terra package has greatly improved the processing speed of zonal stats in R.\nHowever, most analyses in aquatic ecology are not based on buffered points, like we covered here. In addition, zonal statistics over large geographic areas can still be a big task for R, such as for large watersheds.\nIn the next sections, we’ll first cover how to generate watershed boundaries to generate watershed summaries. After that, we’ll discuss the StreamCatTools package for accessing a variety of watershed metrics for which the zonal statistics have already been calculated."
  },
  {
    "objectID": "gis-in-r.html#r-code-appendix",
    "href": "gis-in-r.html#r-code-appendix",
    "title": "4  GIS in R",
    "section": "\n4.5 R Code Appendix",
    "text": "4.5 R Code Appendix\n\n\nlibrary(ggplot2)\n\nid &lt;- c(1:5)\ncities &lt;- c('Ashland','Corvallis','Bend','Portland','Newport')\nlongitude &lt;- c(-122.699, -123.275, -121.313, -122.670, -124.054)\nlatitude &lt;- c(42.189, 44.57, 44.061, 45.523, 44.652)\npopulation &lt;- c(20062, 50297, 61362, 537557, 9603)\n\noregon_cities &lt;- data.frame(id, cities, longitude, latitude, population)\nggplot(\n  data = oregon_cities, \n  aes(x = longitude, y = latitude, size = population, label = cities)\n) +\n  geom_point() +\n  geom_text(hjust = 1, vjust = 1) +\n  theme_bw()\nlibrary(sf)\nls(\"package:sf\")\n\nprint(oregon_cities)\n\noregon_cities &lt;- oregon_cities %&gt;%\n  st_as_sf(coords = c('longitude', 'latitude'), crs = 4269)\n\nprint(oregon_cities)\nlibrary(tidyverse)\n\noregon_cities %&gt;% \n  select(cities)\nst_crs(oregon_cities) == st_crs(5070)\n\noregon_cities &lt;- \n  oregon_cities %&gt;% \n  st_transform(crs = 5070)\n\nprint(oregon_cities)\nggplot(data = oregon_cities) +\n  geom_sf_text(aes(label = cities),\n               hjust=0, vjust=1.5) +\n  geom_sf(aes(size = population)) + \n  xlab('Longitude') +\n  ylab('Latitude') +\n  theme_bw()\n# Plot 1:\n\n# Add 100 Km buffer to cities\ncities_buffer &lt;- \n  oregon_cities %&gt;% \n  st_buffer(100000)\n\n# Plot, color by city, and add centroids\n\n#| label: buffered-cities1\n#| fig-cap: \"Buffered cities w/ overlapping buffers.\"\nggplot(data = cities_buffer) +\n  geom_sf(aes(fill = cities), alpha = 0.5) +\n  geom_sf(data = st_centroid(oregon_cities)) +\n  theme_bw()\n\n# Plot 2:\n\noregon_cities %&gt;% \n  \n  # 100 Km buffer\n  \n  st_buffer(100000) %&gt;%\n  \n  # Split polygons where buffers intersect\n  \n  st_intersection() %&gt;%\n  \n  # Add area to table, but drop units (causes issues with ggplot)\n  \n  mutate(area = st_area(.) %&gt;% \n           units::drop_units(),\n         id = as.factor(1:nrow(.))) %&gt;% \n  \n  # Plot and color by id of polygons\n  \n#| label: buffered-cities2\n#| fig-cap: \"Buffered cities w/ overlapping buffers split.\"\n\nggplot() +\n  geom_sf(aes(fill = id), alpha = 0.5) +\n  theme_bw()\n\nlibrary(terra)\nls(\"package:terra\")\nr &lt;- rast(ncol=10, nrow = 10)\n\nr[] &lt;- runif(n=ncell(r))\n\nr\n\n#| label: basic-raster\n#| fig-cap: \"Basic raster dataset in R.\"\nplot(r)\n# Access data from the ith location in a raster\nr[12]\n\n# Access data based on row and column\nr[2, 2] \n# Create 2 new rasters based on raster r\n\nr2 &lt;- r * 50\nr3 &lt;- sqrt(r * 5)\n\n# Stack rasters and rename to be unique\n\ns &lt;- c(r, r2, r3)\nnames(s) &lt;- c('r1', 'r2', 'r3')\n\n#| label: stacked-rasters\n#| fig-cap: \"Plot of stacked rasters in R.\"\nplot(s)\n\n# Manipulate all rasters in stack\n\ns2 &lt;- (s + 5) / 3\n\n# Give modified rasters a new name (add \"modified\")\n\nnames(s2) &lt;- paste0(names(s), \" modified\")\n\n#| label: modified-stacked-rasters\n#| fig-cap: \"Plot of stacked rasters in R.\"\nplot(c(s, s2))\n\n# Create a matrix where first 2 values are raster values and third value is the class value:\nclass_table &lt;-  c(0, 0.5, 1,\n                  0.5, 1, 2) %&gt;% \n  matrix(ncol = 3, byrow = TRUE)\n\n# Reclassify r based on the reclass matrix\nrcls &lt;- r %&gt;%\n  terra::classify(class_table) \n\nplot(c(r, rcls))\nlibrary(FedData)\n\n# Select just Corvallis and calculate a 10,000-m buffer\ncorvallis &lt;- \n  oregon_cities %&gt;%\n  filter(cities == 'Corvallis') %&gt;% \n  st_buffer(10000)\n\n# Download national elevation data (ned)\nned &lt;- FedData::get_ned(\n  template = corvallis,\n  label = \"corvallis\")\n\nnames(ned) &lt;- \"elevation\"\n\n# Check whether the elevation and buffer data share the same geographic reference system\nst_crs(ned) == st_crs(corvallis)\nned &lt;- terra::project(ned, \n                      'epsg:5070',\n                      method = 'bilinear')\nlibrary(tidyterra)\n\n#| label: elevation-ggplot\n#| fig-cap: \"ggplot of elevation data.\"\nggplot() + \n  tidyterra::geom_spatraster(data = ned) + \n  geom_sf(data = corvallis, \n          fill = NA, \n          color = 'white', \n          lwd = 2) +\n  theme_bw()\n# Calculate several terrain metrics\n?terra::terrain\n# Calculate several terrain metrics\nterrain_metrics &lt;- terrain(ned, \n                           v = c('slope', 'roughness', 'TRI'))\n\n#| label: terrain-plots\n#| fig-cap: \"Plot of elevation-derived terrain metrics.\"\nplot(c(ned, terrain_metrics))\n\n#| label: terrain-ggplots\n#| fig-cap: \"ggplots of elevation-derived terrain metrics.\"\nggplot() + \n  geom_spatraster(data = terrain_metrics) + \n  facet_wrap(~lyr, ncol = 2) + \n  scale_fill_whitebox_c(\n    palette = \"muted\",\n    n.breaks = 12,\n    guide = guide_legend(reverse = TRUE)\n  ) +\n  theme_bw()\n# First, let's combine into a single stack of rasters\nterrain_metrics &lt;- c(terrain_metrics, ned)\n\n# zonal function in terra to calculate zonal statistics\nterra::zonal(terrain_metrics, \n             \n             # Need to convert corvallis `sf` object to terra vector\n             terra::vect(corvallis), \n             \n             # Metric to be calculated\n             mean, na.rm = T)\nlibrary(FedData)\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(maps)\n\ndata('us.cities')\n\nmy_city &lt;- us.cities %&gt;% \n  filter(name == 'Idaho Falls ID') %&gt;% \n  st_as_sf(coords = c('long', 'lat'), crs = 4269) %&gt;% \n  st_transform(crs = 5070) %&gt;% \n  st_buffer(10000)\n\nnlcd &lt;- get_nlcd(\n  template = my_city,\n  year = 2021,\n  label = 'city') %&gt;%\nterra::project('epsg:5070',\n               method = 'near')\n\ncity_nlcd &lt;- terra::extract(nlcd, \n                            terra::vect(my_city))\n\ncity_nlcd %&gt;% \n  group_by(Class) %&gt;% \n  summarise(count = n()) %&gt;% \n  mutate(percent = (count / sum(count)) * 100)\n\n# Bonus: mapping land cover w/ city radius\n\n# Mask then crop the NLCD to the buffer\nnlcd_crop &lt;- terra::mask(nlcd, my_city) %&gt;% \n  terra::crop(my_city)\n\nggplot() + \n  geom_spatraster(data = nlcd_crop) +\n  geom_sf(data = my_city,\n          fill = NA,\n          lwd = 2) +\n  theme_bw()"
  },
  {
    "objectID": "watershed-delineation.html#usgs-streamstats",
    "href": "watershed-delineation.html#usgs-streamstats",
    "title": "5  Watershed Delineation in R",
    "section": "\n5.1 USGS StreamStats",
    "text": "5.1 USGS StreamStats\nThe USGS’s StreamStats is an online service and map interface that allows users to navigate to a desired location and delineate a watershed boundary with the click of a mouse:\nhttps://streamstats.usgs.gov/ss/\nIn addition to the map interface, the data are also accessible via an API:\nhttps://streamstats.usgs.gov/docs/streamstatsservices\nFor example, it is possible to replicate the point and click website by pasting a URL into the browser by combining the following information:\n\nBase service URL\nState where watershed is located\nLongitude\nLatitude\nAdditional options\n\nStreamStats delineates a basin from pre-processed DEMs based on these inputs to the service.\nWe can write a custom function to make this request. This code:\n\nCombines the required inputs into a single text query\nMassages the data returned by the API to create a sf object\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Create custom function to delineate watershed from StreamStats service\nstreamstats_ws = function(state, longitude, latitude){\n  p1 = 'https://streamstats.usgs.gov/streamstatsservices/watershed.geojson?rcode='\n  p2 = '&xlocation='\n  p3 = '&ylocation='\n  p4 = '&crs=4269&includeparameters=false&includeflowtypes=false&includefeatures=true&simplify=true'\n  query <-  paste0(p1, state, p2, toString(longitude), p3, toString(latitude), p4)\n  mydata <- jsonlite::fromJSON(query, simplifyVector = FALSE, simplifyDataFrame = FALSE)\n  poly_geojsonsting <- jsonlite::toJSON(mydata$featurecollection[[2]]$feature, auto_unbox = TRUE)\n  poly <- geojsonio::geojson_sf(poly_geojsonsting) \n  poly\n}\n\n# Define location for delineation (Calapooia Watershed)\nstate <- 'OR'\nlatitude <- 44.62892\nlongitude <- -123.13113\n\n# Delineate watershed\ncal_ws <- streamstats_ws('OR', longitude, latitude) %>% \n  st_transform(crs = 5070)\n\n# Generate point for plotting\ncal_pt <- data.frame(ptid = 'Calapooia River', lon = longitude, lat = latitude)  %>% \n  st_as_sf(coords = c('lon', 'lat'), crs = 4269) %>% \n  st_transform(crs = 5070)\n\ncal_map <- ggplot() + \n  geom_sf(data = cal_ws, fill=NA) +\n  geom_sf(data = cal_pt, size=3) +\n  theme_bw()\n\ncal_map\n\n\n\n\nAs noted above, we can replicate the call to the API by pasting the p1 to p4 + state and lat/lon:\n\nstate <- 'OR'\nlatitude <- 44.62892\nlongitude <- -123.13113\n\np1 = 'https://streamstats.usgs.gov/streamstatsservices/watershed.geojson?rcode='\np2 = '&xlocation='\np3 = '&ylocation='\np4 = '&crs=4269&includeparameters=false&includeflowtypes=false&includefeatures=true&simplify=true'\n\nquery <-  paste0(p1, state, p2, toString(longitude), p3, toString(latitude), p4)\n\nprint(query)\n#> [1] \"https://streamstats.usgs.gov/streamstatsservices/watershed.geojson?rcode=OR&xlocation=-123.13113&ylocation=44.62892&crs=4269&includeparameters=false&includeflowtypes=false&includefeatures=true&simplify=true\"\n\nHowever, when the query is provided to the browser, it does not look like a watershed. The additional lines of code in the function we defined above used the jsonlite and geojsonio packages to massage the output and produce the sf spatial object cal_ws.\nThis code produces a watershed boundary (cal_ws) for the Calapooia watershed in Oregon. But what if the latitude/longitude are slightly off of the stream network?\n\n# A close, but different watershed\nlatitude <- 44.61892\nlongitude <- -123.13731\n\ntrib_ws <- streamstats_ws('OR', longitude, latitude) %>% \n  st_transform(crs = 5070)\n\ntrib_pt <- data.frame(ptid = 'Calapooia Trib', lon = longitude, lat = latitude)  %>% \n  st_as_sf(coords = c('lon', 'lat'), crs = 4269) %>% \n  st_transform(crs = 5070)\n\ncal_map <- cal_map +\n  geom_sf(data = trib_ws, fill=NA, color='red') +\n  geom_sf(data = trib_pt, size=3, color = 'pink') \n\ncal_map\n\n\n\n\nLet’s zoom into the problem. To do this we’ll use the mapview package. Here, we use mapview::mapview() to create an interactive map of the Calapooia watersheds.\n\nlibrary(mapview)\nmapview::mapviewOptions(fgb=FALSE)\n\nmapview(cal_ws, alpha.regions=.08) + \n  mapview(cal_pt, col.regions = 'black') + \n  mapview(trib_ws, alpha.regions=.08, col.regions = 'red') +\n  mapview(trib_pt, col.regions = 'red')  \n\n\n\n\n\n\nWe can see that the second lat/lon delineated a very small watershed that appears to be a tributary to the mainstem of the Calapooia. This highlights one of the challenges of watershed delineation - small discrepancies in lat/lon can produce a vastely different boundary.\nIn the next section, we’ll explore another method for watershed delineation."
  },
  {
    "objectID": "watershed-delineation.html#nhdplustools",
    "href": "watershed-delineation.html#nhdplustools",
    "title": "5  Watershed Delineation in R",
    "section": "\n5.2 nhdplusTools",
    "text": "5.2 nhdplusTools\nnhdplusTools is an R package that can access the Network Linked Data Index (NLDI) service, which provides navigation and extraction of NHDPlus data:\nhttps://doi-usgs.github.io/nhdplusTools/\nnhdplusTools includes network navigation options as well as watershed delineation. The delineation method differs from StreamStats in that the sub-catchments are pre-staged and based on the local catchments of the NHDPlus.To delineate a basin, we must identify the starting point and the NLDI service walks the network for us to generate the watershed.\n\nlibrary(nhdplusTools)\n\n# Calapooia Watershed\nlatitude <- 44.62892\nlongitude <- -123.13113\n\n# Simple feature option to generate point without any other attributes\ncal_pt2 <- st_sfc(st_point(c(longitude, latitude)), crs = 4269)\n\n# Identify the network location (NHDPlus common ID or COMID)\nstart_comid <- nhdplusTools::discover_nhdplus_id(cal_pt2)\n\n# Combine info into list (required by NLDI basin function)\nws_source <- list(featureSource = \"comid\", featureID = start_comid)\n\ncal_ws2 <- nhdplusTools::get_nldi_basin(nldi_feature = ws_source)\n\ncal_map <- ggplot() + \n  geom_sf(data = cal_ws2, fill = NA) +\n  geom_sf(data = cal_pt2) +\n  theme_bw()\n\ncal_map\n\n\n\n\nThe delineated watershed should look familiar since it is the Calapooia River watershed.\nHowever, you may notice the slight difference in the placement of the point and the outlet of the watershed relative to the StreamStats watershed.\nLet’s have a closer look at the two watersheds together:\n\nlibrary(mapview)\nmapview::mapviewOptions(fgb=FALSE)\n\nmapview(cal_pt, col.regions = 'black') + \n  mapview(cal_ws, alpha.regions = .08) + \n  mapview(cal_ws2, alpha.regions = .08, col.regions = 'red') \n\n\n\n\n\n\nBut, what about the small tributary?\nAlthough the points are separated by a distance, they produce the same watershed. This is because the watersheds are based the aggregation of pre-defined sub-catchments of the medium-resolution NHDPlus. Since the points fall within the same local sub-catchment, they have the same outlet and basin area.\n\n# Trib coordinates\nlatitude <- 44.61892\nlongitude <- -123.13731\n\ntrib_pt2 <- st_sfc(st_point(c(longitude, latitude)), crs = 4269)\nstart_comid <- nhdplusTools::discover_nhdplus_id(trib_pt2)\nws_source <- list(featureSource = \"comid\", featureID = start_comid)\n\ntrib_ws2 <- nhdplusTools::get_nldi_basin(nldi_feature = ws_source)\n\nstreams <- nhdplusTools::navigate_nldi(ws_source, mode = \"UT\", \n                                       distance_km = 2000) %>%\n  pluck('UT_flowlines')\n\n# Use get_nhdplus to access the individual stream sub-catchments\ncats <- nhdplusTools::get_nhdplus(comid = streams$nhdplus_comid,\n                                  realization = 'catchment')\n\nmapview(streams, legend = FALSE) + \n  mapview(cal_pt2, col.regions = 'red') + \n  mapview(trib_pt2, col.regions = 'blue') + \n  mapview(cal_ws2, alpha.regions=.08, col.regions = 'red') +\n  mapview(cats, alpha.regions=.08, col.regions = 'blue') \n\n\n\n\n\n\nThis is the major limitation of NHDPlus (and by extension StreamCat). Thus, when deciding whether to use StreamStats or NHDPlusTools for watershed delineation, it is important to consider that:\n\nStreamStats can delineate smaller tributaries, but cannot do large watersheds that cross state lines.\nStreamStats is not available for all U.S. states.\nNHDPlusTools can delineate smallish to large watersheds (e.g., Mississippi River), but misses very small systems. (However, you can try experimenting with their implementation of high-res NHDPlus.)\n\nLet’s put it all together…\n\n\n\n\n\n\nExercise\n\n\n\n\nDelineate the Logan River watershed in Utah at -111.855, 41.707.\nUse the watershed boundary to download NLCD land cover data.\nPlot the NLCD data within the watershed bounday.\nCalculate the percent cover of each land use.\n\n\n\n\n\n\n\n\n\nExercise Solution\n\n\n\n\n\n\nlibrary(FedData)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(tictoc)\nlibrary(nhdplusTools)\nlibrary(sf)\n\ntic()\n\n# Logan River Watershed\nlatitude <- 41.707\nlongitude <- -111.855\n\n# Define the lat/lon\nstart_point <- st_sfc(st_point(c(longitude, latitude)), crs = 4269)\n\n# Find COMID of this point\nstart_comid <- nhdplusTools::discover_nhdplus_id(start_point)\n\n# Create a list object that defines the feature source and starting COMID\nws_source <- list(featureSource = \"comid\", featureID = start_comid)\n\n# Delineate basin\nlogan_ws <- nhdplusTools::get_nldi_basin(nldi_feature = ws_source) %>%\n\n  # Transform to EPSG 5070\n  st_transform(crs = 5070)\n\n# Alternate method\n# logan_ws <- streamstats_ws('UT', longitude, latitude) %>% \n#   st_transform(crs = 5070)\n\nnlcd <- get_nlcd(\n  template = logan_ws,\n  year = 2021,\n  label = 'Logan') %>%\n  \n  # Project raster to EPSG 5070\n  terra::project('epsg:5070', method = 'near') %>% \n  \n  # Mask the raster\n  terra::mask(logan_ws) %>% \n  \n  # Clip the raster\n  terra::crop(logan_ws)\n\nggplot() + \n  geom_spatraster(data = nlcd) +\n  geom_sf(data = logan_ws,\n          fill = NA,\n          lwd = 1.5,\n          color = 'white') +\n  theme_bw()\n\n\n\n\nlogan_nlcd <- terra::extract(nlcd, \n                             terra::vect(logan_ws))\n\nlogan_nlcd %>% \n  group_by(Class) %>% \n  summarise(count = n()) %>% \n  mutate(percent = (count / sum(count)) * 100)\n#> # A tibble: 15 × 3\n#>    Class                         count  percent\n#>    <fct>                         <int>    <dbl>\n#>  1 Open Water                      240  0.0151 \n#>  2 Developed, Open Space         24184  1.52   \n#>  3 Developed, Low Intensity      12049  0.756  \n#>  4 Developed, Medium Intensity    7961  0.500  \n#>  5 Developed High Intensity       1872  0.118  \n#>  6 Barren Land (Rock/Sand/Clay)     86  0.00540\n#>  7 Deciduous Forest             441144 27.7    \n#>  8 Evergreen Forest             451517 28.3    \n#>  9 Mixed Forest                  47735  3.00   \n#> 10 Shrub/Scrub                  568734 35.7    \n#> 11 Grassland/Herbaceous           9819  0.616  \n#> 12 Pasture/Hay                    8310  0.522  \n#> 13 Cultivated Crops              13686  0.859  \n#> 14 Woody Wetlands                 5218  0.328  \n#> 15 Emergent Herbaceous Wetlands    605  0.0380\n\ntoc()\n#> 20.15 sec elapsed\n\n\n\n\nIn the exercise above, we delineated a watershed from a latitude/longitude, transformed its projection, downloaded land cover raster data for the watershed, transformed its projection, masked and clipped the raster for mapping, and calculated the percent of each land cover type within the watershed boundary – all under 30 seconds."
  },
  {
    "objectID": "watershed-delineation.html#r-code-appendix",
    "href": "watershed-delineation.html#r-code-appendix",
    "title": "5  Watershed Delineation in R",
    "section": "\n5.3 R Code Appendix",
    "text": "5.3 R Code Appendix\n\n\n\n\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(ggplot2)\n\n# Create custom function to delineate watershed from StreamStats service\nstreamstats_ws = function(state, longitude, latitude){\n  p1 = 'https://streamstats.usgs.gov/streamstatsservices/watershed.geojson?rcode='\n  p2 = '&xlocation='\n  p3 = '&ylocation='\n  p4 = '&crs=4269&includeparameters=false&includeflowtypes=false&includefeatures=true&simplify=true'\n  query <-  paste0(p1, state, p2, toString(longitude), p3, toString(latitude), p4)\n  mydata <- jsonlite::fromJSON(query, simplifyVector = FALSE, simplifyDataFrame = FALSE)\n  poly_geojsonsting <- jsonlite::toJSON(mydata$featurecollection[[2]]$feature, auto_unbox = TRUE)\n  poly <- geojsonio::geojson_sf(poly_geojsonsting) \n  poly\n}\n\n# Define location for delineation (Calapooia Watershed)\nstate <- 'OR'\nlatitude <- 44.62892\nlongitude <- -123.13113\n\n# Delineate watershed\ncal_ws <- streamstats_ws('OR', longitude, latitude) %>% \n  st_transform(crs = 5070)\n\n# Generate point for plotting\ncal_pt <- data.frame(ptid = 'Calapooia River', lon = longitude, lat = latitude)  %>% \n  st_as_sf(coords = c('lon', 'lat'), crs = 4269) %>% \n  st_transform(crs = 5070)\n\ncal_map <- ggplot() + \n  geom_sf(data = cal_ws, fill=NA) +\n  geom_sf(data = cal_pt, size=3) +\n  theme_bw()\n\ncal_map\nstate <- 'OR'\nlatitude <- 44.62892\nlongitude <- -123.13113\n\np1 = 'https://streamstats.usgs.gov/streamstatsservices/watershed.geojson?rcode='\np2 = '&xlocation='\np3 = '&ylocation='\np4 = '&crs=4269&includeparameters=false&includeflowtypes=false&includefeatures=true&simplify=true'\n\nquery <-  paste0(p1, state, p2, toString(longitude), p3, toString(latitude), p4)\n\nprint(query)\n# A close, but different watershed\nlatitude <- 44.61892\nlongitude <- -123.13731\n\ntrib_ws <- streamstats_ws('OR', longitude, latitude) %>% \n  st_transform(crs = 5070)\n\ntrib_pt <- data.frame(ptid = 'Calapooia Trib', lon = longitude, lat = latitude)  %>% \n  st_as_sf(coords = c('lon', 'lat'), crs = 4269) %>% \n  st_transform(crs = 5070)\n\ncal_map <- cal_map +\n  geom_sf(data = trib_ws, fill=NA, color='red') +\n  geom_sf(data = trib_pt, size=3, color = 'pink') \n\ncal_map\nlibrary(mapview)\nmapview::mapviewOptions(fgb=FALSE)\n\nmapview(cal_ws, alpha.regions=.08) + \n  mapview(cal_pt, col.regions = 'black') + \n  mapview(trib_ws, alpha.regions=.08, col.regions = 'red') +\n  mapview(trib_pt, col.regions = 'red')  \nlibrary(nhdplusTools)\n\n# Calapooia Watershed\nlatitude <- 44.62892\nlongitude <- -123.13113\n\n# Simple feature option to generate point without any other attributes\ncal_pt2 <- st_sfc(st_point(c(longitude, latitude)), crs = 4269)\n\n# Identify the network location (NHDPlus common ID or COMID)\nstart_comid <- nhdplusTools::discover_nhdplus_id(cal_pt2)\n\n# Combine info into list (required by NLDI basin function)\nws_source <- list(featureSource = \"comid\", featureID = start_comid)\n\ncal_ws2 <- nhdplusTools::get_nldi_basin(nldi_feature = ws_source)\n\ncal_map <- ggplot() + \n  geom_sf(data = cal_ws2, fill = NA) +\n  geom_sf(data = cal_pt2) +\n  theme_bw()\n\ncal_map\nlibrary(mapview)\nmapview::mapviewOptions(fgb=FALSE)\n\nmapview(cal_pt, col.regions = 'black') + \n  mapview(cal_ws, alpha.regions = .08) + \n  mapview(cal_ws2, alpha.regions = .08, col.regions = 'red') \n# Trib coordinates\nlatitude <- 44.61892\nlongitude <- -123.13731\n\ntrib_pt2 <- st_sfc(st_point(c(longitude, latitude)), crs = 4269)\nstart_comid <- nhdplusTools::discover_nhdplus_id(trib_pt2)\nws_source <- list(featureSource = \"comid\", featureID = start_comid)\n\ntrib_ws2 <- nhdplusTools::get_nldi_basin(nldi_feature = ws_source)\n\nstreams <- nhdplusTools::navigate_nldi(ws_source, mode = \"UT\", \n                                       distance_km = 2000) %>%\n  pluck('UT_flowlines')\n\n# Use get_nhdplus to access the individual stream sub-catchments\ncats <- nhdplusTools::get_nhdplus(comid = streams$nhdplus_comid,\n                                  realization = 'catchment')\n\nmapview(streams, legend = FALSE) + \n  mapview(cal_pt2, col.regions = 'red') + \n  mapview(trib_pt2, col.regions = 'blue') + \n  mapview(cal_ws2, alpha.regions=.08, col.regions = 'red') +\n  mapview(cats, alpha.regions=.08, col.regions = 'blue') \n\nlibrary(FedData)\nlibrary(terra)\nlibrary(tidyterra)\nlibrary(tictoc)\nlibrary(nhdplusTools)\nlibrary(sf)\n\ntic()\n\n# Logan River Watershed\nlatitude <- 41.707\nlongitude <- -111.855\n\n# Define the lat/lon\nstart_point <- st_sfc(st_point(c(longitude, latitude)), crs = 4269)\n\n# Find COMID of this point\nstart_comid <- nhdplusTools::discover_nhdplus_id(start_point)\n\n# Create a list object that defines the feature source and starting COMID\nws_source <- list(featureSource = \"comid\", featureID = start_comid)\n\n# Delineate basin\nlogan_ws <- nhdplusTools::get_nldi_basin(nldi_feature = ws_source) %>%\n\n  # Transform to EPSG 5070\n  st_transform(crs = 5070)\n\n# Alternate method\n# logan_ws <- streamstats_ws('UT', longitude, latitude) %>% \n#   st_transform(crs = 5070)\n\nnlcd <- get_nlcd(\n  template = logan_ws,\n  year = 2021,\n  label = 'Logan') %>%\n  \n  # Project raster to EPSG 5070\n  terra::project('epsg:5070', method = 'near') %>% \n  \n  # Mask the raster\n  terra::mask(logan_ws) %>% \n  \n  # Clip the raster\n  terra::crop(logan_ws)\n\nggplot() + \n  geom_spatraster(data = nlcd) +\n  geom_sf(data = logan_ws,\n          fill = NA,\n          lwd = 1.5,\n          color = 'white') +\n  theme_bw()\n\nlogan_nlcd <- terra::extract(nlcd, \n                             terra::vect(logan_ws))\n\nlogan_nlcd %>% \n  group_by(Class) %>% \n  summarise(count = n()) %>% \n  mutate(percent = (count / sum(count)) * 100)\n\ntoc()"
  },
  {
    "objectID": "streamcattools.html#why-streamcatlakecat",
    "href": "streamcattools.html#why-streamcatlakecat",
    "title": "6  StreamCat & LakeCat",
    "section": "\n6.1 Why StreamCat/LakeCat?",
    "text": "6.1 Why StreamCat/LakeCat?\nThe examples of watershed delineation and metric extraction we covered thus far have been for relatively small systems. The process to generate watershed information can be far more complicated for large watersheds, when the desired metrics are needed at many sites across a large geographic extent, or when data layers that are not served as part of other R packages.\nFor these reasons, we developed the StreamCat and LakeCat datasets. StreamCat uses the medium resolution NHPlus (version 2.1) as its geospatial framework to provide watershed summaries of several hundred metrics for all accumulative watersheds in the NHDPlus - about 2.6 million stream segments. Likewise, LakeCat provides watershed metrics for close to 400,000 lakes across the conterminous U.S. as well.\nBoth datasets are now accessible through the StreamCatTools package."
  },
  {
    "objectID": "streamcattools.html#accessing-streamcat",
    "href": "streamcattools.html#accessing-streamcat",
    "title": "6  StreamCat & LakeCat",
    "section": "\n6.2 Accessing StreamCat",
    "text": "6.2 Accessing StreamCat\nLet’s revisit the Calapooia watershed and extract percent row crop for 2019:\n\nlibrary(FedData)\nlibrary(terra)\nlibrary(nhdplusTools)\nlibrary(sf)\nlibrary(tidyverse)\n\n# Calapooia Watershed\nlatitude <- 44.62892\nlongitude <- -123.13113\n\n# Make point sf of sample site\npt <- data.frame(site_name = 'Calapooia',\n                 longitude = longitude,\n                 latitude = latitude) %>% \n  st_as_sf(coords = c('longitude', 'latitude'), crs = 4269)\n\n# Generate watershed\ncal_ws <- pt %>% \n  nhdplusTools::discover_nhdplus_id() %>% \n  list(featureSource = \"comid\", featureID = .) %>% \n  nhdplusTools::get_nldi_basin() %>% \n  st_transform(crs = 5070)\n\n# Download NLCD for 2019\nnlcd <- get_nlcd(\n  template = cal_ws,\n  year = 2019,\n  label = 'Calapooia') %>%\n  terra::project('epsg:5070', method = 'near') \n\n# Use terra to extract the watershed metrics\nterra::extract(nlcd,\n               terra::vect(cal_ws)) %>%\n  group_by(Class) %>%\n  summarise(count = n()) %>%\n  mutate(percent = (count / sum(count)) * 100) %>%\n  filter(Class == 'Cultivated Crops')\n#> # A tibble: 1 × 3\n#>   Class             count percent\n#>   <fct>             <int>   <dbl>\n#> 1 Cultivated Crops 112475    10.5\n\nNow let’s extract the same information using StreamCatTools.\nWe can find the metric name by consulting the StreamCat variable list.\nI can also be useful to explore the online map and table interfaces of StreamCat.\n\nlibrary(StreamCatTools)\n\ncomid <- sc_get_comid(pt)\n\nsc_get_data(comid = comid,\n            metric = 'PctCrop2019', \n            aoi = 'watershed')\n#> # A tibble: 1 × 3\n#>      COMID WSAREASQKM PCTCROP2019WS\n#>      <dbl>      <dbl>         <dbl>\n#> 1 23763521       965.          10.5\n\nStreamCat pre-stages the calculated metrics in an online database and accessible via an API to make them available to the public.\nIn addition to watershed-level summaries, StreamCat provides metrics for the local catchment (i.e., the area draining to the stream segment, excluding upstream sources).\n\n\nFigure 6.1: Geospatial framework of the StreamCat Dataset\n\n\n\nsc_get_data(comid = comid,\n            metric = 'PctCrop2019', \n            aoi = 'catchment,watershed')\n#> # A tibble: 1 × 5\n#>      COMID WSAREASQKM CATAREASQKM PCTCROP2019CAT PCTCROP2019WS\n#>      <dbl>      <dbl>       <dbl>          <dbl>         <dbl>\n#> 1 23763521       965.        8.49           36.6          10.5\n\nWe can see the difference between the local catchment and accumulative watershed boundaries in this map.\n\nlibrary(mapview)\nlibrary(nhdplusTools)\nlibrary(tidyverse)\ntrib_pt2 <- st_sfc(st_point(c(longitude, latitude)), crs = 4269)\nstart_comid <- nhdplusTools::discover_nhdplus_id(trib_pt2)\nws_source <- list(featureSource = \"comid\", featureID = start_comid)\n\ntrib_ws2 <- nhdplusTools::get_nldi_basin(nldi_feature = ws_source)\n\nstreams <- nhdplusTools::navigate_nldi(ws_source, mode = \"UT\", \n                                       distance_km = 2000) %>%\n  pluck('UT_flowlines')\n\n# Use get_nhdplus to access the individual stream sub-catchments\nall_catchments <- nhdplusTools::get_nhdplus(comid = streams$nhdplus_comid,\n                                            realization = 'catchment')\n\nfocal_cat <- nhdplusTools::get_nhdplus(comid = start_comid,\n                                       realization = 'catchment')\n\nmapview(streams, color='blue', legend = FALSE) +\n  mapview(focal_cat, alpha.regions=.08, col.regions = 'red') +\n  mapview(all_catchments, alpha.regions=.08, col.regions = 'yellow') \n\n\n\n\n\n\nFor a subset of watershed metrics, summaries within ~100m of the stream segment are also available for catchments and watersheds:\n\n\nFigure 6.2: Riparian buffers (red) of NHD stream lines (white) and on-network NLCD water pixels (blue).\n\n\n\nsc_get_data(comid = comid,\n            metric = 'PctCrop2019', \n            aoi = 'catchment,riparian_catchment,watershed,riparian_watershed') %>% \n  as.data.frame()\n#>      COMID WSAREASQKM WSAREASQKMRP100 CATAREASQKM CATAREASQKMRP100\n#> 1 23763521   964.6101          136.17      8.4933           1.1097\n#>   PCTCROP2019CATRP100 PCTCROP2019WSRP100 PCTCROP2019CAT PCTCROP2019WS\n#> 1                9.08                8.4          36.59         10.49\n\nStreamCatTools also makes it very easy to grab data for entire regions. In this example, we will extract % crop for the entire state of Iowa and plot these percentages as a histogram.\n\nlibrary(ggplot2)\n\niowa_crop <- sc_get_data(state = 'IA',\n                         metric = 'PctCrop2019', \n                         aoi = 'watershed')\n\nggplot() + \n  geom_histogram(data = iowa_crop,\n                 aes(x = PCTCROP2019WS)) + \n  theme_bw()\n\n\n\n\nWe can provide multiple metrics to the request by separating them with a comma.\nNote that the request is provided as a single string, 'PctCrop2001, PctCrop2019', rather than a vector of metrics: c('PctCrop2001', 'PctCrop2019'). However, the request itself is agnostic to formatting of the text. For example, these requests will also work: 'pctcrop2001, pctcrop2019' or 'PCTCROP2001,PCTCROP2019'.\nHere, we compare % watershed crop in 2001 to 2019:\n\niowa_crop <- sc_get_data(state = 'IA',\n                         metric = 'PctCrop2001, PctCrop2019', \n                         aoi = 'watershed')\n\n# Pivot table to long format\niowa_crop_long <- iowa_crop %>% \n  pivot_longer(\n    cols = !COMID:WSAREASQKM,\n    values_to = 'PctCrop',\n    names_to = 'Year')\n\nggplot() + \n  geom_boxplot(data = iowa_crop_long,\n               aes(x = Year,\n                   y = PctCrop)) + \n  theme_bw()\n\n\n\n\nStreamCat contains hundreds of metrics and we recommend consulting the metric list to identify those of interest for your study."
  },
  {
    "objectID": "streamcattools.html#accessing-lakecat",
    "href": "streamcattools.html#accessing-lakecat",
    "title": "6  StreamCat & LakeCat",
    "section": "\n6.3 Accessing LakeCat",
    "text": "6.3 Accessing LakeCat\nLike StreamCat, LakeCat makes local catchment and watershed metrics available for lakes across the lower 48 states. Like StreamCat, the metrics were developed from a framework of pre-staged lake-catchments that allowed for accumulation of results, even when lakes are nested.\nThe challenge of watershed delineation and extraction of data is even greater than for lakes than streams. There are no online services that do this. Therefore, LakeCat is one of the only ways to get watershed metrics for lakes (however, see LAGOS-NE and LAGOS-US GEO).\n As with StreamCat, the Metrics and Definitions page is the best way to examine which variables are available in LakeCat. Currently, fewer variables are available in LakeCat than StreamCat, mainly due to the fact that riparian buffers were not included as an “AOI” for lakes.\nThe R function to access LakeCat data was designed to parallel StreamCat functions. In this example, we:\n\nDefine a sf object of a sample point at Pelican Lake, WI.\nObtain the lake waterbody polygon.\nExtract the COMID (unique ID) to query LakeCat.\nPull data on mean watershed elevation, calcium oxide content of the geology, % sand and organic matter content of soils, and % of the watershed composed of deciduous forest.\n\n\nlibrary(nhdplusTools)\n\n# Pelican Lake, WI\nlatitude <- 45.502840\nlongitude <- -89.198694\n\npelican_pt <- data.frame(site_name = 'Pelican Lake',\n                         latitude = latitude,\n                         longitude = longitude) %>% \n  st_as_sf(coords = c('longitude', 'latitude'), crs = 4326)\n\npelican_lake <- nhdplusTools::get_waterbodies(pelican_pt) \n\ncomid <- pelican_lake %>% \n  pull(comid)\n\nlc_get_data(metric = 'elev, cao, sand, om, pctdecid2019',\n            aoi = 'watershed',\n            comid = comid)\n#> # A tibble: 1 × 7\n#>       COMID WSAREASQKM SANDWS ELEVWS CAOWS PCTDECID2019WS  OMWS\n#>       <dbl>      <dbl>  <dbl>  <dbl> <dbl>          <dbl> <dbl>\n#> 1 167120863       41.0   59.6   491.  4.81           14.7  16.7\n\nAt this time, the ability to query LakeCat based on geography, such as state, county, or hydrologic region is still forthcoming. However, it is possible to query based on multiple COMIDs. In this code, we will create a 100Km buffer around pelican lake, acces those lakes via nhdplusTools, and access LakeCat data for these lakes. We’ll then map lakes and color them by watershed area and compare lake areas with watershed areas.\n\n# 100km buffer to pelican point\npelican_buffer <- \n  pelican_pt %>% \n  st_buffer(10000)\n\npelican_neighbors <- nhdplusTools::get_waterbodies(pelican_buffer) %>% \n  filter(ftype == 'LakePond') %>% \n  rename(COMID = comid)\n\ncomids <- pelican_neighbors %>% \n  pull(COMID)\n\nlc_data <- lc_get_data(metric = 'elev, cao, sand, om, pctdecid2019',\n                       aoi = 'watershed',\n                       comid = comids) \n\npelican_neighbors <- \n  pelican_neighbors %>% \n  left_join(lc_data, join_by('COMID'))\n\nggplot() + \n  geom_sf(data = pelican_neighbors, \n          aes(fill = WSAREASQKM)) + \n  geom_sf(data = pelican_buffer,\n          color = 'red',\n          fill = NA) + \n  scale_fill_viridis_c(option = \"magma\") + \n  theme_bw()\n\n\n\n\nggplot() + \n  geom_point(data = pelican_neighbors,\n             aes(x = WSAREASQKM, \n                 y = areasqkm), \n             size = 4, alpha = 0.5) +\n  scale_x_log10() +\n  xlab('Watershed Area (Km2)') +\n  scale_y_log10() +\n  ylab('Lake Area (Km2)') +\n  theme_bw()\n\n\n\n\nIn the next sections, we will put what we have learned about GIS in R together with the spmodel package to model two types of data. First, we will model the presence/absence of a damselfly genus (Argia) in the northeastern U.S. streams. We’ll access Argia presences/absences via another R package that was recently released called finsyncR. In the second example, we will model the specific conductivity of lakes in several midwestern states."
  },
  {
    "objectID": "streamcattools.html#r-code-appendix",
    "href": "streamcattools.html#r-code-appendix",
    "title": "6  StreamCat & LakeCat",
    "section": "\n6.4 R Code Appendix",
    "text": "6.4 R Code Appendix\n\n\n\n\nlibrary(FedData)\nlibrary(terra)\nlibrary(nhdplusTools)\nlibrary(sf)\nlibrary(tidyverse)\n\n# Calapooia Watershed\nlatitude <- 44.62892\nlongitude <- -123.13113\n\n# Make point sf of sample site\npt <- data.frame(site_name = 'Calapooia',\n                 longitude = longitude,\n                 latitude = latitude) %>% \n  st_as_sf(coords = c('longitude', 'latitude'), crs = 4269)\n\n# Generate watershed\ncal_ws <- pt %>% \n  nhdplusTools::discover_nhdplus_id() %>% \n  list(featureSource = \"comid\", featureID = .) %>% \n  nhdplusTools::get_nldi_basin() %>% \n  st_transform(crs = 5070)\n\n# Download NLCD for 2019\nnlcd <- get_nlcd(\n  template = cal_ws,\n  year = 2019,\n  label = 'Calapooia') %>%\n  terra::project('epsg:5070', method = 'near') \n\n# Use terra to extract the watershed metrics\nterra::extract(nlcd,\n               terra::vect(cal_ws)) %>%\n  group_by(Class) %>%\n  summarise(count = n()) %>%\n  mutate(percent = (count / sum(count)) * 100) %>%\n  filter(Class == 'Cultivated Crops')\nlibrary(StreamCatTools)\n\ncomid <- sc_get_comid(pt)\n\nsc_get_data(comid = comid,\n            metric = 'PctCrop2019', \n            aoi = 'watershed')\nsc_get_data(comid = comid,\n            metric = 'PctCrop2019', \n            aoi = 'catchment,watershed')\nlibrary(mapview)\nlibrary(nhdplusTools)\nlibrary(tidyverse)\ntrib_pt2 <- st_sfc(st_point(c(longitude, latitude)), crs = 4269)\nstart_comid <- nhdplusTools::discover_nhdplus_id(trib_pt2)\nws_source <- list(featureSource = \"comid\", featureID = start_comid)\n\ntrib_ws2 <- nhdplusTools::get_nldi_basin(nldi_feature = ws_source)\n\nstreams <- nhdplusTools::navigate_nldi(ws_source, mode = \"UT\", \n                                       distance_km = 2000) %>%\n  pluck('UT_flowlines')\n\n# Use get_nhdplus to access the individual stream sub-catchments\nall_catchments <- nhdplusTools::get_nhdplus(comid = streams$nhdplus_comid,\n                                            realization = 'catchment')\n\nfocal_cat <- nhdplusTools::get_nhdplus(comid = start_comid,\n                                       realization = 'catchment')\n\nmapview(streams, color='blue', legend = FALSE) +\n  mapview(focal_cat, alpha.regions=.08, col.regions = 'red') +\n  mapview(all_catchments, alpha.regions=.08, col.regions = 'yellow') \nsc_get_data(comid = comid,\n            metric = 'PctCrop2019', \n            aoi = 'catchment,riparian_catchment,watershed,riparian_watershed') %>% \n  as.data.frame()\nlibrary(ggplot2)\n\niowa_crop <- sc_get_data(state = 'IA',\n                         metric = 'PctCrop2019', \n                         aoi = 'watershed')\n\nggplot() + \n  geom_histogram(data = iowa_crop,\n                 aes(x = PCTCROP2019WS)) + \n  theme_bw()\niowa_crop <- sc_get_data(state = 'IA',\n                         metric = 'PctCrop2001, PctCrop2019', \n                         aoi = 'watershed')\n\n# Pivot table to long format\niowa_crop_long <- iowa_crop %>% \n  pivot_longer(\n    cols = !COMID:WSAREASQKM,\n    values_to = 'PctCrop',\n    names_to = 'Year')\n\nggplot() + \n  geom_boxplot(data = iowa_crop_long,\n               aes(x = Year,\n                   y = PctCrop)) + \n  theme_bw()\nlibrary(nhdplusTools)\n\n# Pelican Lake, WI\nlatitude <- 45.502840\nlongitude <- -89.198694\n\npelican_pt <- data.frame(site_name = 'Pelican Lake',\n                         latitude = latitude,\n                         longitude = longitude) %>% \n  st_as_sf(coords = c('longitude', 'latitude'), crs = 4326)\n\npelican_lake <- nhdplusTools::get_waterbodies(pelican_pt) \n\ncomid <- pelican_lake %>% \n  pull(comid)\n\nlc_get_data(metric = 'elev, cao, sand, om, pctdecid2019',\n            aoi = 'watershed',\n            comid = comid)\n# 100km buffer to pelican point\npelican_buffer <- \n  pelican_pt %>% \n  st_buffer(10000)\n\npelican_neighbors <- nhdplusTools::get_waterbodies(pelican_buffer) %>% \n  filter(ftype == 'LakePond') %>% \n  rename(COMID = comid)\n\ncomids <- pelican_neighbors %>% \n  pull(COMID)\n\nlc_data <- lc_get_data(metric = 'elev, cao, sand, om, pctdecid2019',\n                       aoi = 'watershed',\n                       comid = comids) \n\npelican_neighbors <- \n  pelican_neighbors %>% \n  left_join(lc_data, join_by('COMID'))\n\nggplot() + \n  geom_sf(data = pelican_neighbors, \n          aes(fill = WSAREASQKM)) + \n  geom_sf(data = pelican_buffer,\n          color = 'red',\n          fill = NA) + \n  scale_fill_viridis_c(option = \"magma\") + \n  theme_bw()\n\nggplot() + \n  geom_point(data = pelican_neighbors,\n             aes(x = WSAREASQKM, \n                 y = areasqkm), \n             size = 4, alpha = 0.5) +\n  scale_x_log10() +\n  xlab('Watershed Area (Km2)') +\n  scale_y_log10() +\n  ylab('Lake Area (Km2)') +\n  theme_bw()"
  },
  {
    "objectID": "spatial-glm.html#data-prep",
    "href": "spatial-glm.html#data-prep",
    "title": "7  Spatial GLM of “Dancer” damselfly (Argia)",
    "section": "\n7.1 Data Prep",
    "text": "7.1 Data Prep\n\n7.1.1 Biological (Dependent) Data\nFirst, load finsyncR, spmodel, StreamCatTools, and other needed packages.\n\nlibrary(finsyncR)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(StreamCatTools)\nlibrary(nhdplusTools)\nlibrary(spmodel)\nlibrary(data.table)\nlibrary(pROC)\nlibrary(knitr)\n\nNext, we will use finsyncR to get genus-level macroinvert data from just EPA and rarefy to 500 count. The code will also convert the data to occurrence data (1 = detect, 0 = non-detect) and set a seed to make it reproducible. Finally, we will include samples from boatable streams rather than just those that are wadeable.\n\nmacros <- getInvertData(dataType = \"occur\",\n                        taxonLevel = \"Genus\",\n                        agency = \"EPA\",\n                        lifestage = FALSE,\n                        rarefy = TRUE,\n                        rarefyCount = 300,\n                        sharedTaxa = FALSE,\n                        seed = 1,\n                        boatableStreams = T)\n#> \n finsyncR is running: Gathering, joining, and cleaning EPA raw data                    \n finsyncR is running: Rarefying EPA data                              \n finsyncR is running: Applying taxonomic fixes to EPA data                    \n finsyncR is running: Finalizing data for output                          \n finsyncR data synchronization complete\n\nprint(dim(macros))\n#> [1] 6174  856\n\n# Print an example of the data\nkable(macros[1:5, 1:23])\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAgency\nSampleID\nProjectLabel\nSiteNumber\nCollectionDate\nCollectionYear\nCollectionMonth\nCollectionDayOfYear\nLatitude_dd\nLongitude_dd\nCoordinateDatum\nCOMID\nStreamOrder\nWettedWidth\nPredictedWettedWidth_m\nNARS_Ecoregion\nSampleTypeCode\nAreaSampTot_m2\nFieldSplitRatio\nLabSubsamplingRatio\nPropID\nGen_ID_Prop\nAblabesmyia\n\n\n\nEPA\n10000\nNRSA0809\nNRS_KS-10018\n2008-08-05\n2008\n8\n218\n39.01491\n-98.01046\nNAD83\n18865370\n2\n2.2525\n2.68\nSPL\nBERWW\n1.022\nNA\nNA\n1.0000000\n0.9373650\n1\n\n\nEPA\n10001\nNRSA0809\nNRS_KS-10008\n2008-08-11\n2008\n8\n224\n37.39528\n-98.92628\nNAD83\n21012349\n3\n4.9850\n5.37\nSPL\nBERWW\n0.372\nNA\nNA\n0.1770852\n0.8576998\n0\n\n\nEPA\n10004\nNRSA0809\nNRS_KS-10040\n2008-07-15\n2008\n7\n197\n37.78662\n-96.42997\nNAD83\n21515712\n4\n10.0800\n9.05\nTPL\nBERWW\n1.022\nNA\nNA\n0.4374453\n0.9310987\n1\n\n\nEPA\n10006\nNRSA0809\nNRS_KS-10064\n2008-08-25\n2008\n8\n238\n37.99323\n-99.32134\nNAD83\n22082845\n7\n4.5150\n9.18\nSPL\nBERWW\n0.186\nNA\nNA\n0.2187705\n0.9659091\n0\n\n\nEPA\n1002013\nNRSA1314\nNRS_KS-10126\n2013-04-30\n2013\n4\n120\n39.12843\n-95.65402\nNAD83\n3645992\n2\n5.5500\n5.18\nTPL\nBERW\n1.022\nNA\nNA\n0.2916983\n0.9026718\n0\n\n\n\n\n\nLet’s massage the table with dplyr to get what we want, including data on the occurences of Argia:\n\nSelect columns of interest, including the Argia occurrence column.\nRemove data related to the EPA’s Wadeable Streams Assessment (2001-2004).\nConvert “CollectionDate” to a date (lubridate::date()) and convert presence/absence to a factor.\nFinally, convert table to a sfobject and transform to EPSG:5070.\n\n\n# Flexible code so we could model another taxon\ngenus <- 'Argia'\n\ntaxon = macros %>%\n  dplyr::select(SampleID, \n                ProjectLabel, \n                CollectionDate,  \n                Latitude_dd,\n                Longitude_dd,\n                all_of(genus))  %>%\n  #filter(ProjectLabel != 'WSA') %>% \n  mutate(CollectionDate = date(CollectionDate),\n         presence = \n           as.factor(pull(., genus)))  %>% \n  st_as_sf(coords = c('Longitude_dd', 'Latitude_dd'), crs = 4269)  %>% \n  st_transform(crs = 5070)\n\nTo visualize the data, we will read in a layer of lower 48 states to give some context. To do this we can use the tigris package. We will also remove non-conterminous states and transform the projection to our favorite - EPSG:5070.\n\nstates <- tigris::states(cb = TRUE, progress_bar = FALSE)  %>% \n  filter(!STUSPS %in% c('HI', 'PR', 'AK', 'MP', 'GU', 'AS', 'VI'))  %>% \n  st_transform(crs = 5070)\n\nFirst, let’s plot the observed presences/absences in the data…\n\nggplot() + \n  geom_sf(data = states, fill = NA) +\n  geom_sf(data = taxon, \n          aes(color = presence),\n          size = 1.5,\n          alpha = 0.65) + \n  scale_color_manual(values=c(\"#d9d9d9\", \"#08519c\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\") \n\n\n\n\n…and by NRSA survey period:\n\nggplot() + \n  geom_sf(data = states, fill = NA) +\n  geom_sf(data = taxon, \n          aes(color = ProjectLabel),\n          size = 1.5,\n          alpha = 0.75) + \n  scale_color_manual(values=c(\"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\") \n\n\n\n\nFor today’s exercise, we’ll narrow down the samples to the northeaster region of the U.S.\n\nSelect states from the northeaster U.S.\nSelect NRSA sample sites that intersect with these states.\nFilter to just the 2013/2014 and 2018/2019 sample periods.\nCreate a new column for sample year.\nSelect desired columns.\n\n\n\n# Filter to study region (states)\nregion <- states %>% \n  filter(STUSPS %in% c('VT', 'NH', 'ME', 'NY', 'RI',\n                       'MA', 'CT', 'NJ', 'PA', 'DE'))\n\n# Use region as spatial filter (sf::st_filter()) for taxon of interest\ntaxon_rg <- taxon %>% \n  st_filter(region) %>% \n  filter(ProjectLabel %in% c('NRSA1314', 'NRSA1819')) %>% \n  mutate(year = year(ymd(CollectionDate))) %>% \n  select(SampleID:CollectionDate, presence:year) \n\nggplot() + \n  geom_sf(data = region, fill = NA) +\n  geom_sf(data = taxon_rg, \n          aes(color = presence)) + \n  scale_color_manual(values=c(\"#d9d9d9\", \"#08519c\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\") \n\n\n\n\nggplot() + \n  geom_sf(data = region, fill = NA) +\n  geom_sf(data = taxon_rg, \n          aes(color = ProjectLabel)) + \n  scale_color_manual(values=c(\"#1f78b4\", \"#b2df8a\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\") \n\n\n\n\ntaxon_rg %>% \n  pull(presence) %>% \n  table()\n#> .\n#>   0   1 \n#> 485  65\n\n\n7.1.2 Predictor (Independent) Data\n\nObtain list of NHDPlus COMIDs that match sample sites from nhdplusTools\n\n\n\nUse NLDI service via StreamCat to get the COMIDs\nCreate a vector of COMIDs by splitting the COMID string\nAdd COMID to our Argia occurrence table\n\n\ncomids <- sc_get_comid(taxon_rg)\n\n#comids <- read_rds('./data/nrsa_comids.rds')\ncomid_vect <- \n  comids %>%\n  str_split(',') %>%\n  unlist() %>%\n  as.integer()\n\ntaxon_rg <- \n  taxon_rg %>%\n  mutate(COMID = comid_vect) \n\n\nGet non-varying StreamCat data.\n\n\nsc <- \n  sc_get_data(comid = comids,\n              aoi = 'watershed',\n              metric = 'bfi, precip8110, wetindex, elev',\n              showAreaSqKm = TRUE)\n\n\nGet year-specific wetlands within watersheds.\n\n\nNLCD contains data on the distribution of herbaceous (pcthbwet) and woody (pctwdwet) wetlands. We will combine them into a single metric representing the % of the watershed comprised of wetlands.\nDuplicate the data, but offset by 1 year so we can 2019 NLCD to 2018 observations (same for 2013 NLCD and 2014 NRSA).\n\n\nwetlands <- \n  sc_get_data(comid = comids,\n              aoi = 'watershed',\n              metric = 'pctwdwet2013,pcthbwet2013,pctwdwet2019,pcthbwet2019',\n              showAreaSqKm = FALSE) %>% \n  \n  # Sum wetland types to create single wetlands metric\n  mutate(PCTWETLAND2013WS = PCTHBWET2013WS + PCTWDWET2013WS,\n         PCTWETLAND2019WS = PCTHBWET2019WS + PCTWDWET2019WS) %>% \n  \n  # Reduce columns\n  select(COMID, PCTWETLAND2013WS, PCTWETLAND2019WS) %>% \n  \n  # Create long table w/ column name w/out year\n  pivot_longer(!COMID, names_to = 'tmpcol', values_to = 'PCTWETLANDXXXXWS') %>% \n  \n  # Create new column of year by removing \"PCTWETLAND\" and \"WS\" from names\n  mutate(year = as.integer(str_replace_all(tmpcol, 'PCTWETLAND|WS', ''))) \n\n# But some samples have 2014 and 2018 as sample years? How can we trick the data into joining?\n# We can match 2019 data to 2018 observations by subtracting a year and appending it to the data\n\n# Create tmp table with 1 added or subtracted to year of record\ntmp_wetlands <- wetlands %>% \n  mutate(year = ifelse(year == 2013, year + 1, year - 1))\n\n# rbind() wetlands and tmp_wetlands so we have records to join to 2014 and 2018\nwetlands <- wetlands %>% \n  rbind(tmp_wetlands) %>% \n  select(-tmpcol)\n\n\nYear-specific impervious surfaces within 100-m riparian buffer.\n\n\nriparian_imp <- \n  sc_get_data(comid = comids,\n              aoi = 'riparian_watershed',\n              metric = 'pctimp2013, pctimp2019',\n              showAreaSqKm = FALSE) %>% \n  select(-WSAREASQKMRP100) %>% \n  pivot_longer(!COMID, names_to = 'tmpcol', values_to = 'PCTIMPXXXXWSRP100') %>% \n  mutate(year = as.integer(\n    str_replace_all(tmpcol, 'PCTIMP|WSRP100', '')))\n\ntmp_imp <- riparian_imp %>% \n  mutate(year = ifelse(year == 2013, year + 1, year - 1))\n\nriparian_imp <- riparian_imp %>% \n  rbind(tmp_imp) %>% \n  select(-tmpcol)\n\n\nPRISM air temperatures for sample periods\n\n\nThe prism package requires that we set a temporary folder in our work space. Here, we set it to “prism_data” inside of our “data” folder. It will create this folder if it does not already exist.\nWe then stack the climate rasters and use terra::extract() to\n\n\nlibrary(prism)\n\n# Get these years of PRISM\nyears <- c(2013, 2014, 2018, 2019)\n\n# Set the PRISM directory (creates directory in not present)\nprism_set_dl_dir(\"./data/prism_data\", create = TRUE)\n\n# Download monthly PRISM rasters (tmean)\nget_prism_monthlys('tmean', \n                   years = years, \n                   mon = 7:8, \n                   keepZip = FALSE)\n#> \n  |                                                                         \n  |                                                                   |   0%\n  |                                                                         \n  |========                                                           |  12%\n  |                                                                         \n  |=================                                                  |  25%\n  |                                                                         \n  |=========================                                          |  38%\n  |                                                                         \n  |==================================                                 |  50%\n  |                                                                         \n  |==========================================                         |  62%\n  |                                                                         \n  |==================================================                 |  75%\n  |                                                                         \n  |===========================================================        |  88%\n  |                                                                         \n  |===================================================================| 100%\n\n# Create stack of downloaded PRISM rasters\ntmn <- pd_stack((prism_archive_subset(\"tmean\",\"monthly\", \n                                      years = years, \n                                      mon = 7:8)))\n\n# Extract tmean at sample points and massage data\ntmn <- terra::extract(tmn, \n                      # Transform taxon_rg to CRS of PRISM on the fly\n                      taxon_rg %>% \n                        st_transform(crs = st_crs(tmn))) %>%\n  \n  # Add COMIDs to extracted values\n  data.frame(COMID = comid_vect, .) %>%\n  \n  # Remove front and back text from PRISM year/month in names\n  rename_with( ~ stringr::str_replace_all(., 'PRISM_tmean_stable_4kmM3_|_bil', '')) %>% \n  \n  # Pivot to long table and calle column TMEANPRISMXXXXPT, XXXX indicates year\n  pivot_longer(!COMID, names_to = 'year_month', \n               values_to = 'TMEANPRISMXXXXPT') %>% \n  \n  # Create new column of year\n  mutate(year = year(ym(year_month))) %>% \n  \n  # Average July and August temperatures \n  summarise(TMEANPRISMXXXXPT = mean(TMEANPRISMXXXXPT, na.rm = TRUE), \n            .by = c(COMID, year))\n\n\n7.1.3 Combine Dependent and Independent Data\n\nmodel_data <-\n  taxon_rg %>%\n  left_join(sc, join_by(COMID)) %>%\n  left_join(wetlands, join_by(COMID, year)) %>%\n  left_join(riparian_imp, join_by(COMID, year)) %>%\n  left_join(tmn, join_by(COMID, year)) %>%\n  drop_na()\n\ncor(model_data %>%\n      st_drop_geometry() %>%\n      select(WSAREASQKM:TMEANPRISMXXXXPT))\n#>                    WSAREASQKM      ELEVWS WETINDEXWS       BFIWS\n#> WSAREASQKM         1.00000000  0.19649723 -0.0844127 -0.17679333\n#> ELEVWS             0.19649723  1.00000000 -0.6020139 -0.56349466\n#> WETINDEXWS        -0.08441270 -0.60201389  1.0000000  0.40544326\n#> BFIWS             -0.17679333 -0.56349466  0.4054433  1.00000000\n#> PRECIP8110WS      -0.15455456 -0.02973549 -0.1213487  0.17231568\n#> PCTWETLANDXXXXWS  -0.13274790 -0.53982414  0.7838752  0.49003820\n#> PCTIMPXXXXWSRP100 -0.10603160 -0.39811621  0.1008221  0.07181972\n#> TMEANPRISMXXXXPT   0.08241251 -0.68038439  0.3688678  0.25976247\n#>                   PRECIP8110WS PCTWETLANDXXXXWS PCTIMPXXXXWSRP100\n#> WSAREASQKM         -0.15455456      -0.13274790       -0.10603160\n#> ELEVWS             -0.02973549      -0.53982414       -0.39811621\n#> WETINDEXWS         -0.12134865       0.78387523        0.10082210\n#> BFIWS               0.17231568       0.49003820        0.07181972\n#> PRECIP8110WS        1.00000000       0.03862313        0.17634436\n#> PCTWETLANDXXXXWS    0.03862313       1.00000000       -0.04885782\n#> PCTIMPXXXXWSRP100   0.17634436      -0.04885782        1.00000000\n#> TMEANPRISMXXXXPT    0.04030682       0.24384288        0.43121267\n#>                   TMEANPRISMXXXXPT\n#> WSAREASQKM              0.08241251\n#> ELEVWS                 -0.68038439\n#> WETINDEXWS              0.36886778\n#> BFIWS                   0.25976247\n#> PRECIP8110WS            0.04030682\n#> PCTWETLANDXXXXWS        0.24384288\n#> PCTIMPXXXXWSRP100       0.43121267\n#> TMEANPRISMXXXXPT        1.00000000"
  },
  {
    "objectID": "spatial-glm.html#modeling-occurrece-of-genus-argia",
    "href": "spatial-glm.html#modeling-occurrece-of-genus-argia",
    "title": "7  Spatial GLM of “Dancer” damselfly (Argia)",
    "section": "\n7.2 Modeling occurrece of genus Argia\n",
    "text": "7.2 Modeling occurrece of genus Argia\n\n\n7.2.1 Model formulation\n\nformula <-\n  presence ~\n  I(log10(WSAREASQKM)) +\n  ELEVWS +\n  WETINDEXWS +\n  BFIWS +\n  PRECIP8110WS +\n  PCTWETLANDXXXXWS +\n  PCTIMPXXXXWSRP100 +\n  TMEANPRISMXXXXPT\n\nbin_mod <- spglm(formula = formula,\n                 data = model_data,\n                 family = 'binomial',\n                 spcov_type = 'none')\n\nbin_spmod <- spglm(formula = formula,\n                   data = model_data,\n                   family = 'binomial',\n                   spcov_type = 'exponential')\n\nglances(bin_mod, bin_spmod)\n#> # A tibble: 2 × 10\n#>   model      n     p  npar value   AIC  AICc logLik deviance pseudo.r.squared\n#>   <chr>  <int> <dbl> <int> <dbl> <dbl> <dbl>  <dbl>    <dbl>            <dbl>\n#> 1 bin_s…   550     9     3 1394. 1400. 1400.  -697.     201.            0.110\n#> 2 bin_m…   550     9     1 1409. 1411. 1411.  -704.     342.            0.145\n\nsummary(bin_spmod)\n#> \n#> Call:\n#> spglm(formula = formula, family = \"binomial\", data = model_data, \n#>     spcov_type = \"exponential\")\n#> \n#> Deviance Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -1.4692 -0.4226 -0.2762 -0.1450  2.4044 \n#> \n#> Coefficients (fixed):\n#>                       Estimate Std. Error z value Pr(>|z|)    \n#> (Intercept)           8.089431   6.729301   1.202   0.2293    \n#> I(log10(WSAREASQKM))  0.808323   0.202435   3.993 6.52e-05 ***\n#> ELEVWS               -0.006700   0.002643  -2.535   0.0112 *  \n#> WETINDEXWS            0.003059   0.004851   0.631   0.5283    \n#> BFIWS                -0.032951   0.042665  -0.772   0.4399    \n#> PRECIP8110WS         -0.003344   0.002894  -1.156   0.2478    \n#> PCTWETLANDXXXXWS     -0.016386   0.048407  -0.339   0.7350    \n#> PCTIMPXXXXWSRP100     0.088456   0.047796   1.851   0.0642 .  \n#> TMEANPRISMXXXXPT     -0.348964   0.149201  -2.339   0.0193 *  \n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Pseudo R-squared: 0.1101\n#> \n#> Coefficients (exponential spatial covariance):\n#>        de        ie     range \n#> 3.121e+00 9.212e-03 3.636e+04 \n#> \n#> Coefficients (Dispersion for binomial family):\n#> dispersion \n#>          1\n\n\n7.2.2 Model performance\n\n# Function to convert from log odds to probability\nto_prob <- function(x) exp(x)/(1+exp(x))\n\n\nprd_mod <- loocv(bin_mod, cv_predict = TRUE) %>% \n  pluck('cv_predict') %>% \n  to_prob()\n\nprd_spmod <- loocv(bin_spmod, cv_predict = TRUE)%>% \n  pluck('cv_predict') %>% \n  to_prob()\n\npROC::auc(model_data$presence, prd_mod)\n#> Area under the curve: 0.7908\npROC::auc(model_data$presence, prd_spmod)\n#> Area under the curve: 0.9154\n\nmodel_data <- model_data %>%\n  mutate(prd_mod = prd_mod,\n         prd_spmod = prd_spmod)\n\nggplot() +\n  geom_sf(data = region, fill = NA) +\n  geom_sf(data = model_data,\n          aes(color = prd_mod)) +\n  scale_color_viridis_b() +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\n\n\n\nggplot() +\n  geom_sf(data = region, fill = NA) +\n  geom_sf(data = model_data,\n          aes(color = prd_spmod)) +\n  scale_color_viridis_b() +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\n\n\n\n\n7.2.3 EXTRA: Mapping predictions\nThis section provides an example of mapping predicted probabilities Argia across New Jersey at unsampled locations. To do this we must…\n\nSelect New Jersey from the states layer.\nGrab stream segment outlets (points) from the NHDplus with the NLDI service.\nIngest the same StreamCat and PRISM variables to R that were used in the model for all streams in New Jersey. (We’ll make predictions for 2019).\nPredict and plot predicted values across the state.\n\n\nstate <- region %>% \n  filter(STUSPS == \"NJ\") %>% \n  st_transform(crs = 4326)\n\n# Use get_nhdplus to access the individual stream sub-catchments\npourpoints <- \n  nhdplusTools::get_nhdplus(AOI = state,\n                            realization = 'outlet') |> \n  filter(flowdir == \"With Digitized\")\n\n#plot(pourpoints$geometry, pch = 19)\n\n#prd_comids <- paste(pourpoints$comid, collapse = ',')\n\nsc_prd <- sc_get_data(state = 'NJ',\n                      aoi = 'watershed,riparian_watershed',\n                      metric = 'bfi,precip8110,wetindex,elev,pctwdwet2019,pcthbwet2019,pctimp2019') |> \n  mutate(PCTWETLANDXXXXWS = PCTWDWET2019WS + PCTHBWET2019WS) |> \n  rename(PCTIMPXXXXWSRP100 = PCTIMP2019WSRP100) |> \n  select(COMID, WSAREASQKM, ELEVWS, WETINDEXWS, BFIWS, \n         PRECIP8110WS, PCTWETLANDXXXXWS, PCTIMPXXXXWSRP100)\n\ntmn_prd <- \n  pd_stack((prism_archive_subset(\"tmean\",\"monthly\", \n                                 years = 2019, \n                                 mon = 7:8)))\ntmn_prd <-\n  terra::extract(tmn_prd, \n                 pourpoints %>% \n                   st_transform(crs = st_crs(tmn_prd))) |> \n  as.tibble() |> \n  mutate(COMID = pourpoints$comid,\n         TMEANPRISMXXXXPT = (PRISM_tmean_stable_4kmM3_201907_bil + PRISM_tmean_stable_4kmM3_201908_bil)/2) |> \n  select(COMID, TMEANPRISMXXXXPT)\n\nnew_jersey <- sc_prd |> \n  left_join(tmn_prd, join_by(COMID)) |>\n  left_join(pourpoints, join_by(COMID == comid)) |> \n  st_as_sf() |> \n  select(COMID, WSAREASQKM, ELEVWS, WETINDEXWS,\n         BFIWS, PRECIP8110WS, PCTWETLANDXXXXWS,\n         PCTIMPXXXXWSRP100, TMEANPRISMXXXXPT) |> \n  na.omit() \n\nargia_predict <- \n  predict(bin_spmod, \n          newdata = new_jersey,\n          local = TRUE) |> \n  to_prob()\n\nnew_jersey <-\n  new_jersey |> \n  mutate(argia_predict = argia_predict)\n\nggplot() +\n  geom_sf(data = new_jersey,\n          aes(color = argia_predict),\n          size = 0.9) +\n  scale_color_distiller(palette = 'YlOrRd', direction = 2) +\n  theme_bw()"
  },
  {
    "objectID": "spatial-glm.html#r-code-appendix",
    "href": "spatial-glm.html#r-code-appendix",
    "title": "7  Spatial GLM of “Dancer” damselfly (Argia)",
    "section": "\n7.3 R Code Appendix",
    "text": "7.3 R Code Appendix\n\n\n\n\nlibrary(finsyncR)\nlibrary(tidyverse)\nlibrary(sf)\nlibrary(tigris)\nlibrary(StreamCatTools)\nlibrary(nhdplusTools)\nlibrary(spmodel)\nlibrary(data.table)\nlibrary(pROC)\nlibrary(knitr)\nmacros <- getInvertData(dataType = \"occur\",\n                        taxonLevel = \"Genus\",\n                        agency = \"EPA\",\n                        lifestage = FALSE,\n                        rarefy = TRUE,\n                        rarefyCount = 300,\n                        sharedTaxa = FALSE,\n                        seed = 1,\n                        boatableStreams = T)\n\nprint(dim(macros))\n\n# Print an example of the data\nkable(macros[1:5, 1:23])\n# Flexible code so we could model another taxon\ngenus <- 'Argia'\n\ntaxon = macros %>%\n  dplyr::select(SampleID, \n                ProjectLabel, \n                CollectionDate,  \n                Latitude_dd,\n                Longitude_dd,\n                all_of(genus))  %>%\n  #filter(ProjectLabel != 'WSA') %>% \n  mutate(CollectionDate = date(CollectionDate),\n         presence = \n           as.factor(pull(., genus)))  %>% \n  st_as_sf(coords = c('Longitude_dd', 'Latitude_dd'), crs = 4269)  %>% \n  st_transform(crs = 5070)\nstates <- tigris::states(cb = TRUE, progress_bar = FALSE)  %>% \n  filter(!STUSPS %in% c('HI', 'PR', 'AK', 'MP', 'GU', 'AS', 'VI'))  %>% \n  st_transform(crs = 5070)\nggplot() + \n  geom_sf(data = states, fill = NA) +\n  geom_sf(data = taxon, \n          aes(color = presence),\n          size = 1.5,\n          alpha = 0.65) + \n  scale_color_manual(values=c(\"#d9d9d9\", \"#08519c\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\") \nggplot() + \n  geom_sf(data = states, fill = NA) +\n  geom_sf(data = taxon, \n          aes(color = ProjectLabel),\n          size = 1.5,\n          alpha = 0.75) + \n  scale_color_manual(values=c(\"#a6cee3\", \"#1f78b4\", \"#b2df8a\", \"#33a02c\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\") \n\n# Filter to study region (states)\nregion <- states %>% \n  filter(STUSPS %in% c('VT', 'NH', 'ME', 'NY', 'RI',\n                       'MA', 'CT', 'NJ', 'PA', 'DE'))\n\n# Use region as spatial filter (sf::st_filter()) for taxon of interest\ntaxon_rg <- taxon %>% \n  st_filter(region) %>% \n  filter(ProjectLabel %in% c('NRSA1314', 'NRSA1819')) %>% \n  mutate(year = year(ymd(CollectionDate))) %>% \n  select(SampleID:CollectionDate, presence:year) \n\nggplot() + \n  geom_sf(data = region, fill = NA) +\n  geom_sf(data = taxon_rg, \n          aes(color = presence)) + \n  scale_color_manual(values=c(\"#d9d9d9\", \"#08519c\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\") \n\nggplot() + \n  geom_sf(data = region, fill = NA) +\n  geom_sf(data = taxon_rg, \n          aes(color = ProjectLabel)) + \n  scale_color_manual(values=c(\"#1f78b4\", \"#b2df8a\")) +\n  theme_bw() +\n  theme(legend.position=\"bottom\") \n\ntaxon_rg %>% \n  pull(presence) %>% \n  table()\ncomids <- sc_get_comid(taxon_rg)\n\n#comids <- read_rds('./data/nrsa_comids.rds')\ncomid_vect <- \n  comids %>%\n  str_split(',') %>%\n  unlist() %>%\n  as.integer()\n\ntaxon_rg <- \n  taxon_rg %>%\n  mutate(COMID = comid_vect) \nsc <- \n  sc_get_data(comid = comids,\n              aoi = 'watershed',\n              metric = 'bfi, precip8110, wetindex, elev',\n              showAreaSqKm = TRUE)\nwetlands <- \n  sc_get_data(comid = comids,\n              aoi = 'watershed',\n              metric = 'pctwdwet2013,pcthbwet2013,pctwdwet2019,pcthbwet2019',\n              showAreaSqKm = FALSE) %>% \n  \n  # Sum wetland types to create single wetlands metric\n  mutate(PCTWETLAND2013WS = PCTHBWET2013WS + PCTWDWET2013WS,\n         PCTWETLAND2019WS = PCTHBWET2019WS + PCTWDWET2019WS) %>% \n  \n  # Reduce columns\n  select(COMID, PCTWETLAND2013WS, PCTWETLAND2019WS) %>% \n  \n  # Create long table w/ column name w/out year\n  pivot_longer(!COMID, names_to = 'tmpcol', values_to = 'PCTWETLANDXXXXWS') %>% \n  \n  # Create new column of year by removing \"PCTWETLAND\" and \"WS\" from names\n  mutate(year = as.integer(str_replace_all(tmpcol, 'PCTWETLAND|WS', ''))) \n\n# But some samples have 2014 and 2018 as sample years? How can we trick the data into joining?\n# We can match 2019 data to 2018 observations by subtracting a year and appending it to the data\n\n# Create tmp table with 1 added or subtracted to year of record\ntmp_wetlands <- wetlands %>% \n  mutate(year = ifelse(year == 2013, year + 1, year - 1))\n\n# rbind() wetlands and tmp_wetlands so we have records to join to 2014 and 2018\nwetlands <- wetlands %>% \n  rbind(tmp_wetlands) %>% \n  select(-tmpcol)\nriparian_imp <- \n  sc_get_data(comid = comids,\n              aoi = 'riparian_watershed',\n              metric = 'pctimp2013, pctimp2019',\n              showAreaSqKm = FALSE) %>% \n  select(-WSAREASQKMRP100) %>% \n  pivot_longer(!COMID, names_to = 'tmpcol', values_to = 'PCTIMPXXXXWSRP100') %>% \n  mutate(year = as.integer(\n    str_replace_all(tmpcol, 'PCTIMP|WSRP100', '')))\n\ntmp_imp <- riparian_imp %>% \n  mutate(year = ifelse(year == 2013, year + 1, year - 1))\n\nriparian_imp <- riparian_imp %>% \n  rbind(tmp_imp) %>% \n  select(-tmpcol)\nlibrary(prism)\n\n# Get these years of PRISM\nyears <- c(2013, 2014, 2018, 2019)\n\n# Set the PRISM directory (creates directory in not present)\nprism_set_dl_dir(\"./data/prism_data\", create = TRUE)\n\n# Download monthly PRISM rasters (tmean)\nget_prism_monthlys('tmean', \n                   years = years, \n                   mon = 7:8, \n                   keepZip = FALSE)\n\n# Create stack of downloaded PRISM rasters\ntmn <- pd_stack((prism_archive_subset(\"tmean\",\"monthly\", \n                                      years = years, \n                                      mon = 7:8)))\n\n# Extract tmean at sample points and massage data\ntmn <- terra::extract(tmn, \n                      # Transform taxon_rg to CRS of PRISM on the fly\n                      taxon_rg %>% \n                        st_transform(crs = st_crs(tmn))) %>%\n  \n  # Add COMIDs to extracted values\n  data.frame(COMID = comid_vect, .) %>%\n  \n  # Remove front and back text from PRISM year/month in names\n  rename_with( ~ stringr::str_replace_all(., 'PRISM_tmean_stable_4kmM3_|_bil', '')) %>% \n  \n  # Pivot to long table and calle column TMEANPRISMXXXXPT, XXXX indicates year\n  pivot_longer(!COMID, names_to = 'year_month', \n               values_to = 'TMEANPRISMXXXXPT') %>% \n  \n  # Create new column of year\n  mutate(year = year(ym(year_month))) %>% \n  \n  # Average July and August temperatures \n  summarise(TMEANPRISMXXXXPT = mean(TMEANPRISMXXXXPT, na.rm = TRUE), \n            .by = c(COMID, year))\nmodel_data <-\n  taxon_rg %>%\n  left_join(sc, join_by(COMID)) %>%\n  left_join(wetlands, join_by(COMID, year)) %>%\n  left_join(riparian_imp, join_by(COMID, year)) %>%\n  left_join(tmn, join_by(COMID, year)) %>%\n  drop_na()\n\ncor(model_data %>%\n      st_drop_geometry() %>%\n      select(WSAREASQKM:TMEANPRISMXXXXPT))\n\nformula <-\n  presence ~\n  I(log10(WSAREASQKM)) +\n  ELEVWS +\n  WETINDEXWS +\n  BFIWS +\n  PRECIP8110WS +\n  PCTWETLANDXXXXWS +\n  PCTIMPXXXXWSRP100 +\n  TMEANPRISMXXXXPT\n\nbin_mod <- spglm(formula = formula,\n                 data = model_data,\n                 family = 'binomial',\n                 spcov_type = 'none')\n\nbin_spmod <- spglm(formula = formula,\n                   data = model_data,\n                   family = 'binomial',\n                   spcov_type = 'exponential')\n\nglances(bin_mod, bin_spmod)\n\nsummary(bin_spmod)\n# Function to convert from log odds to probability\nto_prob <- function(x) exp(x)/(1+exp(x))\n\n\nprd_mod <- loocv(bin_mod, cv_predict = TRUE) %>% \n  pluck('cv_predict') %>% \n  to_prob()\n\nprd_spmod <- loocv(bin_spmod, cv_predict = TRUE)%>% \n  pluck('cv_predict') %>% \n  to_prob()\n\npROC::auc(model_data$presence, prd_mod)\npROC::auc(model_data$presence, prd_spmod)\n\nmodel_data <- model_data %>%\n  mutate(prd_mod = prd_mod,\n         prd_spmod = prd_spmod)\n\nggplot() +\n  geom_sf(data = region, fill = NA) +\n  geom_sf(data = model_data,\n          aes(color = prd_mod)) +\n  scale_color_viridis_b() +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\n\nggplot() +\n  geom_sf(data = region, fill = NA) +\n  geom_sf(data = model_data,\n          aes(color = prd_spmod)) +\n  scale_color_viridis_b() +\n  theme_bw() +\n  theme(legend.position=\"bottom\")\nstate <- region %>% \n  filter(STUSPS == \"NJ\") %>% \n  st_transform(crs = 4326)\n\n# Use get_nhdplus to access the individual stream sub-catchments\npourpoints <- \n  nhdplusTools::get_nhdplus(AOI = state,\n                            realization = 'outlet') |> \n  filter(flowdir == \"With Digitized\")\n\n#plot(pourpoints$geometry, pch = 19)\n\n#prd_comids <- paste(pourpoints$comid, collapse = ',')\n\nsc_prd <- sc_get_data(state = 'NJ',\n                      aoi = 'watershed,riparian_watershed',\n                      metric = 'bfi,precip8110,wetindex,elev,pctwdwet2019,pcthbwet2019,pctimp2019') |> \n  mutate(PCTWETLANDXXXXWS = PCTWDWET2019WS + PCTHBWET2019WS) |> \n  rename(PCTIMPXXXXWSRP100 = PCTIMP2019WSRP100) |> \n  select(COMID, WSAREASQKM, ELEVWS, WETINDEXWS, BFIWS, \n         PRECIP8110WS, PCTWETLANDXXXXWS, PCTIMPXXXXWSRP100)\n\ntmn_prd <- \n  pd_stack((prism_archive_subset(\"tmean\",\"monthly\", \n                                 years = 2019, \n                                 mon = 7:8)))\ntmn_prd <-\n  terra::extract(tmn_prd, \n                 pourpoints %>% \n                   st_transform(crs = st_crs(tmn_prd))) |> \n  as.tibble() |> \n  mutate(COMID = pourpoints$comid,\n         TMEANPRISMXXXXPT = (PRISM_tmean_stable_4kmM3_201907_bil + PRISM_tmean_stable_4kmM3_201908_bil)/2) |> \n  select(COMID, TMEANPRISMXXXXPT)\n\nnew_jersey <- sc_prd |> \n  left_join(tmn_prd, join_by(COMID)) |>\n  left_join(pourpoints, join_by(COMID == comid)) |> \n  st_as_sf() |> \n  select(COMID, WSAREASQKM, ELEVWS, WETINDEXWS,\n         BFIWS, PRECIP8110WS, PCTWETLANDXXXXWS,\n         PCTIMPXXXXWSRP100, TMEANPRISMXXXXPT) |> \n  na.omit() \n\nargia_predict <- \n  predict(bin_spmod, \n          newdata = new_jersey,\n          local = TRUE) |> \n  to_prob()\n\nnew_jersey <-\n  new_jersey |> \n  mutate(argia_predict = argia_predict)\n\nggplot() +\n  geom_sf(data = new_jersey,\n          aes(color = argia_predict),\n          size = 0.9) +\n  scale_color_distiller(palette = 'YlOrRd', direction = 2) +\n  theme_bw()"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Akaike, Hirotugu. 1974. “A New Look at the Statistical Model\nIdentification.” IEEE Transactions on Automatic Control\n19 (6): 716–23.\n\n\nBates, Douglas, Martin Mächler, Ben Bolker, and Steve Walker. 2015.\n“Fitting Linear Mixed-Effects Models Using lme4.” Journal of Statistical\nSoftware 67 (1): 1–48. https://doi.org/10.18637/jss.v067.i01.\n\n\nBrus, Dick J. 2021. “Statistical Approaches for Spatial Sample\nSurvey: Persistent Misconceptions and New Developments.”\nEuropean Journal of Soil Science 72 (2): 686–703.\n\n\nCressie, Noel. 1985. “Fitting Variogram Models by Weighted Least\nSquares.” Journal of the International Association for\nMathematical Geology 17 (5): 563–86.\n\n\nCurriero, Frank C, and Subhash Lele. 1999. “A Composite Likelihood\nApproach to Semivariogram Estimation.” Journal of\nAgricultural, Biological, and Environmental Statistics, 9–28.\n\n\nDumelle, Michael, Matt Higham, and Jay M. Ver Hoef. 2023. “spmodel: Spatial Statistical Modeling and\nPrediction in R.” PLOS ONE 18 (3): 1–32. https://doi.org/10.1371/journal.pone.0282524.\n\n\nDumelle, Michael, Matt Higham, Jay M Ver Hoef, Anthony R Olsen, and Lisa\nMadsen. 2022. “A Comparison of Design-Based and Model-Based\nApproaches for Finite Population Spatial Sampling and Inference.”\nMethods in Ecology and Evolution 13 (9): 2018–29.\n\n\nHastie, Trevor, Robert Tibshirani, Jerome H Friedman, and Jerome H\nFriedman. 2009. The Elements of Statistical Learning: Data Mining,\nInference, and Prediction. Vol. 2. Springer.\n\n\nJohnson, Jerald B, and Kristian S Omland. 2004. “Model Selection\nin Ecology and Evolution.” Trends in Ecology &\nEvolution 19 (2): 101–8.\n\n\nPebesma, Edzer. 2018. “Simple Features for R:\nStandardized Support for Spatial Vector Data.”\nThe R Journal 10 (1): 439–46. https://doi.org/10.32614/RJ-2018-009.\n\n\nPinheiro, José, and Douglas Bates. 2006. Mixed-Effects Models in\nS and S-PLUS. Springer science &\nbusiness media.\n\n\nVer Hoef, Jay M, Eryn Blagg, Michael Dumelle, Philip M Dixon, Dale L\nZimmerman, and Paul Conn. 2023. “Marginal Inference for\nHierarchical Generalized Linear Mixed Models with Patterned Covariance\nMatrices Using the Laplace Approximation.” arXiv Preprint\narXiv:2305.02978.\n\n\nVer Hoef, Jay M, Michael Dumelle, Matt Higham, Erin E Peterson, and\nDaniel J Isaak. 2023. “Indexing and Partitioning the Spatial\nLinear Model for Large Data Sets.” Plos One 18 (11):\ne0291906.\n\n\nVer Hoef, Jay M, Erin E Peterson, Mevin B Hooten, Ephraim M Hanks, and\nMarie-Josèe Fortin. 2018. “Spatial Autoregressive Models for\nStatistical Inference from Ecological Data.” Ecological\nMonographs 88 (1): 36–59.\n\n\nZimmerman, Dale L, and Jay M Ver Hoef. 2024. Spatial Linear Models\nfor Environmental Data. CRC Press.\n\n\nZuur, Alain F, Elena N Ieno, Neil J Walker, Anatoly A Saveliev, Graham M\nSmith, et al. 2009. Mixed Effects Models and Extensions in Ecology\nwith r. Vol. 574. Springer."
  }
]